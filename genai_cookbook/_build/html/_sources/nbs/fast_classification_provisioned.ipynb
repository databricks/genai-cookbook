{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Classification with Provisioned Throughput\n",
    "\n",
    "This notebook shows how to use the [Provisioned Throughput Foundation Model APIs](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html#provisioned-throughput-foundation-model-apis) for a high-throughput classification task. In particular, we will use the mpt-30b-instruct model to classify the emotions of short texts.\n",
    "\n",
    "## Getting Started: Create a Provisioned Throughput Serving Endpoint\n",
    "\n",
    "To get started, follow the instructions [here](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html#create-your-provisioned-throughput-endpoint-using-the-ui) to create a provisioned throughput serving endpoint. You can [get the model from the Databricks Marketplace](https://www.databricks.com/blog/introducing-ai-model-sharing-databricks): navigate to \"Marketplace\" in your databricks workspace and search for \"MPT Models.\" You can save the models to a catalog in your workspace, and then that model using the provisioned throughput instructions linked above. Note that setting up the endpoint may take a few minutes.\n",
    "\n",
    "The rest of this notebook assumes you have set up the provisioned throughput endpoint. Note that the final results of this notebook will depend on the tokens per second limit configured when setting up the provisioned throughput endpoint.\n",
    "\n",
    "## Classification Example\n",
    "\n",
    "This example will use mpt-30b-instruct to classify the emotions of the texts in the [DAIR AI Emotion dataset](https://huggingface.co/datasets/dair-ai/emotion). The focus of this example is throughput: there will be room left to improve on the classification component of this example through tweaking the prompt, model parameters, and model choice.\n",
    "\n",
    "### Set up libraries and Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69589d4d-de2e-44bc-9524-38bd5fed438c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upgrade to use the newest Databricks SDK\n",
    "%pip install --upgrade aiohttp tqdm datasets\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1cddcac-4769-4932-9c64-a0d49c54c471",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get() \n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "endpoint_name = \"<your-endpoint-name>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6558865-6006-4655-9d5a-21e13ceb9d5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotion = load_dataset(\"dair-ai/emotion\", cache_dir=\"/Volumes/daniel_liden/examples/datasets\")\n",
    "emotion_messages = emotion['train']['text']\n",
    "\n",
    "complete_prompts = []\n",
    "prompt_template = \"\"\"Your task is to classify the emotion of the provided message. Only use one of the specified emotions for your classification. The valid classifications are: JOY, SADNESS, ANGER, FEAR, LOVE, SURPRISE. No other classifications will be accepted. Even if none of these seems like a perfect fit, pick the closest one.\n",
    "\n",
    "### Instruction:\n",
    "Read the message below and classify its emotion:\n",
    "\n",
    "<message>{}</message>\n",
    "\n",
    "Expected Format:\n",
    "Respond with the emotion in the following format only:\n",
    "\n",
    "EMOTION\n",
    "\n",
    "Do not include any explanation or additional text.\n",
    "\n",
    "### Examples:\n",
    "- Message: \"I was shocked by the testimony!\"\n",
    "  Response: SURPRISE\n",
    "\n",
    "- Message: \"I don't know how I'm ever going to move on after my team's loss...\"\n",
    "  Response: SADNESS\n",
    "\n",
    "Ensure your response strictly follows the given format and only uses one of the specified emotion classifications.\n",
    "### Response:\\n\"\"\"\n",
    "\n",
    "# Loop through each message in the emotion_messages list\n",
    "for message in emotion_messages:\n",
    "    formatted_prompt = prompt_template.format(message)\n",
    "    complete_prompts.append(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d5ebd4e-f6a0-45a9-b73b-43fb46666bc9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Configure API calls with aiohttp\n",
    "To improve throughput, we use the [aiohttp](https://docs.aiohttp.org/en/stable/) library to make API calls concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fc9979-957c-42f8-9463-fe77164cebdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "import asyncio\n",
    "import time\n",
    "import statistics\n",
    "from tqdm.asyncio import tqdm  # Import tqdm for asyncio\n",
    "\n",
    "\n",
    "# Placeholder variables for endpoint and headers\n",
    "endpoint_url = f\"{API_ROOT}/serving-endpoints/{endpoint_name}/invocations\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"text/json\",\n",
    "    \"Authorization\": f\"Bearer {API_TOKEN}\"\n",
    "}\n",
    "\n",
    "latencies = []\n",
    "data = []\n",
    "\n",
    "async def worker(index, prompt, concurrency_semaphore):\n",
    "    async with concurrency_semaphore:  # Use the passed semaphore for concurrency control\n",
    "        input_data = {\n",
    "            \"inputs\": {\n",
    "                \"prompt\": [prompt]\n",
    "            },\n",
    "            \"params\": {\n",
    "                \"max_tokens\": 10,\n",
    "                \"temperature\": 0.2,\n",
    "                \"top_p\": 0.1\n",
    "            }\n",
    "        }\n",
    "\n",
    "        request_start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            timeout = aiohttp.ClientTimeout(total=3 * 3600)\n",
    "            async with aiohttp.ClientSession(timeout=timeout) as session:\n",
    "                async with session.post(endpoint_url, headers=headers, json=input_data) as response:\n",
    "                    if response.ok:\n",
    "                        response_data = await response.json()\n",
    "                        latency = time.time() - request_start_time\n",
    "                        latencies.append(latency)\n",
    "                        data.append(response_data)\n",
    "                    else:\n",
    "                        error_response = await response.text()\n",
    "                        print(f\"Request failed, status: {response.status}, error: {error_response}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "async def process_prompts(complete_prompts, num_concurrent_calls):\n",
    "    concurrency_semaphore = asyncio.Semaphore(num_concurrent_calls)  # Create semaphore based on num_concurrent_calls\n",
    "    tasks = [worker(i, prompt, concurrency_semaphore) for i, prompt in enumerate(complete_prompts)]\n",
    "    for task in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"Generating Data\"):\n",
    "        await task\n",
    "\n",
    "async def main(complete_prompts, num_concurrent_calls=15):  # Default concurrency level set to 15\n",
    "    print(\"Starting data generation...\")\n",
    "    await process_prompts(complete_prompts, num_concurrent_calls)\n",
    "    if latencies:\n",
    "        median_latency = statistics.median(latencies)\n",
    "        print(f\"Median latency (s): {median_latency}\")\n",
    "    else:\n",
    "        print(\"No data collected.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick overview of what the above functions do:\n",
    "\n",
    "- `worker(index, prompt, concurrency_semaphore)`: Executes a single asynchronous HTTP POST request using a provided prompt and a [concurrency semaphore](https://docs.python.org/3/library/asyncio-sync.html#asyncio.Semaphore) to control the rate of concurrent requests, recording latency and response data.\n",
    "\n",
    "- `process_prompts(complete_prompts, num_concurrent_calls)`: Distributes prompts among workers, enforcing concurrency limits with a semaphore, and tracks the completion of all tasks with progress output.\n",
    "\n",
    "- `main(complete_prompts, num_concurrent_calls=15)`: Initiates the process of generating data by calling `process_prompts` with the complete list of prompts and the specified number of concurrent calls, and summarizes the median latency upon completion.\n",
    "\n",
    "\n",
    "### Run the API Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "557d34a2-b966-4ab9-b70a-665fa30ec53e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data generation...\n",
      "\rGenerating Data: 100%|██████████| 16000/16000 [09:33<00:00, 27.92it/s]\n",
      "Median latency (s): 2.189871907234192\n"
     ]
    }
   ],
   "source": [
    "await main(complete_prompts, num_concurrent_calls=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaac6064-4698-434f-b495-451ed8779419",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'predictions': [{'candidates': [{'text': 'FEAR',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 220,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 223}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'LOVE',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 216,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 219}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'LOVE',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 238,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 241}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'SADNESS',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 224,\n",
       "     'output_tokens': 4,\n",
       "     'total_tokens': 228}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'FEELING',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 216,\n",
       "     'output_tokens': 4,\n",
       "     'total_tokens': 220}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'JOY',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 262,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 265}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'LOVE',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 262,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 265}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'SURPRISE',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 264,\n",
       "     'output_tokens': 5,\n",
       "     'total_tokens': 269}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'LOVE',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 251,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 254}}]},\n",
       " {'predictions': [{'candidates': [{'text': 'FEAR',\n",
       "      'metadata': {'finish_reason': 'stop'}}],\n",
       "    'metadata': {'input_tokens': 211,\n",
       "     'output_tokens': 3,\n",
       "     'total_tokens': 214}}]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a sketch of one approach to accomplish high-throughput classification with the [Provisioned Throughput Foundation Model APIs](https://docs.databricks.com/en/machine-learning/foundation-models/deploy-prov-throughput-foundation-model-apis.html)."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "fast_classification_workspace_client",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
