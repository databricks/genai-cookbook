
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Section 5: Hands-on guide to implementing high-quality RAG &#8212; Databricks Generative AI Cookbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-6BZ4NTBHVJ"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-6BZ4NTBHVJ');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-6BZ4NTBHVJ');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nbs/5-hands-on copy 3';</script>
    <link rel="icon" href="../_static/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo2.png" class="logo__image only-light" alt="Databricks Generative AI Cookbook - Home"/>
    <script>document.write(`<img src="../_static/logo2.png" class="logo__image only-dark" alt="Databricks Generative AI Cookbook - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Learning how RAG works</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1-introduction-to-rag.html">Introduction to retrieval-augmented generation (RAG)</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="2-fundamentals-unstructured.html">Fundamentals of RAG over unstructured documents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="2-fundamentals-unstructured-data-pipeline.html">Data pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="2-fundamentals-unstructured-chain.html">Retrieval, augmentation, and generation (aka RAG Chain)</a></li>
<li class="toctree-l2"><a class="reference internal" href="2-fundamentals-unstructured-eval.html">Evaluation &amp; monitoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="2-fundamentals-unstructured-llmops.html">Governance and LLMops</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="3-deep-dive.html">Deep dive into RAG over unstructured documents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="3-deep-dive-data-pipeline.html">Data pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="3-deep-dive-chain.html">Retrieval, augmentation, and generation (aka RAG Chain)</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Evaluating RAG apps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4-evaluation.html">Evaluation overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-evaluation-eval-sets.html">Establishing Ground Truth: Creating Evaluation Sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="4-evaluation-metrics.html">Assessing Performance: Metrics that Matter</a></li>

<li class="toctree-l1"><a class="reference internal" href="4-evaluation-infra.html">Enabling Measurement: Supporting Infrastructure</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">How to implement RAG</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5-hands-on.html">1. <strong>Section 5:</strong> Hands-on guide to implementing high-quality RAG</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/databricks-genai-cookbook/cookbook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/databricks-genai-cookbook/cookbook/issues/new?title=Issue%20on%20page%20%2Fnbs/5-hands-on copy 3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/nbs/5-hands-on copy 3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Section 5: Hands-on guide to implementing high-quality RAG</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-driven-development">Evaluation-driven development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gather-requirements">Gather requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#is-the-use-case-a-good-fit-for-rag">Is the use case a good fit for RAG?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements-questions">Requirements questions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#user-experience">User Experience</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-constraints">Performance constraints</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#security">Security</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment">Deployment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-collect-feedback-on-poc">Build &amp; Collect Feedback on POC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-build-a-poc">How to build a POC</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-sample-code">1. Import the sample code.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-your-application">2. Configure your application</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-your-data">3. Prepare your data.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-poc-chain-to-the-quality-lab-review-app">4. Deploy the POC chain to the Quality Lab Review App</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#share-the-review-app-with-stakeholders">5. Share the Review App with stakeholders</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-poc-s-quality">Evaluate the POC‚Äôs quality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#etl-the-logs-to-an-evaluation-set-run-evaluation">1. ETL the logs to an Evaluation Set &amp; run evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#review-evaluation-results">2. Review evaluation results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improve-rag-quality">Improve RAG quality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understand-rag-quality-improvement-levers"><strong>Step 1:</strong> Understand RAG quality improvement levers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-identify-the-root-cause-of-quality-issues"><strong>Step 2:</strong> Identify the root cause of quality issues</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-quality">Retrieval quality</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-retrieval-quality">Debugging retrieval quality</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons-for-poor-retrieval-quality">Common reasons for poor retrieval quality</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generation-quality">Generation quality</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-generation-quality">Debugging generation quality</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons-for-poor-generation-quality">Common reasons for poor generation quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-implement-and-evaluate-changes"><strong>Step 3:</strong> Implement and evaluate changes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pipeline-changes">Data pipeline changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-chain-changes">RAG chain changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-a-potential-fix-that-could-improve-quality">Testing a potential fix that could improve quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deployment</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="section-5-hands-on-guide-to-implementing-high-quality-rag">
<h1><strong>Section 5:</strong> Hands-on guide to implementing high-quality RAG<a class="headerlink" href="#section-5-hands-on-guide-to-implementing-high-quality-rag" title="Link to this heading">#</a></h1>
<p>This section walks you through Databricks recommended development workflow for building, testing, and deploying a high-quality RAG application: <strong>evaluation-driven development</strong>. This workflow is based on the Mosaic Research team‚Äôs best practices for building and evaluating high quality RAG applications. If quality is important to your business, Databricks recommends following an evaluation-driven workflow:</p>
<ol class="arabic simple">
<li><p>Define the requirements</p></li>
<li><p>Collect stakeholder feedback on a rapid proof of concept (POC)</p></li>
<li><p>Evaluate the POC‚Äôs quality</p></li>
<li><p>Iteratively diagnose and fix quality issues</p></li>
<li><p>Deploy to production</p></li>
<li><p>Monitor in production</p></li>
</ol>
<img alt="../_images/1_img4.png" class="align-center" src="../_images/1_img4.png" />
<p>Mapping to this workflow, this section provides ready-to-run sample code for every step and every suggestion to improve quality.</p>
<p>Throughout, we will demonstrate evaluation-driven development using one of Databricks‚Äô internal use generative AI cases: using a RAG bot to help answer customer support questions in order to [1] reduce support costs [2] improve the customer experience.</p>
<section id="evaluation-driven-development">
<h2>Evaluation-driven development<a class="headerlink" href="#evaluation-driven-development" title="Link to this heading">#</a></h2>
<p>There are two core concepts in <strong>evaluation-driven development:</strong></p>
<ol class="arabic">
<li><p><strong>Metrics:</strong> Defining high-quality</p>
<p><em>Similar to how you set business goals each year, you need to define what high-quality means for your use case.</em> <em>Databricks‚Äô Quality Lab provides a suggested set of</em> <em>N metrics to use, the most important of which is answer accuracy or correctness - is the RAG application providing the right answer?</em></p>
</li>
<li><p><strong>Evaluation:</strong> Objectively measuring the metrics</p>
<p><em>To objectively measure quality, you need an evaluation set, which contains questions with known-good answers validated by humans. While this may seem scary at first - you probably don‚Äôt have an evaluation set sitting ready to go - this guide walks you through the process of developing and iteratively refining this evaluation set.</em></p>
</li>
</ol>
<p>Anchoring against metrics and an evaluation set provides the following benefits:</p>
<ol class="arabic simple">
<li><p>You can iteratively and confidently refine your application‚Äôs quality during development - no more vibe checks or guessing if a change resulted in an improvement.</p></li>
<li><p>Getting alignment with business stakeholders on the readiness of the application for production becomes more straightforward when you can confidently state, <em>‚Äúwe know our application answers the most critical questions to our business correctly and doesn‚Äôt hallucinate.‚Äù</em></p></li>
</ol>
<p><em>&gt;&gt; Evaluation-driven development is known in the academic research community as ‚Äúhill climbing‚Äù akin to climbing a hill to reach the peak - where the hill is your metric and the peak is 100% accuracy on your evaluation set.</em></p>
</section>
<section id="gather-requirements">
<h2>Gather requirements<a class="headerlink" href="#gather-requirements" title="Link to this heading">#</a></h2>
<img alt="../_images/2_img3.png" class="align-center" src="../_images/2_img3.png" />
<p>Defining clear and comprehensive use case requirements is a critical first step in developing a successful RAG application. These requirements serve two primary purposes. Firstly, they help determine whether RAG is the most suitable approach for the given use case. If RAG is indeed a good fit, these requirements guide solution design, implementation, and evaluation decisions. Investing time at the outset of a project to gather detailed requirements can prevent significant challenges and setbacks later in the development process, and ensures that the resulting solution meets the needs of end-users and stakeholders. Well-defined requirements provide the foundation for the subsequent stages of the development lifecycle we‚Äôll walk through.</p>
<section id="is-the-use-case-a-good-fit-for-rag">
<h3>Is the use case a good fit for RAG?<a class="headerlink" href="#is-the-use-case-a-good-fit-for-rag" title="Link to this heading">#</a></h3>
<p>The first thing you‚Äôll need to establish is whether RAG is even the right approach for your use case. Given the hype around RAG, it‚Äôs tempting to view it as a possible solution for any problem. However, there are nuances as to when RAG is suitable versus not.</p>
<p>RAG is a good fit when:</p>
<ul class="simple">
<li><p>Reasoning over retrieved information (both unstructured and structured)</p></li>
<li><p>Synthesizing information from multiple sources (e.g., generating a summary of key points from different articles on a topic)</p></li>
<li><p>Dynamic retrieval based on a user query is necessary (e.g., given a user query, determine what data source to retrieve from)</p></li>
<li><p>The use case requires generating novel content based on retrieved information (e.g., answering questions, providing explanations, offering recommendations)</p></li>
</ul>
<p>Conversely, RAG may not be the best fit when:</p>
<ul class="simple">
<li><p>The task does not require query-specific retrieval. For example, generating call transcript summaries; even if individual transcripts are provided as context in the LLM prompt, the retrieved information remains the same for each summary.</p></li>
<li><p>Extremely low-latency responses are required (i.e., when responses are required in milliseconds)</p></li>
<li><p>The output is expected to be an exact copy of the retrieved information without modification (e.g., a search engine that returns verbatim snippets from documents)</p></li>
<li><p>Simple rule-based or templated responses are sufficient (e.g., a customer support chatbot that provides predefined answers based on keywords)</p></li>
<li><p>Input data needs to be reformatted (e.g., a user provides some input text and expects it to be transformed to a table)</p></li>
</ul>
</section>
<section id="requirements-questions">
<h3>Requirements questions<a class="headerlink" href="#requirements-questions" title="Link to this heading">#</a></h3>
<p>Having established that RAG is indeed a good fit for your use case, consider the following questions to capture concrete requirements. For each requirement, we have prioritized them:</p>
<ul class="simple">
<li><p>üü¢ P0 : Must define this requirement before starting your POC</p></li>
<li><p>üü° P1: Must define before going to production, but can iteratively refine during the POC</p></li>
<li><p>‚ö™ P2: Nice to have requirement</p></li>
</ul>
<section id="user-experience">
<h4>User Experience<a class="headerlink" href="#user-experience" title="Link to this heading">#</a></h4>
<p><em>Define how users will interact with the RAG system and what kind of responses are expected</em></p>
<ul class="simple">
<li><p>üü¢ P0 What will a typical request to the RAG chain look like? Ask stakeholders for examples of potential user queries.</p></li>
<li><p>üü¢ P0 What kind of responses will users expect (e.g., short answers, long-form explanations, a combination, or something else)?</p></li>
<li><p>üü° P1 How will users interact with the system? Through a chat interface, search bar, or some other modality?</p></li>
<li><p>üü° P1 What tone or style should generated responses take? (e.g., formal, conversational, technical)</p></li>
<li><p>üü° P1 How should the application handle ambiguous, incomplete, or irrelevant queries? Should any form of feedback or guidance be provided in such cases?</p></li>
<li><p>‚ö™ P2 Are there specific formatting or presentation requirements for the generated output? Should the output include any metadata in addition to the chain‚Äôs response?</p></li>
</ul>
</section>
<section id="data">
<h4>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h4>
<p><em>Determine the nature, source(s), and quality of the data that will be used in the RAG solution</em></p>
<ul class="simple">
<li><p>üü¢ P0 What are the available sources to use?</p></li>
<li><p>For each data source:</p>
<ul>
<li><p>üü¢ P0 Is data structured or unstructured?</p></li>
<li><p>üü¢ P0 What is the source format of the retrieval data (e.g., PDFs, documentation with images/tables, structured API responses)?</p></li>
<li><p>üü¢ P0 Where does that data reside?</p></li>
<li><p>üü¢ P0 How much data is available?</p></li>
<li><p>üü° P1 How frequently is the data updated? How should those updates be handled?</p></li>
<li><p>üü° P1 Are there any known data quality issues or inconsistencies for each data source?</p></li>
</ul>
</li>
</ul>
<p>Consider creating an inventory table to consolidate this information, for example:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Data Source</p></th>
<th class="head"><p>Source</p></th>
<th class="head"><p>File type(s)</p></th>
<th class="head"><p>Size</p></th>
<th class="head"><p>Update frequency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data source 1</p></td>
<td><p>Unity Catalog Volume</p></td>
<td><p>JSON</p></td>
<td><p>10GB</p></td>
<td><p>Daily</p></td>
</tr>
<tr class="row-odd"><td><p>Data source 2</p></td>
<td><p>Public API</p></td>
<td><p>XML</p></td>
<td><p>n/a (API)</p></td>
<td><p>Real-time</p></td>
</tr>
<tr class="row-even"><td><p>Data source 3</p></td>
<td><p>SharePoint</p></td>
<td><p>PDF, DOCX</p></td>
<td><p>500MB</p></td>
<td><p>Monthly</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="performance-constraints">
<h4>Performance constraints<a class="headerlink" href="#performance-constraints" title="Link to this heading">#</a></h4>
<p><em>Capture performance and resource requirements for the RAG application</em></p>
<ul class="simple">
<li><p>üü° P1 What is the maximum acceptable latency for generating the responses?</p>
<ul>
<li><p>üü° P1 What is the maximum acceptable time to first token?</p></li>
<li><p>üü° P1 If the output is being streamed, is higher total latency acceptable?</p></li>
</ul>
</li>
<li><p>üü° P1 Are there any cost limitations on compute resources available for inference?</p></li>
<li><p>üü° P1 What are the expected usage patterns and peak loads?</p></li>
<li><p>üü° P1 How many concurrent users or requests should the system be able to handle?</p>
<ul>
<li><p><strong>NOTE:</strong> Databricks natively handles such scalability requirements, through the ability to scale automatically with <a class="reference external" href="https://docs.databricks.com/en/machine-learning/model-serving/index.html">Model Serving</a>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="evaluation">
<h4>Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading">#</a></h4>
<p><em>Establish how the RAG solution will be evaluated and improved over time</em></p>
<ul class="simple">
<li><p>üü¢ P0 What is the business goal / KPI you want to impact? What is the baseline value and what is the target?</p></li>
<li><p>üü¢ P0 Which users or stakeholders will provide initial and ongoing feedback?</p></li>
<li><p>üü¢ P0 What metrics should be used to assess the quality of generated responses?</p>
<ul>
<li><p>Note: Databricks Quality Lab provides a recommended set of metrics to yo use</p></li>
</ul>
</li>
<li><p>üü° P1 What is the set of questions the RAG app must be good at to go to production?</p></li>
<li><p>üü° P1 Does an <a class="reference internal" href="4-evaluation.html#establishing-ground-truth-creating-evaluation-sets"><span class="std std-ref">evaluation set</span></a> exist? Is it possible to get an evaluation set of user queries, along with ground-truth answers and (optionally) the correct supporting documents that should be retrieved?</p></li>
<li><p>üü° P1 How will user feedback be collected and incorporated into the system?</p></li>
</ul>
</section>
<section id="security">
<h4>Security<a class="headerlink" href="#security" title="Link to this heading">#</a></h4>
<p><em>Identify any security and privacy considerations</em></p>
<ul class="simple">
<li><p>üü¢ P0 Are there sensitive/confidential data that needs to be handled with care?</p></li>
<li><p>üü° P1 Do access controls need to be implemented in the solution (e.g., a given user can only retrieve from a restricted set of documents)?</p></li>
</ul>
</section>
<section id="deployment">
<h4>Deployment<a class="headerlink" href="#deployment" title="Link to this heading">#</a></h4>
<p><em>Understanding how the RAG solution will be integrated, deployed, and maintained</em></p>
<ul class="simple">
<li><p>üü° P1 How should the RAG solution integrate with existing systems and workflows?</p></li>
<li><p>üü° P1 How should the model be deployed, scaled, and versioned?</p>
<ul>
<li><p><strong>NOTE:</strong> we will cover how this end-to-end lifecycle can be handled on Databricks with MLflow, Unity Catalog, Agent SDK, and Model Serving**.**</p></li>
</ul>
</li>
</ul>
<p>Note that this is by no means an exhaustive list of questions. However, it should provide a solid foundation for capturing the key requirements for your RAG solution.</p>
<p>Let‚Äôs look at how some of these questions apply to the Databricks customer support RAG application:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Considerations</p></th>
<th class="head"><p>Requirements</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>User experience</p></td>
<td><p>- Interaction modality<br>- Typical user query examples<br>- Expected response format/style<br>- Handling ambiguous/irrelevant queries</p></td>
<td><p>- Chat interface integrated with Slack<br>- Example queries:<br>  - ‚ÄúHow do I reduce cluster startup time?‚Äù<br>  - ‚ÄúWhat kind of support plan do I have?‚Äù<br>- Clear, technical responses with code snippets and links to relevant documentation where appropriate<br>- Provide contextual suggestions and escalate to Databricks support engineers when needed</p></td>
</tr>
<tr class="row-odd"><td><p>Data</p></td>
<td><p>- Number and type of data sources<br>- Data format and location<br>- Data size and update frequency<br>- Data quality and consistency</p></td>
<td><p>- 3 data sources<br>  - Databricks documentation (HTML, PDF)<br>  - Resolved support tickets (JSON)<br>  - Community forum posts (Delta table)<br>- Data stored in Unity Catalog and updated weekly<br>- Total data size: 5 GB<br>- Consistent data structure and quality maintained by dedicated docs and support teams</p></td>
</tr>
<tr class="row-even"><td><p>Performance</p></td>
<td><p>- Maximum acceptable latency<br>- Cost constraints<br>- Expected usage and concurrency</p></td>
<td><p>- Maximum latency:<br>  - &lt;5 seconds<br>- Cost constraints:<br>  - [confidential]<br>- Expected peak load:<br>  - 200 concurrent users</p></td>
</tr>
<tr class="row-odd"><td><p>Evaluation</p></td>
<td><p>- Evaluation dataset availability<br>- Quality metrics<br>- User feedback collection</p></td>
<td><p>- SMEs from each product area will help review outputs and adjust incorrect answers to create the evaluation dataset<br>- Business KPIs<br>  - Increase in support ticket resolution rate<br>  - Decrease in user time spent per support ticket<br>- Quality metrics<br>  - LLM judged answer correctness &amp; relevance<br>  - LLM judges retrieval precision<br>  - User upvote/downvote<br>- Feedback collection<br>  - Slack will be instrumented to provide a thumbs up / down</p></td>
</tr>
<tr class="row-even"><td><p>Security</p></td>
<td><p>- Sensitive data handling<br>- Access control requirements</p></td>
<td><p>- No sensitive customer data should be in the retrieval source<br>- User authentication through Databricks Community SSO</p></td>
</tr>
<tr class="row-odd"><td><p>Deployment</p></td>
<td><p>- Integration with existing systems<br>- Deployment and versioning</p></td>
<td><p>- Integration with Databricks support ticket system<br>- Chain deployed as a Databricks Model Serving endpoint</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
</section>
<section id="build-collect-feedback-on-poc">
<h2>Build &amp; Collect Feedback on POC<a class="headerlink" href="#build-collect-feedback-on-poc" title="Link to this heading">#</a></h2>
<img alt="../_images/3_img3.png" class="align-center" src="../_images/3_img3.png" />
<p>The first step in evaluation-driven development is to build a proof of concept (POC). A POC offers several benefits:</p>
<ol class="arabic simple">
<li><p>Provides a directional view on the feasibility of your use case with RAG</p></li>
<li><p>Allows collecting initial feedback from stakeholders, which in turn enables you to create the first version of your Evaluation Set</p></li>
<li><p>Establishes a baseline measurement of quality to start to iterate from</p></li>
</ol>
<p>Databricks recommends building your POC using the simplest RAG chain architecture and our recommended defaults for each knob/parameter.</p>
<blockquote>
<div><p><em>!! Important: our recommended default parameters are by no means perfect, nor are they intended to be. Rather, they are a place to start from - the next steps of our workflow guide you through iterating on these parameters.</em></p>
<p><em>Why start from a simple POC? There are hundreds of possible combinations of knobs you can tune within your RAG application. You can easily spend weeks tuning these knobs, but if you do so before you can systematically evaluate your RAG, you‚Äôll end up in what we call the POC doom loop - iterating on settings, but with no way to objectively know if you made an improvement ‚Äì all while your stakeholders sit around impatiently waiting.</em></p>
</div></blockquote>
<p>The POC templates in this guide are designed with quality iteration in mind - that is, they are parameterized with the knobs that our research has shown are most important to tune in order to improve RAG quality. Each knob has a smart default.</p>
<p>Said differently, these templates are not ‚Äú3 lines of code that magically make a RAG‚Äù - rather, they are a well-structured RAG application that can be tuned for quality in the following steps of an evaluation-driven development workflow.</p>
<p>This enables you to quickly deploy a POC, but transition quickly to quality iteration without needing to rewrite your code.</p>
<section id="how-to-build-a-poc">
<h3>How to build a POC<a class="headerlink" href="#how-to-build-a-poc" title="Link to this heading">#</a></h3>
<p><strong>Expected time:</strong> 30-60 minutes</p>
<p><strong>Requirements:</strong></p>
<ul class="simple">
<li><p>Data from your <a class="reference internal" href="#requirements-questions"><span class="xref myst">requirements</span></a> is available in your <a class="reference external" href="https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">Lakehouse</a> inside a <a class="reference external" href="https://www.databricks.com/product/unity-catalog">Unity Catalog</a> <a class="reference external" href="https://docs.databricks.com/en/connect/unity-catalog/volumes.html">volume</a> or <a class="reference external" href="https://docs.databricks.com/en/delta/index.html">Delta Table</a></p></li>
<li><p>Access to a <a class="reference external" href="https://docs.databricks.com/en/generative-ai/vector-search.html">Mosaic AI Vector Search</a> endpoint [<a class="reference external" href="https://docs.databricks.com/en/generative-ai/create-query-vector-search.html">instructions</a>]</p></li>
<li><p>Write access to Unity Catalog schema</p></li>
<li><p>A single-user cluster with DBR 14.3+</p></li>
</ul>
<p>At the end of this step, you will have deployed the Quality Lab Review App which allows your stakeholders to test and provide feedback on your POC. Detailed logs from your stakeholder‚Äôs usage and their feedback will flow to Delta Tables in your Lakehouse.</p>
<img alt="../_images/4_img2.png" class="align-center" src="../_images/4_img2.png" />
<p>Below is the technical architecture of the POC application.</p>
<img alt="../_images/5_img2.png" class="align-center" src="../_images/5_img2.png" />
<p>By default, the POC uses the open source models available on <a class="reference external" href="https://www.databricks.com/product/pricing/foundation-model-serving">Mosaic AI Foundation Model Serving</a>. However, because the POC uses Mosaic AI Model Serving, which supports <em>any foundation model</em>, using a different model is easy - simply configure that model in Model Serving and then replace the embedding_endpoint_name and llm_endpoint_name parameters in the POC code.</p>
<ul class="simple">
<li><p>[follow these steps for other open source models in the marketplace e.g., PT]</p></li>
<li><p>[follow these steps for models such as Azure OpenAI, OpenAI, Cohere, Anthropic, Google Gemini, etc e.g., external models]</p></li>
</ul>
<section id="import-the-sample-code">
<h4>1. Import the sample code.<a class="headerlink" href="#import-the-sample-code" title="Link to this heading">#</a></h4>
<p>To get started, <a class="reference external" href="https://docs.databricks.com/en/repos/index.html">import this Git Repository to your Databricks Workspace</a>. This repository contains the entire set of sample code. Based on your data, select one of the following folders that contains the POC application code.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>File type</p></th>
<th class="head"><p>Source</p></th>
<th class="head"><p>POC application folder</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PDF files</p></td>
<td><p>UC Volume</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>JSON files w/ HTML content &amp; metadata</p></td>
<td><p>UC Volume</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>Powerpoint files</p></td>
<td><p>UC Volume</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>DOCX files</p></td>
<td><p>UC Volume</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p>HTML content</p></td>
<td><p>Delta Table</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p>Markdown or regular text</p></td>
<td><p>Delta Table</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
</div>
<p>If you don‚Äôt have any data ready, and just want to follow along using the Databricks Customer Support Bot example, you can use this pipeline which uses a Delta Table of the Databricks Docs stored as HTML.</p>
<p>If your data doesn‚Äôt meet one of the above requirements, [insert instructions on how to customize].</p>
<p>Once you have imported the code, you will have the following notebooks:</p>
<img alt="../_images/6_img.png" class="align-center" src="../_images/6_img.png" />
</section>
<section id="configure-your-application">
<h4>2. Configure your application<a class="headerlink" href="#configure-your-application" title="Link to this heading">#</a></h4>
<p>Follow the instructions in the <code class="docutils literal notranslate"><span class="pre">00_config</span></code> Notebook to configure the following settings:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">RAG_APP_NAME</span></code>: The name of the RAG application. This is used to name the chain‚Äôs UC model and prepended to the output Delta Tables + Vector Indexes</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">UC_CATALOG</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">UC_SCHEMA</span></code>: <a class="reference external" href="https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#create-a-catalog">Create Unity Catalog</a> and a Schema where the output Delta Tables with the parsed/chunked documents and Vector Search indexes are stored</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">UC_MODEL_NAME</span></code>: Unity Catalog location to log and store the chain‚Äôs model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VECTOR_SEARCH_ENDPOINT</span></code>: <a class="reference external" href="https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-endpoint">Create Vector Search Endpoint</a> to host the resulting vector index</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SOURCE_PATH</span></code>: <a class="reference external" href="https://docs.databricks.com/en/connect/unity-catalog/volumes.html#create-and-work-with-volumes">Create Volumes</a> for source documents as <code class="docutils literal notranslate"><span class="pre">SOURCE_PATH</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MLFLOW_EXPERIMENT_NAME</span></code>: MLflow Experiment to use for this application. Using the same experiment allows you to track runs across Notebooks and store a single history for your application.</p></li>
</ol>
<p>Run the <code class="docutils literal notranslate"><span class="pre">00_validate_config</span></code> to check that your configuration is valid and all resources are available. You will see an <code class="docutils literal notranslate"><span class="pre">rag_chain_config.yaml</span></code> file appear in your directory - we will do this in step 4 to deploy the application.</p>
</section>
<section id="prepare-your-data">
<h4>3. Prepare your data.<a class="headerlink" href="#prepare-your-data" title="Link to this heading">#</a></h4>
<p>The POC data pipeline is a Databricks Notebook based on Apache Spark that provides a default implementation of the parameters outlined below.</p>
<p>To run this pipeline and generate your initial Vector Index:</p>
<ol class="arabic simple">
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">02_poc_data_pipeline</span></code> Notebook and connect it to your single-user cluster</p></li>
<li><p>Press Run All to execute the data pipeline</p></li>
<li><p>In the last cell of the notebook, you can see the resulting Delta Tables and Vector Index.</p></li>
</ol>
<img alt="../_images/7_img.png" class="align-center" src="../_images/7_img.png" />
<p>Parameters and their default values that are configured in <code class="docutils literal notranslate"><span class="pre">00_config</span></code>.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Knob</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="3-deep-dive.html#parsing"><span class="std std-ref">Parsing strategy</span></a></p></td>
<td><p>Extracting relevant information from the raw data using appropriate parsing techniques</p></td>
<td><p>Varies based on document type, but generally an open source parsing library</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="3-deep-dive.html#chunking"><span class="std std-ref">Chunking strategy</span></a></p></td>
<td><p>Breaking down the parsed data into smaller, manageable chunks for efficient retrieval</p></td>
<td><p>Token Text Splitter, which splits text along using a chunk size of 4000 tokens and a stride of 500 tokens.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="3-deep-dive.html#embedding-model"><span class="std std-ref">Embedding model</span></a></p></td>
<td><p>Converting the chunked text data into a numerical vector representation that captures its semantic meaning</p></td>
<td><p>GTE-Large-v1.5 on the Databricks FMAPI pay-per-token</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="deploy-the-poc-chain-to-the-quality-lab-review-app">
<h4>4. Deploy the POC chain to the Quality Lab Review App<a class="headerlink" href="#deploy-the-poc-chain-to-the-quality-lab-review-app" title="Link to this heading">#</a></h4>
<p>The POC chain is a RAG chain that provides a default implementation of the parameters outlined below.</p>
<blockquote>
<div><p>Note: The POC Chain uses MLflow code-based logging. To understand more about code-based logging, [link to docs].</p>
</div></blockquote>
<ol class="arabic simple">
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">03_deploy_poc_to_review_app</span></code> Notebook</p></li>
<li><p>Run each cell of the Notebook.</p></li>
<li><p>You will see the MLflow Trace that shows you how the POC application works. Adjust the input question to one that is relevant to your use case, and re-run the cell to ‚Äúvibe check‚Äù the application.</p></li>
</ol>
<img alt="../_images/8_img.png" class="align-center" src="../_images/8_img.png" />
<ol class="arabic simple" start="4">
<li><p>Modify the default instructions to be relevant to your use case.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>   <span class="n">instructions_to_reviewer</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;## Instructions for Testing the </span><span class="si">{</span><span class="n">RAG_APP_NAME</span><span class="si">}</span><span class="s2">&#39;s Initial Proof of Concept (PoC)</span>

<span class="s2">   Your inputs are invaluable for the development team. By providing detailed feedback and corrections, you help us fix issues and improve the overall quality of the application. We rely on your expertise to identify any gaps or areas needing enhancement.</span>

<span class="s2">   1. **Variety of Questions**:</span>
<span class="s2">      - Please try a wide range of questions that you anticipate the end users of the application will ask. This helps us ensure the application can handle the expected queries effectively.</span>

<span class="s2">   2. **Feedback on Answers**:</span>
<span class="s2">      - After asking each question, use the feedback widgets provided to review the answer given by the application.</span>
<span class="s2">      - If you think the answer is incorrect or could be improved, please use &quot;Edit Answer&quot; to correct it. Your corrections will enable our team to refine the application&#39;s accuracy.</span>

<span class="s2">   3. **Review of Returned Documents**:</span>
<span class="s2">      - Carefully review each document that the system returns in response to your question.</span>
<span class="s2">      - Use the thumbs up/down feature to indicate whether the document was relevant to the question asked. A thumbs up signifies relevance, while a thumbs down indicates the document was not useful.</span>

<span class="s2">   Thank you for your time and effort in testing </span><span class="si">{</span><span class="n">RAG_APP_NAME</span><span class="si">}</span><span class="s2">. Your contributions are essential to delivering a high-quality product to our end users.&quot;&quot;&quot;</span>

   <span class="nb">print</span><span class="p">(</span><span class="n">instructions_to_reviewer</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Run the deployment cell to get a link to the Review App.</p></li>
</ol>
<img alt="../_images/9_img.png" class="align-center" src="../_images/9_img.png" />
<ol class="arabic simple" start="6">
<li><p>Grant individual users permissions to access the Review App.</p></li>
</ol>
<img alt="../_images/10_img.png" class="align-center" src="../_images/10_img.png" />
<ol class="arabic simple" start="7">
<li><p>Test the Review App by asking a few questions yourself and providing feedback.</p>
<ul class="simple">
<li><p>You can view the data in Delta Tables. Note that results can take up to 2 hours to appear in the Delta Tables.</p></li>
</ul>
</li>
</ol>
<p>Parameters and their default values configured in 00_config:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Knob</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Default value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference internal" href="3-deep-dive.html#query-understanding"><span class="std std-ref">Query understanding</span></a></p></td>
<td><p>Analyzing and transforming user queries to better represent intent and extract relevant information, such as filters or keywords, to improve the retrieval process.</p></td>
<td><p>None, the provided query is directly embedded.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="3-deep-dive.html#retrieval"><span class="std std-ref">Retrieval</span></a></p></td>
<td><p>Finding the most relevant chunks of information given a retrieval query. In the unstructured data case, this typically involves one or a combination of semantic or keyword-based search.</p></td>
<td><p>Semantic search with K = 5 chunks retrieved</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="3-deep-dive.html#prompt-augmentation"><span class="std std-ref">Prompt augmentation</span></a></p></td>
<td><p>Combining a user query with retrieved information and instructions to guide the LLM towards generating high-quality responses.</p></td>
<td><p>A simple RAG prompt template</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="3-deep-dive.html#llm"><span class="std std-ref">LLM</span></a></p></td>
<td><p>Selecting the most appropriate model (and model parameters) for your application to optimize/balance performance, latency, and cost.</p></td>
<td><p>Databricks-dbrx-instruct hosted using Databricks FMAPI pay-per-token</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="3-deep-dive.html#post-processing-guardrails"><span class="std std-ref">Post processing &amp; guardrails</span></a></p></td>
<td><p>Applying additional processing steps and safety measures to ensure the LLM-generated responses are on-topic, factually consistent, and adhere to specific guidelines or constraints.</p></td>
<td><p>None</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="share-the-review-app-with-stakeholders">
<h4>5. Share the Review App with stakeholders<a class="headerlink" href="#share-the-review-app-with-stakeholders" title="Link to this heading">#</a></h4>
<p>You can now share your POC RAG application with your stakeholders to get their feedback.</p>
<p>We suggest distributing your POC to at least 3 stakeholders and having them each ask 10 - 20 questions. It is important to have multiple stakeholders test your POC so you can have a diverse set of perspectives to include in your Evaluation Set.</p>
</section>
</section>
</section>
<section id="evaluate-the-poc-s-quality">
<h2>Evaluate the POC‚Äôs quality<a class="headerlink" href="#evaluate-the-poc-s-quality" title="Link to this heading">#</a></h2>
<img alt="../_images/11_img.png" class="align-center" src="../_images/11_img.png" />
<p><strong>Expected time:</strong> 30-60 minutes</p>
<p><strong>Requirements:</strong></p>
<ul class="simple">
<li><p>Stakeholders have used your POC and provided feedback</p></li>
<li><p>All requirements from <a class="reference internal" href="#how-to-build-a-poc"><span class="xref myst">POC step</span></a></p>
<ul>
<li><p>Data from your <a class="reference internal" href="#requirements-questions"><span class="xref myst">requirements</span></a> is available in your <a class="reference external" href="https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html">Lakehouse</a> inside a <a class="reference external" href="https://www.databricks.com/product/unity-catalog">Unity Catalog</a> <a class="reference external" href="https://docs.databricks.com/en/connect/unity-catalog/volumes.html">volume</a> or <a class="reference external" href="https://docs.databricks.com/en/delta/index.html">Delta Table</a></p></li>
<li><p>Access to a <a class="reference external" href="https://docs.databricks.com/en/generative-ai/vector-search.html">Mosaic AI Vector Search</a> endpoint [<a class="reference external" href="https://docs.databricks.com/en/generative-ai/create-query-vector-search.html">instructions</a>]</p></li>
<li><p>Write access to Unity Catalog schema</p></li>
<li><p>A single-user cluster with DBR 14.3+</p></li>
</ul>
</li>
</ul>
<p>Now that your stakeholders have used your POC, we can use their feedback to measure the POC‚Äôs quality and establish a baseline.</p>
<section id="etl-the-logs-to-an-evaluation-set-run-evaluation">
<h3>1. ETL the logs to an Evaluation Set &amp; run evaluation<a class="headerlink" href="#etl-the-logs-to-an-evaluation-set-run-evaluation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Open the <code class="docutils literal notranslate"><span class="pre">04_evaluate_poc_quality</span></code> Notebook.</p></li>
<li><p>Adjust the configuration at the top to point to your Review App‚Äôs logs.</p></li>
<li><p>Run the cell to create an initial Evaluation Set that includes</p>
<ul class="simple">
<li><p>3 types of logs</p>
<ol class="arabic simple">
<li><p>Requests with a üëç :</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">request</span></code>: As entered by the user</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">expected_response</span></code>: If the user edited the response, that is used, otherwise, the model‚Äôs generated response.</p></li>
</ul>
</li>
<li><p>Requests with a üëé :</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">request</span></code>: As entered by the user</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">expected_response</span></code>: If the user edited the response, that is used, otherwise, null.</p></li>
</ul>
</li>
<li><p>Requests without any feedback</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">request</span></code>: As entered by the user</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Across all types of requests, if the user üëç a chunk from the <code class="docutils literal notranslate"><span class="pre">retrieved_context</span></code>, the <code class="docutils literal notranslate"><span class="pre">doc_uri</span></code> of that chunk is included in <code class="docutils literal notranslate"><span class="pre">expected_retrieved_context</span></code> for the question.</p></li>
</ul>
</li>
</ol>
<blockquote>
<div><p>note: Databricks recommends that your Evaluation Set contains at least 30 questions to get started.</p>
</div></blockquote>
<ol class="arabic simple" start="4">
<li><p>Inspect the Evaluation Set to understand the data that is included. You need to validate that your Evaluation Set contains a representative and challenging set of questions.</p></li>
<li><p>Optionally, save your evaluation set to a Delta Table for later use</p></li>
<li><p>Evaluate the POC with Quality Lab‚Äôs LLM Judge-based evaluation. Open MLflow to view the results.</p></li>
</ol>
<img alt="../_images/12_img.png" class="align-center" src="../_images/12_img.png" />
<img alt="../_images/13_img.png" class="align-center" src="../_images/13_img.png" />
</section>
<section id="review-evaluation-results">
<h3>2. Review evaluation results<a class="headerlink" href="#review-evaluation-results" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Now, let‚Äôs open MLflow to inspect the results.</p></li>
<li><p>In the Run tab, we can see each of the computed metrics. Refer to [metrics overview] section for an explanation of what each metric tells you about your application.</p></li>
<li><p>In the Evaluation tab, we can inspect the questions, RAG application‚Äôs outputs, and each of the LLM judge‚Äôs assessments.</p></li>
</ol>
<p>Now that you have a baseline understanding of the POC‚Äôs quality, we can shift focus to identifying the root causes of any quality issues and iteratively improving the app.</p>
<p>It is worth noting: if the results meet your requirements for quality, you can skip directly to the Deployment section.</p>
</section>
</section>
<section id="improve-rag-quality">
<h2>Improve RAG quality<a class="headerlink" href="#improve-rag-quality" title="Link to this heading">#</a></h2>
<img alt="../_images/14_img.png" class="align-center" src="../_images/14_img.png" />
<p>While a basic RAG chain is relatively straightforward to implement, refining it to consistently produce high-quality outputs is often non-trivial. Identifying the root causes of issues and determining which levers of the solution to pull to improve output quality requires understanding the various components and their interactions.</p>
<p>Simply vectorizing a set of documents, retrieving them via semantic search, and passing the retrieved documents to an LLM is not sufficient to guarantee optimal results. To yield high-quality outputs, you need to consider factors such as (but not limited to) chunking strategy of documents, choice of LLM and model parameters, or whether to include a query understanding step. As a result, ensuring high quality RAG outputs will generally involve iterating over both the data pipeline (e.g., chunking) and the RAG chain itself (e.g., choice of LLM).</p>
<p>This section is divided into 3 steps:</p>
<ol class="arabic simple">
<li><p>Understand RAG quality improvement levers</p></li>
<li><p>Identifying the root cause of quality issues</p></li>
<li><p>Implementing and evaluating fixes to the identified root cause</p></li>
</ol>
<section id="step-1-understand-rag-quality-improvement-levers">
<h3><strong>Step 1:</strong> Understand RAG quality improvement levers<a class="headerlink" href="#step-1-understand-rag-quality-improvement-levers" title="Link to this heading">#</a></h3>
<p>From a conceptual point of view, it‚Äôs helpful to view RAG quality issues through the lens of two key aspects:</p>
<ul class="simple">
<li><p><strong>Retrieval quality</strong></p>
<ul>
<li><p>Are you retrieving the most relevant information for a given retrieval query?</p>
<ul>
<li><p>It‚Äôs difficult to generate high quality RAG output if the context provided to the LLM is missing important information or contains superfluous information.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Generation quality</strong></p>
<ul>
<li><p>Given the retrieved information and the original user query, is the LLM generating the most accurate, coherent, and helpful response possible?</p>
<ul>
<li><p>Issues here can manifest as hallucinations, inconsistent output, or failure to directly address the user query.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>From an implementation standpoint, we can divide our RAG solution into two components which can be iterated on to address quality challenges:</p>
<p><a class="reference internal" href="3-deep-dive.html#data-pipeline-1"><span class="std std-ref"><strong>Data pipeline</strong></span></a></p>
<img alt="../_images/15_img.png" class="align-center" src="../_images/15_img.png" />
<ul class="simple">
<li><p>What is the composition of the input data corpus?</p></li>
<li><p>How raw data is extracted and transformed into a usable format (e.g., parsing a PDF document)</p></li>
<li><p>How documents are split into smaller chunks and how those chunks are formatted (e.g., chunking strategy, chunk size)</p></li>
<li><p>What metadata (e.g., section title, document title) is extracted about each document/chunk? How is this metadata included (or not included) in each chunk?</p></li>
<li><p>Which embedding model is used to convert text into vector representations for similarity search</p></li>
</ul>
<p><a class="reference internal" href="3-deep-dive.html#rag-chain"><span class="std std-ref"><strong>RAG chain</strong></span></a></p>
<img alt="../_images/16_img.png" class="align-center" src="../_images/16_img.png" />
<ul class="simple">
<li><p>The choice of LLM and its parameters (e.g., temperature, max tokens)</p></li>
<li><p>The retrieval parameters (e.g., number of chunks/documents retrieved)</p></li>
<li><p>The retrieval approach (e.g., keyword vs. hybrid vs. semantic search, rewriting the user‚Äôs query, transforming a user‚Äôs query into filters, re-ranking)</p></li>
<li><p>How to format the prompt with retrieved context, to guide the LLM towards desired output</p></li>
</ul>
<p>It‚Äôs tempting to assume a clean division between retrieval issues (simply update the data pipeline) and generation issues (update the RAG chain). However, the reality is more nuanced. Retrieval quality can be influenced by <em>both</em> the data pipeline (e.g., parsing/chunking strategy, metadata strategy, embedding model) and the RAG chain (e.g., user query transformation, number of chunks retrieved, re-ranking). Similarly, generation quality will invariably be impacted by poor retrieval (e.g., irrelevant or missing information affecting model output).</p>
<p>This overlap underscores the need for a holistic approach to RAG quality improvement. By understanding which components to change across both the data pipeline and RAG chain, and how these changes affect the overall solution, you can make targeted updates to improve RAG output quality.</p>
</section>
<section id="step-2-identify-the-root-cause-of-quality-issues">
<h3><strong>Step 2:</strong> Identify the root cause of quality issues<a class="headerlink" href="#step-2-identify-the-root-cause-of-quality-issues" title="Link to this heading">#</a></h3>
<section id="retrieval-quality">
<h4>Retrieval quality<a class="headerlink" href="#retrieval-quality" title="Link to this heading">#</a></h4>
<section id="debugging-retrieval-quality">
<h5>Debugging retrieval quality<a class="headerlink" href="#debugging-retrieval-quality" title="Link to this heading">#</a></h5>
<p>Retrieval quality is arguably the most important component of a RAG application. If the most relevant chunks are not returned for a given query, the LLM will not have access to the necessary information to generate a high-quality response. Poor retrieval can thus lead to irrelevant, incomplete, or hallucinated output.</p>
<p>As discussed in <a class="reference internal" href="4-evaluation.html"><span class="doc std std-doc">Section 4: Evaluation</span></a>, metrics such as precision and recall can be calculated using a set of evaluation queries and corresponding ground-truth chunks/documents. If evaluation results indicate that relevant chunks are not being returned, you will need to investigate further to identify the root cause. This step requires manual effort to analyze the underlying data. With Mosaic AI, this becomes considerably easier given the tight integration between the data platform (Unity Catalog and Vector Search), and experiment tracking (MLflow LLM evaluation and MLflow tracing).</p>
<p>Here‚Äôs a step-by-step process to address <strong>retrieval quality</strong> issues:</p>
<ol class="arabic simple">
<li><p>Identify a set of test queries with low retrieval quality metrics.</p></li>
<li><p>For each query, manually examine the retrieved chunks and compare them to the ground-truth retrieval documents.</p></li>
<li><p>Look for patterns or common issues among the queries with low retrieval quality. Some examples might include:</p>
<ul class="simple">
<li><p>Relevant information is missing from the vector database entirely</p></li>
<li><p>Insufficient number of chunks/documents returned for a retrieval query</p></li>
<li><p>Chunks are too small and lack sufficient context</p></li>
<li><p>Chunks are too large and contain multiple, unrelated topics</p></li>
<li><p>The embedding model fails to capture semantic similarity for domain-specific terms</p></li>
</ul>
</li>
<li><p>Based on the identified issue, hypothesize potential root causes and corresponding fixes. See the ‚Äú<a class="reference internal" href="#common-reasons-for-poor-retrieval-quality"><span class="xref myst">Common reasons for poor retrieval quality</span></a>‚Äù table below for guidance on this.</p></li>
<li><p>Implement the proposed fix for the most promising or impactful root cause, following <a class="reference internal" href="#step-3-implement-and-evaluate-changes"><span class="xref myst">step 3</span></a>. This may involve modifying the data pipeline (e.g., adjusting chunk size, trying a different embedding model) or the RAG chain (e.g., implementing hybrid search, retrieving more chunks).</p></li>
<li><p>Re-run the evaluation on the updated system and compare the retrieval quality metrics to the previous version. Once retrieval quality is at a desired level, proceed to evaluating generation quality (see <a class="reference internal" href="#debugging-generation-quality"><span class="xref myst">Debugging generation quality</span></a>).</p></li>
<li><p>If retrieval quality is still not satisfactory, repeat steps 4-6 for the next most promising fixes until the desired performance is achieved.</p></li>
</ol>
</section>
<section id="common-reasons-for-poor-retrieval-quality">
<h5>Common reasons for poor retrieval quality<a class="headerlink" href="#common-reasons-for-poor-retrieval-quality" title="Link to this heading">#</a></h5>
<p>Each of these potential fixes are tagged as one of three types. Based on the type of change, you will follow different steps in section 3.</p>
<!-- ![data-pipeline](../images/5-hands-on/data_pipeline.png) -->
<table>
<thead>
<tr>
<th>Retrieval Issue</th>
<th>Debugging Steps</th>
<th>Potential Fix</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chunks are too small</td>
<td><ul><li>Examine chunks for incomplete cut-off information</li></ul></td>
<td><ul><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/>  Increase chunk size and/or overlap</li><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Try different chunking strategy</li></ul></td>
</tr>
<tr>
<td>Chunks are too large</td>
<td><ul><li>Check if retrieved chunks contain multiple, unrelated topics</td>
<td><ul><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Decrease chunk size</li><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Improve chunking strategy to avoid mixture of unrelated topics (e.g., semantic chunking)</li></ul></td>
</tr>
<tr>
<td>Chunks don&#39;t have enough information about the text from which they were taken</td>
<td><ul><li>Assess if the lack of context for each chunk is causing confusion or ambiguity in the retrieved results</li></ul></td>
<td><ul><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Chain codeTry adding metadata &amp; titles to each chunk (e.g., section titles)</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Retrieve more chunks, and use an LLM with larger context size</li></ul></td>
</tr>
<tr>
<td>Embedding model doesn&#39;t accurately understand the domain and/or key phrases in user queries</td>
<td><ul><li>Check if semantically similar chunks are being retrieved for the same query</li></ul></td>
<td><ul><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Try different embedding models</li><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Fine-tune embedding model on domain-specific data</li></ul></td>
</tr>
<tr>
<td>Limited retrieval quality due to embedding model&#39;s lack of domain understanding</td>
<td><ul><li>Look at retrieved results to check if they are semantically relevant but miss key domain-specific information</li></ul></td>
<td><ul><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Hybrid search</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Over-fetch retrieval results, and re-rank. Only feed top re-ranked results into the LLM context</li></ul></td>
</tr>
<tr>
<td>Relevant information missing from the vector database</td>
<td><ul><li>Check if any relevant documents or sections are missing from the vector database</li></ul></td>
<td><ul><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Add more relevant documents to the vector database</li><li><img src="../_images/data_pipeline.png" alt="data-pipeline" height="20"/> Improve document parsing and metadata extraction</li></ul></td>
</tr>
<tr>
<td>Retrieval queries are poorly formulated</td>
<td><ul><li>If user queries are being directly used for semantic search, analyze these queries and check for ambiguity, or lack of specificity. This can happen easily in multi-turn conversations where the raw user query references previous parts of the conversation, making it unsuitable to use directly as a retrieval query.</li><li>Check if query terms match terminology used in the search corpus</li></ul></td>
<td><ul><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Add query expansion or transformation approaches (i.e., given a user query, transform the query prior to semantic search)</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Add query understanding to identify intent and entities (e.g., use an LLM to extract properties to use in metadata filtering)</li></ul></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="generation-quality">
<h4>Generation quality<a class="headerlink" href="#generation-quality" title="Link to this heading">#</a></h4>
<section id="debugging-generation-quality">
<h5>Debugging generation quality<a class="headerlink" href="#debugging-generation-quality" title="Link to this heading">#</a></h5>
<p>Even with optimal retrieval, if the LLM component of a RAG chain cannot effectively utilize the retrieved context to generate accurate, coherent, and relevant responses, the final output quality will suffer. Issues with generation quality can arise as hallucinations, inconsistencies, or failure to concisely address the user‚Äôs query, to name a few.</p>
<p>To identify generation quality issues, you can use the approach outlined in the <a class="reference internal" href="#section-4-evaluation"><span class="xref myst">Evaluation section</span></a>. If evaluation results indicate poor generation quality (e.g., low accuracy, coherence, or relevance scores), you‚Äôll need to investigate further to identify the root cause.</p>
<p>The following is a step-by-step process to address <strong>generation quality</strong> issues:</p>
<ol class="arabic simple">
<li><p>Identify a set of test queries with low generation quality metrics.</p></li>
<li><p>For each query, manually examine the generated response and compare it to the retrieved context and the ground-truth response.</p></li>
<li><p>Look for patterns or common issues among the queries with low generation quality. Some examples:</p>
<ul class="simple">
<li><p>Generating information not present in the retrieved context or outputting contradicting information with respect to the retrieved context (i.e., hallucination)</p></li>
<li><p>Failure to directly address the user‚Äôs query given the provided retrieved context</p></li>
<li><p>Generating responses that are overly verbose, difficult to understand or lack logical coherence</p></li>
</ul>
</li>
<li><p>Based on the identified issues, hypothesize potential root causes and corresponding fixes. See the ‚Äú<a class="reference internal" href="#common-reasons-for-poor-generation-quality"><span class="xref myst">Common reasons for poor generation quality</span></a>‚Äù table below for guidance.</p></li>
<li><p>Implement the proposed fix for the most promising or impactful root cause. This may involve modifying the RAG chain (e.g., adjusting the prompt template, trying a different LLM) or the data pipeline (e.g., adjusting the chunking strategy to provide more context).</p></li>
<li><p>Re-run evals on the updated system and compare generation quality metrics to the previous version. If there is significant improvement, consider deploying the updated RAG application for further testing with end-users (see the <a class="reference internal" href="#deployment"><span class="xref myst">Deployment</span></a> section).</p></li>
<li><p>If the generation quality is still not satisfactory, repeat steps 4-6 for the next most promising fix until the desired performance is achieved.</p></li>
</ol>
</section>
<section id="common-reasons-for-poor-generation-quality">
<h5>Common reasons for poor generation quality<a class="headerlink" href="#common-reasons-for-poor-generation-quality" title="Link to this heading">#</a></h5>
<p>Each of these potential fixes are tagged as one of three types. Based on the type of change, you will follow different steps in section 3.</p>
<table>
<thead>
<tr>
<th>Generation Issue</th>
<th>Debugging Steps</th>
<th>Potential Fix</th>
</tr>
</thead>
<tbody>
<tr>
<td>Generating information not present in the retrieved context (e.g., hallucinations)</td>
<td><ul><li>Compare generated responses to retrieved context to identify hallucinated information</li><li>Assess if certain types of queries or retrieved context are more prone to hallucinations</td>
<td><ul><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Update prompt template to emphasize reliance on retrieved context</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Use a more capable LLM</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Implement a fact-checking or verification step post-generation</td>
</tr>
<tr>
<td>Failure to directly address the user&#39;s query or providing overly generic responses</td>
<td><ul><li>Compare generated responses to user queries to assess relevance and specificity</li><li>Check if certain types of queries result in the correct context being retrieved, but the LLM producing low quality output</td>
<td><ul><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Improve prompt template to encourage direct, specific responses</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Retrieve more targeted context by improving the retrieval process</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Re-rank retrieval results to put most relevant chunks first, only provide these to the LLM</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Use a more capable LLM</td>
</tr>
<tr>
<td>Generating responses that are difficult to understand or lack logical flow</td>
<td><ul><li>Assess output for logical flow, grammatical correctness, and understandability</li><li>Analyze if incoherence occurs more often with certain types of queries or when certain types of context are retrieved</td>
<td><ul><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Change prompt template to encourage coherent, well-structured response</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Provide more context to the LLM by retrieving additional relevant chunks</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Use a more capable LLM</td>
</tr>
<tr>
<td>Generated responses are not in the desired format or style</td>
<td><ul><li>Compare output to expected format and style guidelines</li><li>Assess if certain types of queries or retrieved context are more likely to result in format/style deviations</td>
<td><ul><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Update prompt template to specify the desired output format and style</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Implement a post-processing step to convert the generated response into the desired format</li><li><img src="../_images/chain_code.png" alt="chain-code" height="20"/> Add a step to validate output structure/style, and output a fallback answer if needed.</li><li><img src="../_images/chain_config.png" alt="chain-config" height="20"/> Use an LLM fine-tuned to provide outputs in a specific format or style</td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="step-3-implement-and-evaluate-changes">
<h3><strong>Step 3:</strong> Implement and evaluate changes<a class="headerlink" href="#step-3-implement-and-evaluate-changes" title="Link to this heading">#</a></h3>
<p>As discussed above, when working to improve the quality of the RAG system, changes can be broadly categorized into three buckets:</p>
<ol class="arabic simple">
<li><p><strong><img alt="Data pipeline" src="../_images/data_pipeline.png" /></strong> Data pipeline changes</p></li>
<li><p><strong><img alt="Chain config" src="../_images/chain_config.png" /></strong> RAG chain configuration changes</p></li>
<li><p><strong><img alt="Chain code" src="../_images/chain_code.png" /></strong> RAG chain code changes</p></li>
</ol>
<p>Depending on the specific issue you are trying to address, you may need to apply changes to one or both of these components. In some cases, simultaneous changes to both the data pipeline and RAG chain may be necessary to achieve the desired quality improvements.</p>
<section id="data-pipeline-changes">
<h4>Data pipeline changes<a class="headerlink" href="#data-pipeline-changes" title="Link to this heading">#</a></h4>
<p><strong>Data pipeline changes</strong> involve modifying how input data is processed, transformed, or stored before being used by the RAG chain. Examples of data pipeline changes include (and are not limited to):</p>
<ul class="simple">
<li><p>Trying a different chunking strategy</p></li>
<li><p>Iterating on the document parsing process</p></li>
<li><p>Changing the embedding model</p></li>
</ul>
<p>Implementing a data pipeline change will generally require re-running the entire pipeline to create a new vector index. This process involves reprocessing the input documents, regenerating the vector embeddings, and updating the vector index with new embeddings and metadata.</p>
</section>
<section id="rag-chain-changes">
<h4>RAG chain changes<a class="headerlink" href="#rag-chain-changes" title="Link to this heading">#</a></h4>
<p><strong>RAG chain changes</strong> involve modifying steps or parameters of the RAG chain itself, without necessarily changing the underlying vector database. Examples of RAG chain changes include (and are not limited to):</p>
<ul class="simple">
<li><p>Changing the LLM</p></li>
<li><p>Modifying the prompt template</p></li>
<li><p>Adjusting the retrieval component (e.g., number of retrieval chunks, reranking, query expansion)</p></li>
<li><p>Introducing additional processing steps such as a query understanding step</p></li>
</ul>
<p>RAG chain updates may involve editing the <strong>RAG chain configuration file</strong> (e.g., changing the LLM parameters or prompt template), <em>or</em> modifying the actual <strong>RAG chain code</strong> (e.g., adding new processing steps or retrieval logic).</p>
</section>
<section id="testing-a-potential-fix-that-could-improve-quality">
<h4>Testing a potential fix that could improve quality<a class="headerlink" href="#testing-a-potential-fix-that-could-improve-quality" title="Link to this heading">#</a></h4>
<p>Once you have identified a potential fix based on the debugging process outlined above, follow these steps to test your changes:</p>
<ol class="arabic">
<li><p>Make the necessary changes to the data pipeline or RAG chain code</p>
<ul class="simple">
<li><p>See the <a class="reference internal" href="#code-examples"><span class="xref myst">code examples</span></a> below for how and where to make these changes</p></li>
<li><p>If required, re-run the data pipeline to update the vector index with the new embeddings and metadata</p></li>
</ul>
</li>
<li><p>Log a new version of your chain to MLflow</p>
<ul>
<li><p>Ensure that any config files (i.e., for both your data pipeline and RAG chain) are logged to the MLflow run</p>
<ul>
<li><SCREENSHOT OF LOGGED CHAIN>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Run evaluation on this new chain</p></li>
<li><p>Review evaluation results</p>
<ul>
<li><p>Analyze the evaluation metrics to determine if there has been an improvement the RAG chain‚Äôs performance</p>
<ul>
<li><SCREENSHOT OF EVALS>
</li>
</ul>
</li>
<li><p>Compare the traces and LLM judge results for individual queries before and after the changes to gain insights into the impact of your changes</p></li>
</ul>
</li>
<li><p>Iterate on the fixes</p>
<ul class="simple">
<li><p>If the evaluation results do not show the desired improvement, iterate on your changes based on the insights gained from analysis.</p></li>
<li><p>Repeat steps 1-4 until you are satisfied with the improvement in the RAG chain‚Äôs output quality</p></li>
</ul>
</li>
<li><p>Deploy the updated RAG chain for user feedback</p>
<ul>
<li><p>Once evaluation results indicate improvement, register the chain to Unity Catalog and deploy the updated RAG chain via the Review App.</p></li>
<li><p>Gather feedback from stakeholders and end-users through one or both of the following:</p>
<ul>
<li><p>Have stakeholders interact with the app directly in the RAG Studio UI and provide feedback on response quality</p>
<ul>
<li><SCREENSHOT OF REVIEW APP>
</li>
</ul>
</li>
<li><p>Generate responses using the updated chain for the set of evaluation queries and seek feedback on those specific responses</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Monitor and analyze user feedback</p>
<ul class="simple">
<li><p>Review these results using a <a class="reference external" href="https://docs.databricks.com/en/dashboards/index.html#dashboards">dashboard</a>.</p></li>
<li><p>Monitor metrics such as the percentage of positive and negative feedback, as well as any specific comments or issues raised by users.</p></li>
</ul>
</li>
</ol>
<!--
### Code Examples
| | Component | Change(s) |
|---|---|---|
| **Data pipeline changes**<br><br>1. Re-run data pipeline to create new vector index<br>2. Log new version of RAG chain using the updated index<br>3. Run evals on new chain | [Parser](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/02_parse_docs.py) | - Change parsing strategy<br>  - [Add new parsing strategy to notebook](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/parser_library.py)<br>  - [Update data pipeline config](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/00_config.py#L29) |
| | [Chunking](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/03_chunk_docs.py) | - Chunking strategy<br>  - [Add or update existing chunking strategy](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/chunker_library.py)<br>  - [Update data pipeline config](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/00_config.py#L30-L35)<br>- Change chunk sizes of existing chunking strategy<br>  - [Update data pipeline config](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/00_config.py#L30-L35)<br>- Add metadata to chunks<br>- Semantic chunking |
| | [Embedding<br>model](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/04_vector_index.py#L41) | - Change embedding model<br>  - [Update data pipeline config](https://github.com/databricks-field-eng/field-ai-examples/blob/main/dev/data_processing/notebook_version/data_prep/00_config.py#L18-L24) |
| **RAG chain config changes**<br><br>1. If no changes to data pipeline, do *not* re-run data pipeline<br>2. Log new version of RAG chain using the updated index<br>3. Run evals on new chain | [LLM](#llm) | - Change LLM or its parameters<br>  - [Update RAG chain config](https://github.com/epec254/rag_code/blob/main/RAG%20Cookbook/B_pdf_rag_with_multi_turn_chat/2_rag_chain_config.yaml#L1-L4) |
| | [Prompt<br>Template](/nbs/3-deep-dive.md#prompt-augmentation) | - Iterate on prompt template<br>  - [Update RAG chain config](https://github.com/epec254/rag_code/blob/main/RAG%20Cookbook/B_pdf_rag_with_multi_turn_chat/2_rag_chain_config.yaml#L5-L14) |
| | [Hybrid search](/nbs/3-deep-dive.md#retrieval) | - Try hybrid search instead of semantic search<br>  - Update RAG chain code |
| **RAG chain code changes**<br><br>1. If no changes to data pipeline, *do not* re-run data pipeline<br>2. Log new version of RAG chain using the updated index<br>3. Run evals on new chain | [Reranker](/nbs/3-deep-dive.md#retrieval) | - Add reranker step to RAG chain<br>  - [Update RAG chain code](https://github.com/epec254/rag_code/pull/19) |
| | [Query<br>Expansion](/nbs/3-deep-dive.md#query-understanding) | - Add query expansion step<br>  - Update RAG chain code<br>  - NOTE: Implement this [example prompt](https://docs.llamaindex.ai/en/stable/examples/query_transformations/query_transform_cookbook/#query-rewriting-custom) into the multi turn |
| | [Guardrails](/nbs/3-deep-dive.md#post-processing-guardrails) | - Add post-processing guardrails step to RAG chain<br>  - Create a version of this [chain](https://github.com/epec254/rag_code/tree/main/RAG%20Cookbook/B_pdf_rag_with_multi_turn_chat) that includes a sample guardrail prompt using the current advanced DBdemo as an example |
-->
</section>
</section>
</section>
<section id="id1">
<h2>Deployment<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<img alt="../_images/17_img.png" class="align-center" src="../_images/17_img.png" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation-driven-development">Evaluation-driven development</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gather-requirements">Gather requirements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#is-the-use-case-a-good-fit-for-rag">Is the use case a good fit for RAG?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#requirements-questions">Requirements questions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#user-experience">User Experience</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-constraints">Performance constraints</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluation">Evaluation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#security">Security</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deployment">Deployment</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#build-collect-feedback-on-poc">Build &amp; Collect Feedback on POC</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-build-a-poc">How to build a POC</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#import-the-sample-code">1. Import the sample code.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#configure-your-application">2. Configure your application</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#prepare-your-data">3. Prepare your data.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deploy-the-poc-chain-to-the-quality-lab-review-app">4. Deploy the POC chain to the Quality Lab Review App</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#share-the-review-app-with-stakeholders">5. Share the Review App with stakeholders</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-the-poc-s-quality">Evaluate the POC‚Äôs quality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#etl-the-logs-to-an-evaluation-set-run-evaluation">1. ETL the logs to an Evaluation Set &amp; run evaluation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#review-evaluation-results">2. Review evaluation results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improve-rag-quality">Improve RAG quality</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-understand-rag-quality-improvement-levers"><strong>Step 1:</strong> Understand RAG quality improvement levers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-identify-the-root-cause-of-quality-issues"><strong>Step 2:</strong> Identify the root cause of quality issues</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#retrieval-quality">Retrieval quality</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-retrieval-quality">Debugging retrieval quality</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons-for-poor-retrieval-quality">Common reasons for poor retrieval quality</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generation-quality">Generation quality</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#debugging-generation-quality">Debugging generation quality</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons-for-poor-generation-quality">Common reasons for poor generation quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-implement-and-evaluate-changes"><strong>Step 3:</strong> Implement and evaluate changes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-pipeline-changes">Data pipeline changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#rag-chain-changes">RAG chain changes</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-a-potential-fix-that-could-improve-quality">Testing a potential fix that could improve quality</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deployment</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Databricks GenAI Community
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>