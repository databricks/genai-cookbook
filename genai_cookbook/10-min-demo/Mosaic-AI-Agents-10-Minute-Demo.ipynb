{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3f7c14-b5dd-41d4-919b-caec7ae0e5ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Databricks notebook\n",
    "\n",
    "\n",
    "### **IMPORTANT** To run this in your Databricks workspace, import this Databricks Notebook URL to your workspace: [https://ai-cookbook.io/10-min-demo/mosaic-ai-agents-demo-dbx-notebook.html](https://ai-cookbook.io/10-min-demo/mosaic-ai-agents-demo-dbx-notebook.html)\n",
    "\n",
    "\n",
    "<br/><br/><br/><br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### TLDR; this notebook will:\n",
    "1. Deploy a RAG application built with [Agent Framework](https://docs.databricks.com/generative-ai/retrieval-augmented-generation.html) to the [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html) review application\n",
    "    - The review application is used by your business stakeholders to provide feedback on your app\n",
    "2. Evaluate the quality of the application with [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html) and MLflow\n",
    "    - These AI-assisted evaluations are used by developers to improve the application's quality\n",
    "\n",
    "#### Products used:\n",
    "- [**Mosaic AI Agent Framework**](https://docs.databricks.com/generative-ai/retrieval-augmented-generation.html) SDK to quickly and safely build high-quality RAG applications.\n",
    "- [**Mosaic AI Agent Evaluation**](https://docs.databricks.com/generative-ai/agent-evaluation/index.html) AI-assisted evaluation tool to determines if outputs are high-quality.  Provides an intuitive UI to get feedback from human stakeholders.\n",
    "- [**Mosaic AI Model Serving**](https://docs.databricks.com/generative-ai/deploy-agent.html) Hosts the application's logic as a production-ready, scalable REST API.\n",
    "- [**MLflow**](https://docs.databricks.com/mlflow/index.html) Tracks and manages the application lifecycle, including evaluation results and application code/config\n",
    "- [**Generative AI Cookbook**](https://ai-cookbook.io/) A definitive how-to guide, backed by a code repo, for building high-quality Gen AI apps, developed in partnership with Mosaic AIâ€™s research team.\n",
    "\n",
    "#### *Requires a [single-user](https://docs.databricks.com/en/compute/configure.html#access-modes) cluster running on DBR 14.3+*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y mlflow mlflow-skinny\n",
    "%pip install -U -qqqq databricks-agents mlflow mlflow-skinny databricks-vectorsearch databricks-sdk langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "329fe0cd-06b0-4e7f-9b0c-5b4fea891662",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup: Load the necessary data and code from the Databricks Cookbook repo\n",
    "\n",
    "The following cell clones the Generative AI cookbook repo from `https://github.com/databricks/genai-cookbook` into a folder `genai-cookbook` in the same folder as this notebook using a [Git Folder](https://docs.databricks.com/en/repos/index.html).  \n",
    "\n",
    "Alternatively, you can manually clone the Git repo `https://github.com/databricks/genai-cookbook` to a folder `genai-cookbook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fd92bc-9cd9-4a70-8124-531425fdc140",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk.core import DatabricksError\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "CURRENT_FOLDER = os.getcwd()\n",
    "QUICK_START_REPO_URL = \"https://github.com/databricks/genai-cookbook.git\"\n",
    "QUICK_START_REPO_SAVE_FOLDER = \"genai-cookbook\"\n",
    "\n",
    "if os.path.isdir(QUICK_START_REPO_SAVE_FOLDER):\n",
    "    raise Exception(\n",
    "        f\"{QUICK_START_REPO_SAVE_FOLDER} folder already exists, please change the variable QUICK_START_REPO_SAVE_FOLDER to be a non-existant path.\"\n",
    "    )\n",
    "\n",
    "# Clone the repo\n",
    "w = WorkspaceClient()\n",
    "try:\n",
    "    w.repos.create(\n",
    "        url=QUICK_START_REPO_URL, provider=\"github\", path=f\"{CURRENT_FOLDER}/{QUICK_START_REPO_SAVE_FOLDER}\"\n",
    "    )\n",
    "    print(f\"Cloned sample code repo to: {QUICK_START_REPO_SAVE_FOLDER}\")\n",
    "except DatabricksError as e:\n",
    "    if e.error_code == \"RESOURCE_ALREADY_EXISTS\":\n",
    "        print(\"Repo already exists. Skipping creation\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed to clone the quick start code.  You can manually import this by creating a Git folder from the contents of {QUICK_START_REPO_URL} in the {QUICK_START_REPO_SAVE_FOLDER} folder in your workspace and then re-running this Notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f8b9ac0-ada2-4dc0-925e-fe0a4556ac71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Application configuration\n",
    "\n",
    "We've selected defaults for the following parameters based on your user name, but inspect and change if you prefer to use existing resources.  Any missing resources will be created in the next step.\n",
    "\n",
    "1. `UC_CATALOG` & `UC_SCHEMA`: [Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#create-a-catalog) and a Schema where the output Delta Tables with the parsed/chunked documents and Vector Search indexes are stored\n",
    "2. `UC_MODEL_NAME`: Unity Catalog location to log and store the chain's model\n",
    "3. `VECTOR_SEARCH_ENDPOINT`: [Vector Search Endpoint](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-endpoint) to host the resulting vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c110ac04-46f4-4acc-9374-4fb8bc42354f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the current user name to create any necesary resources\n",
    "w = WorkspaceClient()\n",
    "user_name = w.current_user.me().user_name.split(\"@\")[0].replace(\".\", \"\")\n",
    "\n",
    "# UC Catalog & Schema where outputs tables/indexs are saved\n",
    "# If this catalog/schema does not exist, you need create catalog/schema permissions.\n",
    "UC_CATALOG = f'{user_name}_catalog'\n",
    "UC_SCHEMA = f'agent_demo'\n",
    "\n",
    "# UC Model name where the POC chain is logged\n",
    "UC_MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.doc_bot\"\n",
    "\n",
    "# Vector Search endpoint where index is loaded\n",
    "# If this does not exist, it will be created\n",
    "VECTOR_SEARCH_ENDPOINT = f'{user_name}_vector_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0008153-dad5-45da-9491-3f659964e43f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check if the UC Catalog & Vector Search endpoint exist; create otherwise\n",
    "\n",
    "The code is this cell checks if the resources exist, trying to create them if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c0d2d9c-fa28-4acb-ad1e-22c3b6257a23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointStatusState, EndpointType\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointStateReady\n",
    "from databricks.sdk.errors import ResourceDoesNotExist, NotFound, PermissionDenied\n",
    "import os\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create UC Catalog if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.catalogs.get(UC_CATALOG)\n",
    "    print(f\"PASS: UC catalog `{UC_CATALOG}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.catalogs.create(name=UC_CATALOG)\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}` does not exist, and no permissions to create.  Please provide an existing UC Catalog.\")\n",
    "        raise ValueError(f\"Unity Catalog `{UC_CATALOG}` does not exist.\")\n",
    "        \n",
    "# Create UC Schema if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.schemas.get(full_name=f\"{UC_CATALOG}.{UC_SCHEMA}\")\n",
    "    print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}.{UC_SCHEMA}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.schemas.create(name=UC_SCHEMA, catalog_name=UC_CATALOG)\n",
    "        print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` created\")\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}.{UC_SCHEMA}` does not exist, and no permissions to create.  Please provide an existing UC Schema.\")\n",
    "        raise ValueError(\"Unity Catalog Schema `{UC_CATALOG}.{UC_SCHEMA}` does not exist.\")\n",
    "\n",
    "# Create the Vector Search endpoint if it does not exist\n",
    "vector_search_endpoints = w.vector_search_endpoints.list_endpoints()\n",
    "if sum([VECTOR_SEARCH_ENDPOINT == ve.name for ve in vector_search_endpoints]) == 0:\n",
    "    print(f\"Please wait, creating Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}`.  This can take up to 20 minutes...\")\n",
    "    w.vector_search_endpoints.create_endpoint_and_wait(VECTOR_SEARCH_ENDPOINT, endpoint_type=EndpointType.STANDARD)\n",
    "\n",
    "# Make sure vector search endpoint is online and ready.\n",
    "w.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(VECTOR_SEARCH_ENDPOINT)\n",
    "\n",
    "print(f\"PASS: Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}` exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfe074a2-2b13-4fc4-af8b-56b79646017b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Build & deploy the application\n",
    "\n",
    "Below is a high-level overview of the architecture we will deploy:\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-basic.png?raw=true\" style=\"width: 800px; margin-left: 10px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecada3c6-1869-4f73-b8a9-dca7a1ec64b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-basic-prep-3.png?raw=true\" style=\"float: right; margin-left: 10px\" width=\"400px\">\n",
    "\n",
    "## 1/ Create the Vector Search Index\n",
    "\n",
    "First, we copy the sample data to a Delta Table and sync to a Vector Search index.  Here, we use the [gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) embedding model hosted on [Databricks Foundational Model APIs](https://docs.databricks.com/en/machine-learning/foundation-models/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc198cf9-ff4f-4c5d-8b1f-cf3a91d50c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC locations to store the chunked documents & index\n",
    "CHUNKS_DELTA_TABLE = f\"{UC_CATALOG}.{UC_SCHEMA}.databricks_docs_chunked\"\n",
    "CHUNKS_VECTOR_INDEX = f\"{UC_CATALOG}.{UC_SCHEMA}.databricks_docs_chunked_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "671baebf-809c-4251-a645-ab0954b87a60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Workspace URL for printing links to the delta table/vector index\n",
    "workspace_url = SparkSession.getActiveSession().conf.get(\n",
    "    \"spark.databricks.workspaceUrl\", None\n",
    ")\n",
    "\n",
    "# Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "# Load the chunked data to Delta Table & enable change-data capture to allow the table to sync to Vector Search\n",
    "chunked_docs_df = spark.read.parquet(\n",
    "    f\"file:{CURRENT_FOLDER}/{QUICK_START_REPO_SAVE_FOLDER}/quick_start_demo/chunked_databricks_docs.snappy.parquet\"\n",
    ")\n",
    "chunked_docs_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(CHUNKS_DELTA_TABLE)\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {CHUNKS_DELTA_TABLE} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"View Delta Table at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_DELTA_TABLE.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "# Embed and sync chunks to a vector index\n",
    "print(\n",
    "    f\"Embedding docs & creating Vector Search Index, this will take ~5 - 10 minutes.\\nView Index Status at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_VECTOR_INDEX.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "index = vsc.create_delta_sync_index_and_wait(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "    index_name=CHUNKS_VECTOR_INDEX,\n",
    "    primary_key=\"chunk_id\",\n",
    "    source_table_name=CHUNKS_DELTA_TABLE,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    embedding_source_column=\"chunked_text\",\n",
    "    embedding_model_endpoint_name=\"databricks-gte-large-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65140792-7797-41f1-8a25-6d0fef01fa89",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2/ Deploy to the review application\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/rag-basic-chain-1.png?raw=true\" style=\"float: right\" width=\"500px\">\n",
    "\n",
    "Now that our Vector Search index is ready, let's prepare the RAG chain and deploy it to the review application backed by a scalable-production ready REST API on Model serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a55621-08dd-4989-b41c-853f1d1eafb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Configuring our Chain parameters\n",
    "\n",
    "Databricks makes it easy to parameterize your chain with MLflow Model Configurations. Later, you can tune application quality by adjusting these parameters, such as the system prompt or retrieval settings.  Most applications will include many more parameters, but for this demo, we'll keep the configuration to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f159652-01ab-4f52-b566-7c8d481cc5e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": \"databricks-dbrx-instruct\",  # the foundation model we want to use\n",
    "    \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT,  # Endoint for vector search\n",
    "    \"vector_search_index\": f\"{CHUNKS_VECTOR_INDEX}\",\n",
    "    \"llm_prompt_template\": \"\"\"You are an assistant that answers questions. Use the following pieces of retrieved context to answer the question. Some pieces of context may be irrelevant, in which case you should not use them to form the answer.\\n\\nContext: {context}\"\"\", # LLM Prompt template\n",
    "}\n",
    "\n",
    "# Here, we define an input example in the schema required by Agent Framework\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What is Retrieval-augmented Generation?\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249795a5-6061-42ca-996e-57e891288f4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Log the application & view trace\n",
    "\n",
    "We first register the chain as an MLflow model and inspect the MLflow Trace to understand what is happening inside the chain.\n",
    "\n",
    "#### MLflow trace\n",
    "<br/>\n",
    "<img src=\"https://ai-cookbook.io/_images/mlflow_trace2.gif\" width=\"80%\" style=\"margin-left: 10px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "949a9787-b1d0-4c9d-a521-e6a386e43a90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"databricks-docs-bot\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(\n",
    "            os.getcwd(),\n",
    "            f\"{QUICK_START_REPO_SAVE_FOLDER}/quick_start_demo/sample_rag_chain\",\n",
    "        ),  # Chain code file from the quick start repo\n",
    "        model_config=chain_config,  # Chain configuration set above\n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=input_example,  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "    )\n",
    "\n",
    "# Test the chain locally to see the MLflow Trace\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71ea99f-cb80-4203-ac46-65558eead4ca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Deploy the application\n",
    "\n",
    "Now, we:\n",
    "1. Register the application in Unity Catalog\n",
    "2. Use Agent Framework to deploy to the Quality Lab review application\n",
    "\n",
    "Along side the review ap, a scalable, production-ready Model Serving endpoint is also deployed.\n",
    "\n",
    "#### Agent Evaluation review application\n",
    "<img src=\"https://ai-cookbook.io/_images/review_app2.gif\" width=\"90%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a41c38d-dbe0-4455-9f4e-deb1fdfff83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import time\n",
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "\n",
    "# Use Unity Catalog to log the chain\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=UC_MODEL_NAME)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)\n",
    "\n",
    "# Wait for the Review App to be ready\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 10 - 20 minutes.\", end=\"\")\n",
    "while w.serving_endpoints.get(deployment_info.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment_info.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a41ed0-170d-48b4-a1f3-cd8ad7ed1215",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3/ Use Agent Evaluation to evaluate your application\n",
    "\n",
    "## 3.1/ Have stakeholders chat your bot to build your evaluation dataset\n",
    "\n",
    "Normally, you would now give access to internal domain experts and have them test and review the bot.  **Your domain experts do NOT need to have Databricks Workspace access** - you can assign permissions to any user in your SSO if you have enabled [SCIM](https://docs.databricks.com/en/admin/users-groups/scim/index.html)\n",
    "\n",
    "This is a critical step to build or improve your evaluation dataset: have users ask questions to your bot, and provide the bot with output answer when they don't answer properly.\n",
    "\n",
    "Your applicaation is automatically capturing all stakeholder questions and bot responses, including the MLflow Trace for each, into Delta Tables in your Lakehouse. On top of that, Databricks makes it easy to track feedback from your end user: if the chatbot doesn't give a good answer and the user gives a thumbdown, their feedback is included in the Delta Tables.\n",
    "\n",
    "Your evaluation dataset forms the basis of your development workflow to improve quality: identifying the root causes of quality issues and then objectively measuring the impact of your fixes.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"https://ai-cookbook.io/_images/review_app2.gif\" style=\"margin-left: 10px\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0ba714-6875-487e-aaa4-c4f49c092e2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2/ Run Evaluation of your Chain\n",
    "\n",
    "Now, let's use everage Agent Evaluation's specialized AI evaluators to evaluate our model performance.  Agent Evaluation is integrated into `mlflow.evaluate(...)`, all you need to do is pass `model_type=\"databricks-agent\"`.\n",
    "\n",
    "For this demo, we use a toy 10 question evaluation dataset.  Read more about our [best practices](https://ai-cookbook.io/nbs/4-evaluation-eval-sets.html) on the size of your evaluation dataset.\n",
    "\n",
    "<img src=\"https://ai-cookbook.io/_images/mlflow-eval-agent.gif\" style=\"margin-left: 10px\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4dff818-995f-4bfe-bfac-c453bc8e1f97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_eval_set = [\n",
    "    {\n",
    "        \"request_id\": \"5482\",\n",
    "        \"request\": \"What happens if I try to access an index that is out of bounds in an array using the [ ] operator in Databricks SQL when spark.sql.ansi.enabled is set to false?\",\n",
    "        \"expected_response\": \"If you try to access an index that is out of bounds in an array using the [ ] operator in Databricks SQL when spark.sql.ansi.enabled is set to false, Databricks will return NULL instead of raising an error.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"2112\",\n",
    "        \"request\": \"Why is a long-running stage in my Spark job only showing one task, and how can I resolve this issue?\",\n",
    "        \"expected_response\": \"A long-running stage with one task in a Spark job could be due to several reasons such as:\\n\\n1. Expensive User Defined Functions (UDFs) on small data\\n2. Window function without a PARTITION BY statement\\n3. Reading from an unsplittable file type like gzip\\n4. Setting the multiLine option when reading a JSON or CSV file\\n5. Schema inference of a large file\\n6. Use of repartition(1) or coalesce(1)\\n\\nTo resolve this issue, you can:\\n\\n1. Optimize your UDFs or replace them with built-in functions if possible.\\n2. Ensure that you have a proper PARTITION BY statement in your window functions.\\n3. Avoid using unsplittable file types like gzip. Instead, use splittable file types like snappy or lz4.\\n4. Avoid setting the multiLine option when reading JSON or CSV files.\\n5. Perform schema inference on a small sample of your data and then apply it to the entire dataset.\\n6. Avoid using repartition(1) or coalesce(1) unless necessary.\\n\\nBy implementing these changes, you should be able to resolve the issue of a long-running stage with only one task in your Spark job.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"5054\",\n",
    "        \"request\": \"How can I represent 4-byte single-precision floating point numbers in Databricks SQL and what are their limits?\",\n",
    "        \"expected_response\": \"4-byte single-precision floating point numbers can be represented in Databricks SQL using the `FLOAT` or `REAL` syntax. The range of numbers that can be represented is from -3.402E+38 to +3.402E+38, including negative infinity, positive infinity, and NaN (not a number). Here are some examples of how to represent these numbers:\\n\\n* `+1F` represents 1.0\\n* `5E10F` represents 5E10\\n* `5.3E10F` represents 5.3E10\\n* `-.1F` represents -0.1\\n* `2.F` represents 2.0\\n* `-5555555555555555.1F` represents -5.5555558E15\\n* `CAST(6.1 AS FLOAT)` represents 6.1\\n\\nNote that `FLOAT` is a base-2 numeric type, so the representation of base-10 literals may not be exact. If you need to accurately represent fractional or large base-10 numbers, consider using the `DECIMAL` type instead.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"2003\",\n",
    "        \"request\": \"How can I identify the reason for failing executors in my Databricks workspace, and what steps can I take to resolve memory issues?\",\n",
    "        \"expected_response\": \"1. Identify failing executors: In your Databricks workspace, navigate to the compute's Event log to check for any explanations regarding executor failures. Look for messages indicating spot instance losses or cluster resizing due to autoscaling. If using spot instances, refer to 'Losing spot instances' documentation. For autoscaling, refer to 'Learn more about cluster resizing' documentation.\\n\\n2. Check executor logs: If no information is found in the event log, go to the Spark UI and click the Executors tab. Here, you can access logs from failed executors to investigate further.\\n\\n3. Identify memory issues: If the above steps do not provide a clear reason for failing executors, it is likely a memory issue. To dig into memory issues, refer to the 'Spark memory issues' documentation.\\n\\n4. Resolve memory issues: To resolve memory issues, consider the following steps:\\n\\n   a. Increase executor memory: Allocate more memory to executors by adjusting the 'spark.executor.memory' property in your Spark configuration.\\n\\n   b. Increase driver memory: Allocate more memory to the driver by adjusting the 'spark.driver.memory' property in your Spark configuration.\\n\\n   c. Use off-heap memory: Enable off-heap memory by setting the 'spark.memory.offHeap.enabled' property to 'true' and allocating off-heap memory using the 'spark.memory.offHeap.size' property.\\n\\n   d. Optimize data processing: Review your data processing workflows and optimize them for memory efficiency. This may include reducing data shuffling, using broadcast variables, or caching data strategically.\\n\\n   e. Monitor memory usage: Monitor memory usage in your Databricks workspace to identify potential memory leaks or inefficient memory utilization. Use tools like the Spark UI, Ganglia, or Grafana to monitor memory usage.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(sample_eval_set)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e050b93-095d-4404-9187-9ad4b0252017",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=logged_chain_info.run_id):\n",
    "    # Evaluate\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_df,  # Your evaluation set\n",
    "        model=logged_chain_info.model_uri,  # previously logged model\n",
    "        model_type=\"databricks-agent\",  # activate Mosaic AI Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5532a082-fee6-4670-aec8-cea0b95f2122",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# What's next?\n",
    "\n",
    "\n",
    "## Code-based quick starts\n",
    "\n",
    "| Time required | Outcome | Link |\n",
    "|------ | ---- | ---- |\n",
    "| ðŸ•§ <br/> 10 minutes | Sample RAG app deployed to web-based chat app that collects feedback | âœ… |\n",
    "| ðŸ•§ðŸ•§ðŸ•§ <br/>60 minutes | POC RAG app with *your data* deployed to a chat UI that can collect feedback from your business stakeholders | [Deploy POC w/ your data](https://ai-cookbook.io/nbs/5-hands-on-build-poc.html)|\n",
    "| ðŸ•§ðŸ•§ <br/>30 minutes | Comprehensive quality/cost/latency evaluation of your POC app | - [Evaluate your POC](https://ai-cookbook.io/nbs/5-hands-on-evaluate-poc.html) <br/> - [Identify the root causes of quality issues](https://ai-cookbook.io/nbs/5-hands-on-improve-quality-step-1.html) |\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mosaic-AI-Agents-10-Minute-Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
