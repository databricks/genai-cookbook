# Databricks notebook source
# MAGIC %pip install -qqqq -U -r requirements.txt

# COMMAND ----------

dbutils.library.restartPython()

# COMMAND ----------

from cookbook.tools.vector_search import (
    VectorSearchRetrieverTool,
    VectorSearchSchema,
)
from cookbook.tools.uc_tool import UCTool

# COMMAND ----------

retriever_tool = VectorSearchRetrieverTool(
    name="search_product_docs",
    description="Use this tool to search for product documentation.",
    vector_search_index="dbdemos.dbdemos_rag_chatbot.databricks_documentation_vs_index",
    vector_search_schema=VectorSearchSchema(
        # These columns are the default values used in the `01_data_pipeline` notebook
        # If you used a different column names in that notebook OR you are using a pre-built vector index, update the column names here.
        chunk_text="content",  # Contains the text of each document chunk
        document_uri="url",  # The document URI of the chunk e.g., "/Volumes/catalog/schema/volume/file.pdf" - displayed as the document ID in the Review App
        # additional_metadata_columns=[],  # Additional columns to return from the vector database and present to the LLM
    )
)


translate_sku_tool = UCTool(uc_function_name="devanshu_pandey.cmhc_demo.vector_index_search_tool")

tools = [retriever_tool]

# COMMAND ----------

entry_point = dbutils.notebook.entry_point

host_name = f'https://{entry_point.getDbutils().notebook().getContext().browserHostName().get()}'

token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()

# COMMAND ----------

base_url = f"{host_name}/serving-endpoints/"
base_url

# COMMAND ----------

def is_termination_message(message):
    content = message.get("content", "")
    return (content and "TERMINATE" in content.upper()) or (message['role'] == 'user' and 'tool_calls' not in message)

# COMMAND ----------

from databricks.sdk import WorkspaceClient
from databricks.sdk.service.serving import ChatMessage, ChatMessageRole
from typing import List, Optional


class DatabricksModelServingClient:
    def __init__(self, config, **kwargs):
        self.workspace = WorkspaceClient()
        self.openai_client = self.workspace.serving_endpoints.get_open_ai_client()
        self.endpoint_name = config.get("endpoint_name")
        self.llm_config = config.get("llm_config")

    def create(self, input_data):
      messages = []
      for message in input_data['messages']:
        message.pop("name", None)
        messages.append(message)

      response = self.openai_client.chat.completions.create(
          model=self.endpoint_name,
          messages=messages,
          tools=input_data['tools'],
          tool_choice="auto",
          **self.llm_config
      )
      
      return response

    def message_retrieval(self, response):
      # Process and return messages from the response
      return [choice.message for choice in response.choices]

    def cost(self, response):
      # Implement cost calculation if applicable
      return 0

    def get_usage(self, response):
      usage = response.usage
      # Implement usage statistics if available
      return {"prompt_tokens": usage.prompt_tokens, "total_tokens": usage.total_tokens, "completion_tokens": usage.completion_tokens}

# COMMAND ----------

config_list = {
    "model_client_cls": "DatabricksModelServingClient",
    "model": "gpt4o",
    "endpoint_name": "casaman-gpt4", # "databricks-meta-llama-3-3-70b-instruct",
    "llm_config": {"temperature": 0.5, "max_tokens": 1500}
    
}

# COMMAND ----------

import os

from autogen import ConversableAgent

def create_agents(system_prompt, chat_history):

    def is_termination_message(message):
        content = message.get("content", "")
        return (content and "TERMINATE" in content.upper()) or (message['role'] == 'user' and 'tool_calls' not in message)

    # The user proxy agent is used for interacting with the assistant agent
    # and executes tool calls.
    user_proxy = ConversableAgent(
        name="User",
        llm_config=False,
        is_termination_msg=is_termination_message,
        human_input_mode="NEVER",
    )

    assistant = ConversableAgent(
        name="Assistant",
        system_message="You are a helpful AI assistant. "
        "You can help with simple calculations. "
        "Return 'TERMINATE' when the task is done.",
        llm_config={"config_list": [config_list]},
        chat_messages={user_proxy: chat_history}
    )

    return assistant, user_proxy
    
assistant, user_proxy = create_agents('test', [])

# COMMAND ----------

from autogen import register_function

for tool in tools:
  register_function(
      tool,
      caller=assistant,  # The assistant agent can suggest calls to the calculator.
      executor=user_proxy,  # The user proxy agent can execute the calculator calls.
      name=tool.name,
      description=tool.description,  # A description of the tool.
  )

# COMMAND ----------

translate_sku_tool._toolkit.tools[0].register_function(callers = assistant,
                                executors = user_proxy )

# COMMAND ----------

assistant.register_model_client(model_client_cls=DatabricksModelServingClient)

# COMMAND ----------

chat_result = user_proxy.initiate_chat(assistant, message="What is mlflow in databricks?")

# COMMAND ----------

assistant.last_message(user_proxy)

# COMMAND ----------

history = assistant.chat_messages[user_proxy]
history

# COMMAND ----------

assistant, user_proxy = create_agents('test', history)

# COMMAND ----------

assistant.chat_messages[user_proxy]

# COMMAND ----------

chat_result = user_proxy.initiate_chat(assistant, message="This is the second turn of the conversation. Can you summary our actual conversation?", clear_history=False)

# COMMAND ----------

assistant.llm_config["tools"]

# COMMAND ----------

assistant.last_message(user_proxy)