{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11e094-fffb-4ab5-9a3c-c8e096072b24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain 0.1.20 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.25 which is incompatible.\nlangchain 0.1.20 requires langsmith<0.2.0,>=0.1.17, but you have langsmith 0.2.3 which is incompatible.\nlangchain-community 0.0.38 requires langchain-core<0.2.0,>=0.1.52, but you have langchain-core 0.3.25 which is incompatible.\nlangchain-community 0.0.38 requires langsmith<0.2.0,>=0.1.0, but you have langsmith 0.2.3 which is incompatible.\nlangchain-text-splitters 0.0.2 requires langchain-core<0.3,>=0.1.28, but you have langchain-core 0.3.25 which is incompatible.\nydata-profiling 4.5.1 requires pandas!=1.4.0,<2.1,>1.1, but you have pandas 2.2.3 which is incompatible.\nydata-profiling 4.5.1 requires pydantic<2,>=1.8.1, but you have pydantic 2.10.3 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0m\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -qqqq -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e304bc46-67f7-415f-9c39-2fcd173a8af4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e450879c-040b-4168-83e4-e41a4d1e54b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored on calling ctypes callback function: <function _ThreadpoolInfo._find_modules_with_dl_iterate_phdr.<locals>.match_module_callback at 0x7ff9cf400ae0>\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 400, in match_module_callback\n    self._make_module_from_path(filepath)\n  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 515, in _make_module_from_path\n    module = module_class(filepath, prefix, user_api, internal_api)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 606, in __init__\n    self.version = self.get_version()\n                   ^^^^^^^^^^^^^^^^^^\n  File \"/databricks/python/lib/python3.11/site-packages/threadpoolctl.py\", line 646, in get_version\n    config = get_config().split()\n             ^^^^^^^^^^^^^^^^^^\nAttributeError: 'NoneType' object has no attribute 'split'\n"
     ]
    }
   ],
   "source": [
    "from cookbook.tools.vector_search import (\n",
    "    VectorSearchRetrieverTool,\n",
    "    VectorSearchSchema,\n",
    ")\n",
    "from cookbook.tools.uc_tool import UCTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3fca500-aa82-47dc-9154-71a6e1a9f73e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:unitycatalog.ai.core.databricks:Current SparkSession in the active environment is not a pyspark.sql.connect.session.SparkSession instance. Classic runtime does not support all functionalities of the unitycatalog-ai framework. To use the full capabilities of unitycatalog-ai, execute your code using a client that is attached to a Serverless runtime cluster. To learn more about serverless, see the guide at: https://docs.databricks.com/en/compute/serverless/index.html#connect-to-serverless-compute for more details.\n"
     ]
    }
   ],
   "source": [
    "retriever_tool = VectorSearchRetrieverTool(\n",
    "    name=\"search_product_docs\",\n",
    "    description=\"Use this tool to search for product documentation.\",\n",
    "    vector_search_index=\"dbdemos.dbdemos_rag_chatbot.databricks_documentation_vs_index\",\n",
    "    vector_search_schema=VectorSearchSchema(\n",
    "        # These columns are the default values used in the `01_data_pipeline` notebook\n",
    "        # If you used a different column names in that notebook OR you are using a pre-built vector index, update the column names here.\n",
    "        chunk_text=\"content\",  # Contains the text of each document chunk\n",
    "        document_uri=\"url\",  # The document URI of the chunk e.g., \"/Volumes/catalog/schema/volume/file.pdf\" - displayed as the document ID in the Review App\n",
    "        # additional_metadata_columns=[],  # Additional columns to return from the vector database and present to the LLM\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "translate_sku_tool = UCTool(uc_function_name=\"devanshu_pandey.cmhc_demo.vector_index_search_tool\")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c10ef87a-b8e5-45e8-8dcb-8f93f10be258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "entry_point = dbutils.notebook.entry_point\n",
    "\n",
    "host_name = f'https://{entry_point.getDbutils().notebook().getContext().browserHostName().get()}'\n",
    "\n",
    "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5f8ce82-4f8b-483d-ac34-72f0a553a592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://adb-984752964297111.11.azuredatabricks.net/serving-endpoints/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = f\"{host_name}/serving-endpoints/\"\n",
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4685a40a-2733-4546-9847-10f879c43730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def is_termination_message(message):\n",
    "    content = message.get(\"content\", \"\")\n",
    "    return (content and \"TERMINATE\" in content.upper()) or (message['role'] == 'user' and 'tool_calls' not in message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b497b6d2-9b82-4ed7-85c5-d648f8df7b58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.serving import ChatMessage, ChatMessageRole\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class DatabricksModelServingClient:\n",
    "    def __init__(self, config, **kwargs):\n",
    "        self.workspace = WorkspaceClient()\n",
    "        self.openai_client = self.workspace.serving_endpoints.get_open_ai_client()\n",
    "        self.endpoint_name = config.get(\"endpoint_name\")\n",
    "        self.llm_config = config.get(\"llm_config\")\n",
    "\n",
    "    def create(self, input_data):\n",
    "      messages = []\n",
    "      for message in input_data['messages']:\n",
    "        message.pop(\"name\", None)\n",
    "        messages.append(message)\n",
    "\n",
    "      response = self.openai_client.chat.completions.create(\n",
    "          model=self.endpoint_name,\n",
    "          messages=messages,\n",
    "          tools=input_data['tools'],\n",
    "          tool_choice=\"auto\",\n",
    "          **self.llm_config\n",
    "      )\n",
    "      \n",
    "      return response\n",
    "\n",
    "    def message_retrieval(self, response):\n",
    "      # Process and return messages from the response\n",
    "      return [choice.message for choice in response.choices]\n",
    "\n",
    "    def cost(self, response):\n",
    "      # Implement cost calculation if applicable\n",
    "      return 0\n",
    "\n",
    "    def get_usage(self, response):\n",
    "      usage = response.usage\n",
    "      # Implement usage statistics if available\n",
    "      return {\"prompt_tokens\": usage.prompt_tokens, \"total_tokens\": usage.total_tokens, \"completion_tokens\": usage.completion_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c130294-fa62-4c16-8b4a-a22ec87e8b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config_list = {\n",
    "    \"model_client_cls\": \"DatabricksModelServingClient\",\n",
    "    \"model\": \"gpt4o\",\n",
    "    \"endpoint_name\": \"casaman-gpt4\", # \"databricks-meta-llama-3-3-70b-instruct\",\n",
    "    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950623c1-a8e6-4ee0-b363-dde5cebc509d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-17 20:21:39] {509} INFO - Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.oai.client:Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "def create_agents(system_prompt, chat_history):\n",
    "\n",
    "    def is_termination_message(message):\n",
    "        content = message.get(\"content\", \"\")\n",
    "        return (content and \"TERMINATE\" in content.upper()) or (message['role'] == 'user' and 'tool_calls' not in message)\n",
    "\n",
    "    # The user proxy agent is used for interacting with the assistant agent\n",
    "    # and executes tool calls.\n",
    "    user_proxy = ConversableAgent(\n",
    "        name=\"User\",\n",
    "        llm_config=False,\n",
    "        is_termination_msg=is_termination_message,\n",
    "        human_input_mode=\"NEVER\",\n",
    "    )\n",
    "\n",
    "    assistant = ConversableAgent(\n",
    "        name=\"Assistant\",\n",
    "        system_message=\"You are a helpful AI assistant. \"\n",
    "        \"You can help with simple calculations. \"\n",
    "        \"Return 'TERMINATE' when the task is done.\",\n",
    "        llm_config={\"config_list\": [config_list]},\n",
    "        chat_messages={user_proxy: chat_history}\n",
    "    )\n",
    "\n",
    "    return assistant, user_proxy\n",
    "    \n",
    "assistant, user_proxy = create_agents('test', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b8fb8e-1496-4fa3-8914-e450ba096851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-17 20:21:41] {509} INFO - Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.oai.client:Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "from autogen import register_function\n",
    "\n",
    "for tool in tools:\n",
    "  register_function(\n",
    "      tool,\n",
    "      caller=assistant,  # The assistant agent can suggest calls to the calculator.\n",
    "      executor=user_proxy,  # The user proxy agent can execute the calculator calls.\n",
    "      name=tool.name,\n",
    "      description=tool.description,  # A description of the tool.\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba1c2c1-a7f3-4506-8b79-cec97ee7e92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 12-17 20:21:43] {509} INFO - Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.oai.client:Detected custom model client in config: DatabricksModelServingClient, model client can not be used until register_model_client is called.\n"
     ]
    }
   ],
   "source": [
    "translate_sku_tool._toolkit.tools[0].register_function(callers = assistant,\n",
    "                                executors = user_proxy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b4a8bc2-1b7a-4880-952c-4b185fa3b47f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assistant.register_model_client(model_client_cls=DatabricksModelServingClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a90296d7-57ef-4c48-829d-3ab3eca6e2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mUser\u001B[0m (to Assistant):\n\nWhat is mlflow in databricks?\n\n--------------------------------------------------------------------------------\n\u001B[31m\n>>>>>>>> USING AUTO REPLY...\u001B[0m\n\u001B[33mAssistant\u001B[0m (to User):\n\n\u001B[32m***** Suggested tool call (call_RcySHcYfv5iSQcboahKj0tu5): search_product_docs *****\u001B[0m\nArguments: \n{\"query\":\"mlflow in databricks\"}\n\u001B[32m************************************************************************************\u001B[0m\n\n--------------------------------------------------------------------------------\n\u001B[35m\n>>>>>>>> EXECUTING FUNCTION search_product_docs...\u001B[0m\n\u001B[33mUser\u001B[0m (to Assistant):\n\n\u001B[33mUser\u001B[0m (to Assistant):\n\n\u001B[32m***** Response from calling tool (call_RcySHcYfv5iSQcboahKj0tu5) *****\u001B[0m\n[{\"page_content\": \"ML lifecycle management using MLflow  \\nThis article describes how MLflow is used in Databricks for machine learning lifecycle management. It also includes examples that introduce each MLflow component and links to content that describe how these components are hosted within Databricks.  \\nML lifecycle management in Databricks is provided by managed MLflow. Databricks provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features such as experiment and run management and notebook revision capture.  \\nFirst-time users should begin with Get started with MLflow experiments, which demonstrates the basic MLflow tracking APIs.  \\nWhat is MLflow?\\nWhat is MLflow?\\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle. It has the following primary components:  \\nTracking: Allows you to track experiments to record and compare parameters and results.  \\nModels: Allow you to manage and deploy models from a variety of ML libraries to a variety of model serving and inference platforms.  \\nProjects: Allow you to package ML code in a reusable, reproducible form to share with other data scientists or transfer to production.  \\nModel Registry: Allows you to centralize a model store for managing models’ full lifecycle stage transitions: from staging to production, with capabilities for versioning and annotating. Databricks provides a managed version of the Model Registry in Unity Catalog.  \\nModel Serving: Allows you to host MLflow models as REST endpoints. Databricks provides a unified interface to deploy, govern, and query your served AI models.  \\nMLflow supports Java, Python, R, and REST APIs.  \\nNote  \\nIf you’re just getting started with Databricks, consider using MLflow on Databricks Community Edition, which provides a simple managed MLflow experience for lightweight experimentation. Remote execution of MLflow projects is not supported on Databricks Community Edition. We plan to impose moderate limits on the number of experiments and runs. For the initial launch of MLflow on Databricks Community Edition no limits are imposed.  \\nMLflow data stored in the control plane (experiment runs, metrics, tags and params) is encrypted using a platform-managed key. Encryption using Customer-managed keys for managed services is not supported for that data. On the other hand, the MLflow models and artifacts stored in your root (DBFS) storage can be encrypted using your own key by configuring customer-managed keys for workspace storage.\\n\\nMLflow tracking\\nMLflow tracking\\nMLflow on Databricks offers an integrated experience for tracking and securing training runs for machine learning and deep learning models.  \\nTrack model development using MLflow  \\nDatabricks Autologging\\n\\nModel lifecycle management\\nModel lifecycle management\\nMLflow Model Registry is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Databricks provides a hosted version of the MLflow Model Registry in Unity Catalog. Unity Catalog provides centralized model governance, cross-workspace access, lineage, and deployment. For details about managing the model lifecycle in Unity Catalog, see Manage model lifecycle in Unity Catalog.  \\nIf your workspace is not enabled for Unity Catalog, you can use the Workspace Model Registry.  \\nModel Registry concepts  \\nModel: An MLflow Model logged from an experiment or run that is logged with one of the model flavor’s mlflow.<model-flavor>.log_model methods. After a model is logged, you can register it with the Model Registry.  \\nRegistered model: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.  \\nModel version: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.  \\nModel alias: An alias is a mutable, named reference to a particular version of a registered model. Typical uses of aliases are to specify which model versions are deployed in a given environment in your model training workflows or to write inference workloads that target a specific alias. For example, you could assign the “Champion” alias of your “Fraud Detection” registered model to the model version that should serve the majority of production traffic, and then write inference workloads that target that alias (that is, make predictions using the “Champion” version).  \\nModel stage (workspace model registry only): A model version can be assigned one or more stages. MLflow provides predefined stages for the common use cases: None, Staging, Production, and Archived. With the appropriate permission you can transition a model version between stages or you can request a model stage transition. Model version stages are not used in Unity Catalog.  \\nDescription: You can annotate a model’s intent, including a description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.  \\nExample notebooks  \\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see the following:  \\nModels in Unity Catalog example  \\nWorkspace Model Registry example\\n\\nModel deployment\", \"metadata\": {\"similarity_score\": 0.0061636227, \"url\": \"https://docs.databricks.com/en/mlflow/index.html\"}, \"id\": 25705.0}, {\"page_content\": \"Deploy models for batch inference and prediction  \\nThis article describes how to deploy MLflow models for offline (batch and streaming) inference. Databricks recommends that you use MLflow to deploy machine learning models for batch or streaming inference. For general information about working with MLflow models, see Log, load, register, and deploy MLflow models.  \\nFor information about real-time model serving on Databricks, see Model serving with Databricks.  \\nUse MLflow for model inference\\nUse MLflow for model inference\\nMLflow helps you generate code for batch or streaming inference.  \\nIn the MLflow Model Registry, you can automatically generate a notebook for batch or streaming inference via Delta Live Tables.  \\nIn the MLflow Run page for your model, you can copy the generated code snippet for inference on pandas or Apache Spark DataFrames.  \\nYou can also customize the code generated by either of the above options. See the following notebooks for examples:  \\nThe model inference example uses a model trained with scikit-learn and previously logged to MLflow to show how to load a model and use it to make predictions on data in different formats. The notebook illustrates how to apply the model as a scikit-learn model to a pandas DataFrame, and how to apply the model as a PySpark UDF to a Spark DataFrame.  \\nThe MLflow Model Registry example shows how to build, manage, and deploy a model with Model Registry. On that page, you can search for .predict to identify examples of offline (batch) predictions.\\n\\nCreate a Databricks job\\nCreate a Databricks job\\nTo run batch or streaming predictions as a job, create a notebook or JAR that includes the code used to perform the predictions. Then, execute the notebook or JAR as a Databricks job. Jobs can be run either immediately or on a schedule.\\n\\nStreaming inference\\nStreaming inference\\nFrom the MLflow Model Registry, you can automatically generate a notebook that integrates the MLflow PySpark inference UDF with Delta Live Tables.  \\nYou can also modify the generated inference notebook to use the Apache Spark Structured Streaming API.  \\nInference with deep learning models  \\nFor information about and examples of deep learning model inference on Databricks, see the following articles:  \\nDeep learning model inference workflow  \\nDeep learning model inference performance tuning guide  \\nReference solutions for machine learning\\n\\nInference with MLlib and XGBoost4J models\\nInference with MLlib and XGBoost4J models\\nFor scalable model inference with MLlib and XGBoost4J models, use the native transform methods to perform inference directly on Spark DataFrames. The MLlib example notebooks include inference steps.\\n\\nCustomize and optimize model inference\\nCustomize and optimize model inference\\nWhen you use the MLflow APIs to run inference on Spark DataFrames, you can load the model as a Spark UDF and apply it at scale using distributed computing.  \\nYou can customize your model to add pre-processing or post-processing and to optimize computational performance for large models. A good option for customizing models is the MLflow pyfunc API, which allows you to wrap a model with custom logic.  \\nIf you need to do further customization, you can manually wrap your machine learning model in a Pandas UDF or a pandas Iterator UDF. See the deep learning examples.  \\nFor smaller datasets, you can also use the native model inference routines provided by the library.  \\nModel inference example Deep learning model inference workflow Deep learning model inference performance tuning guide\", \"metadata\": {\"similarity_score\": 0.005802475, \"url\": \"https://docs.databricks.com/en/machine-learning/model-inference/index.html\"}, \"id\": 25572.0}, {\"page_content\": \"Use MLflow models in a Delta Live Tables pipeline\\nNote  \\nTo use MLflow models in a Unity Catalog-enabled pipeline, your pipeline must be configured to use the preview channel. To use the current channel, you must configure your pipeline to publish to the Hive metastore.  \\nYou can use MLflow-trained models in Delta Live Tables pipelines. MLflow models are treated as transformations in Databricks, meaning they act upon a Spark DataFrame input and return results as a Spark DataFrame. Because Delta Live Tables defines datasets against DataFrames, you can convert Apache Spark workloads that leverage MLflow to Delta Live Tables with just a few lines of code. For more on MLflow, see ML lifecycle management using MLflow.  \\nIf you already have a Python notebook calling an MLflow model, you can adapt this code to Delta Live Tables by using the @dlt.table decorator and ensuring functions are defined to return transformation results. Delta Live Tables does not install MLflow by default, so make sure you %pip install mlflow and import mlflow and dlt at the top of your notebook. For an introduction to Delta Live Tables syntax, see Example: Ingest and process New York baby names data.  \\nTo use MLflow models in Delta Live Tables, complete the following steps:  \\nObtain the run ID and model name of the MLflow model. The run ID and model name are used to construct the URI of the MLflow model.  \\nUse the URI to define a Spark UDF to load the MLflow model.  \\nCall the UDF in your table definitions to use the MLflow model.  \\nThe following example shows the basic syntax for this pattern:  \\n%pip install mlflow import dlt import mlflow run_id= \\\"<mlflow-run-id>\\\" model_name = \\\"<the-model-name-in-run>\\\" model_uri = f\\\"runs:/{run_id}/{model_name}\\\" loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri) @dlt.table def model_predictions(): return dlt.read(<input-data>) .withColumn(\\\"prediction\\\", loaded_model_udf(<model-features>))  \\nAs a complete example, the following code defines a Spark UDF named loaded_model_udf that loads an MLflow model trained on loan risk data. The data columns used to make the prediction are passed as an argument to the UDF. The table loan_risk_predictions calculates predictions for each row in loan_risk_input_data.  \\n%pip install mlflow import dlt import mlflow from pyspark.sql.functions import struct run_id = \\\"mlflow_run_id\\\" model_name = \\\"the_model_name_in_run\\\" model_uri = f\\\"runs:/{run_id}/{model_name}\\\" loaded_model_udf = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri) categoricals = [\\\"term\\\", \\\"home_ownership\\\", \\\"purpose\\\", \\\"addr_state\\\",\\\"verification_status\\\",\\\"application_type\\\"] numerics = [\\\"loan_amnt\\\", \\\"emp_length\\\", \\\"annual_inc\\\", \\\"dti\\\", \\\"delinq_2yrs\\\", \\\"revol_util\\\", \\\"total_acc\\\", \\\"credit_length_in_years\\\"] features = categoricals + numerics @dlt.table( comment=\\\"GBT ML predictions of loan risk\\\", table_properties={ \\\"quality\\\": \\\"gold\\\" } ) def loan_risk_predictions(): return dlt.read(\\\"loan_risk_input_data\\\") .withColumn('predictions', loaded_model_udf(struct(features)))\\n\\nRetain manual deletes or updates\", \"metadata\": {\"similarity_score\": 0.005774803, \"url\": \"https://docs.databricks.com/en/delta-live-tables/transform.html\"}, \"id\": 24547.0}, {\"page_content\": \"February 2019  \\nThese features and Databricks platform improvements were released in February 2019.  \\nNote  \\nReleases are staged. Your Databricks account may not be updated until up to a week after the initial release date.  \\nManaged MLflow on Databricks Public Preview\\nManaged MLflow on Databricks Public Preview\\nFebruary 26 - March 5, 2019: Version 2.92  \\nMLflow is an open source platform for managing the end-to-end machine learning lifecycle. It tackles three primary functions:  \\nTracking experiments to record and compare parameters and results.  \\nManaging and deploying models from a variety of ML libraries to a variety of model serving and inference platforms.  \\nPackaging ML code in a reusable, reproducible form to share with other data scientists or transfer to production.  \\nDatabricks now provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features such as experiment management, run management, and notebook revision capture. MLflow on Databricks offers an integrated experience for tracking and securing machine learning model training runs and running machine learning projects. By using managed MLflow on Databricks, you get the advantages of both platforms, including:  \\nWorkspaces: Collaboratively track and organize experiments and results within Databricks Workspaces with a hosted MLflow Tracking Server and integrated experiment UI. When you use MLflow in notebooks, Databricks automatically captures notebook revisions so you can reproduce the same code and runs later.  \\nSecurity: Take advantage of one common security model for the entire ML lifecycle via ACLs.  \\nJobs: Run MLflow projects as Databricks jobs remotely and directly from Databricks notebooks.  \\nHere’s a demo of a tracking workflow in a Databricks Workspace:  \\nFor details, see Track ML and deep learning training runs and Run MLflow Projects on Databricks.\\n\\nAzure Data Lake Storage Gen2 connector is generally available\\nAzure Data Lake Storage Gen2 connector is generally available\\nFebruary 15, 2019  \\nAzure Data Lake Storage Gen2 (ADLS Gen2), the next-generation data lake solution for big data analytics, is now GA, as is the ADLS Gen2 connector for Databricks. We are also pleased to announce that ADLS Gen2 supports Databricks Delta when you are running clusters on Databricks Runtime 5.2 and above.\\n\\nPython 3 now the default when you create clusters\\nPython 3 now the default when you create clusters\\nFebruary 12-19, 2019: Version 2.91  \\nThe default Python version for clusters created using the UI has switched from Python 2 to Python 3. The default for clusters created using the REST API is still Python 2.  \\nExisting clusters will not change their Python versions. But if you’ve been in the habit of taking the Python 2 default when you create new clusters, you’ll need to start paying attention to your Python version selection.\\n\\nAdditional cluster instance types\\nAdditional cluster instance types\\nFebruary 12-19, 2019: Version 2.91  \\nDatabricks now provides Beta support for the following Amazon EC2 instance types:  \\nc5.18xlarge  \\nr5.24xlarge  \\nr4.16xlarge  \\nm5.24xlarge\\n\\nDelta Lake generally available\\nDelta Lake generally available\\nFebruary 1, 2019  \\nNow everyone can get the benefits of Databricks Delta’s powerful transactional storage layer and super-fast reads: as of February 1, Delta Lake is GA and available on all supported versions of Databricks Runtime. For information about Delta, see the What is Delta Lake?.\", \"metadata\": {\"similarity_score\": 0.0056295595, \"url\": \"https://docs.databricks.com/en/release-notes/product/2019/february.html\"}, \"id\": 26114.0}, {\"page_content\": \"AI and Machine Learning on Databricks  \\nThis article describes the tools that Databricks provides to help you build and monitor AI and ML workflows. The diagram shows how these components work together to help you implement your model development and deployment process.  \\nWhy use Databricks for machine learning and deep learning?\\nWhy use Databricks for machine learning and deep learning?\\nWith Databricks, you can implement the full ML lifecycle on a single platform with end-to-end governance throughout the ML pipeline. Databricks includes the following built-in tools to support ML workflows:  \\nUnity Catalog for governance, discovery, versioning, and access control for data, features, models, and functions.  \\nLakehouse Monitoring for data monitoring.  \\nFeature engineering and serving.  \\nSupport for the model lifecycle:  \\nDatabricks AutoML for automated model training.  \\nMLflow for model development tracking.  \\nUnity Catalog for model management.  \\nDatabricks Model Serving for high-availability, low-latency model serving. This includes deploying LLMs using:  \\nFoundation Model APIs which allow you to access and query state-of-the-art open models from a serving endpoint.  \\nExternal models which allow you to access models hosted outside of Databricks.  \\nLakehouse Monitoring to track model prediction quality and drift.  \\nDatabricks Workflows for automated workflows and production-ready ETL pipelines.  \\nDatabricks Git folders for code management and Git integration.\\n\\nDeep learning on Databricks\\nDeep learning on Databricks\\nConfiguring infrastructure for deep learning applications can be difficult.  \\nDatabricks Runtime for Machine Learning takes care of that for you, with clusters that have built-in compatible versions of the most common deep learning libraries like TensorFlow, PyTorch, and Keras, and supporting libraries such as Petastorm, Hyperopt, and Horovod. Databricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. It also supports libraries like Ray to parallelize compute processing for scaling ML workflows and AI applications.  \\nDatabricks Runtime ML clusters also include pre-configured GPU support with drivers and supporting libraries. Databricks Model Serving enables creation of scalable GPU endpoints for deep learning models with no extra configuration.  \\nFor machine learning applications, Databricks recommends using a cluster running Databricks Runtime for Machine Learning. See Create a cluster using Databricks Runtime ML.  \\nTo get started with deep learning on Databricks, see:  \\nBest practices for deep learning on Databricks  \\nDeep learning on Databricks  \\nReference solutions for deep learning\\n\\nLarge language models (LLMs) and generative AI on Databricks\\nLarge language models (LLMs) and generative AI on Databricks\\nDatabricks Runtime for Machine Learning includes libraries like Hugging Face Transformers and LangChain that allow you to integrate existing pre-trained models or other open-source libraries into your workflow. The Databricks MLflow integration makes it easy to use the MLflow tracking service with transformer pipelines, models, and processing components. In addition, you can integrate OpenAI models or solutions from partners like John Snow Labs in your Databricks workflows.  \\nWith Databricks, you can customize a LLM on your data for your specific task. With the support of open source tooling, such as Hugging Face and DeepSpeed, you can efficiently take a foundation LLM and train it with your own data to improve its accuracy for your specific domain and workload. You can then leverage the custom LLM in your generative AI applications.  \\nIn addition, Databricks provides Foundation Model APIs and external models which allows you to access and query state-of-the-art open models from a serving endpoint. Using Foundation Model APIs, developers can quickly and easily build applications that leverage a high-quality generative AI model without maintaining their own model deployment.  \\nFor SQL users, Databricks provides AI functions that SQL data analysts can use to access LLM models, including from OpenAI, directly within their data pipelines and workflows. See AI Functions on Databricks.\\n\\nDatabricks Runtime for Machine Learning\", \"metadata\": {\"similarity_score\": 0.0054116026, \"url\": \"https://docs.databricks.com/en/machine-learning/index.html\"}, \"id\": 25533.0}]\n\u001B[32m**********************************************************************\u001B[0m\n\n--------------------------------------------------------------------------------\n\u001B[31m\n>>>>>>>> USING AUTO REPLY...\u001B[0m\n\u001B[33mAssistant\u001B[0m (to User):\n\nMLflow in Databricks is an open-source platform for managing the end-to-end machine learning lifecycle. It provides several key components to facilitate machine learning projects:\n\n1. **Tracking**: MLflow allows you to track experiments to record and compare parameters and results. This helps in managing and organizing experiments and results within Databricks Workspaces.\n\n2. **Models**: MLflow enables you to manage and deploy models from various ML libraries to different model serving and inference platforms.\n\n3. **Projects**: It allows you to package ML code in a reusable, reproducible form to share with other data scientists or transfer to production.\n\n4. **Model Registry**: This component helps in centralizing a model store for managing models' full lifecycle stage transitions, from staging to production, with capabilities for versioning and annotating.\n\n5. **Model Serving**: MLflow allows you to host ML models as REST endpoints, providing a unified interface to deploy, govern, and query your served AI models.\n\nDatabricks offers a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Databricks workspace features, such as experiment and run management and notebook revision capture. MLflow supports Java, Python, R, and REST APIs. \n\nFor those new to Databricks, MLflow on Databricks Community Edition provides a simple managed MLflow experience for lightweight experimentation.\n\n--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/databricks.mlflow.trace": "\"tr-233b2640bd5e45ed90347d540d53c7e2\"",
      "text/plain": [
       "Trace(request_id=tr-233b2640bd5e45ed90347d540d53c7e2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(assistant, message=\"What is mlflow in databricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ff36339-23e9-42f2-964c-ad6eea85ef40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'content': \"MLflow is an open-source platform used within Databricks for managing the end-to-end machine learning lifecycle. It offers several key components to facilitate this:\\n\\n1. **Tracking**: It allows you to track experiments and record metrics related to machine learning models, enabling you to compare parameters and results across different runs.\\n   \\n2. **Models**: It supports the management and deployment of models from various ML libraries to different serving and inference platforms, allowing for a broad usage across different environments.\\n\\n3. **Projects**: Allows ML code to be packaged in a reusable and reproducible manner so that it can be shared with other data scientists or transferred into production environments.\\n\\n4. **Model Registry**: This component centralizes a model store to manage the full lifecycle of models, from training and validation to deployment and monitoring.\\n\\nDatabricks provides a fully managed and hosted version of MLflow that is integrated with its workspace features, including experiment and run management and notebook revision capture, along with enterprise-grade security and availability features. \\n\\nThis managed MLflow experience makes it easier to track, manage, and deploy ML models while leveraging the distributed computing power and security features of Databricks. \\n\\nFor those just getting started, Databricks Community Edition offers a simplified version of managed MLflow suitable for lightweight experimentation.\\n\\nFor more detailed guidance, you can utilize the resources harmonized in Databricks' MLflow documentation.\",\n",
       " 'role': 'assistant',\n",
       " 'name': 'Assistant'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant.last_message(user_proxy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92599841-450c-425d-bede-309459a037c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = assistant.chat_messages[user_proxy]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20d128ab-713c-4871-b18a-c5aa42544437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant, user_proxy = create_agents('test', history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "911a9c70-7291-467c-9a22-1b4bbf9b31b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant.chat_messages[user_proxy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7393180-cd57-48fc-af71-27f9b04fa205",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(assistant, message=\"This is the second turn of the conversation. Can you summary our actual conversation?\", clear_history=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e931ada-d33b-4ea3-9a7e-faf363019807",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant.llm_config[\"tools\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b2976c3-0360-419d-8667-4a2495706777",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n",
       "\u001B[0m                ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SyntaxError",
        "evalue": "invalid syntax (command-3722098005799721-2479447087, line 5)"
       },
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;36m  File \u001B[0;32m<command-3722098005799721>, line 5\u001B[0;36m\u001B[0m\n\u001B[0;31m    \"llm_config\": {\"temperature\": 0.5, \"max_tokens\": 1500}\u001B[0m\n\u001B[0m                ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "assistant.last_message(user_proxy)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "autogen_started",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}