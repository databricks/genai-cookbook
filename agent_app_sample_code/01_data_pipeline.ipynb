{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c756f50-2063-4a07-b964-e5d6de29abb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Unstructured data pipeline for the Agent's Retriever\n",
    "\n",
    "By the end of this notebook, you will have transformed your unstructured documents into a vector index that can be queried by your Agent.\n",
    "\n",
    "This means:\n",
    "- Documents loaded into a delta table.\n",
    "- Documents are chunked.\n",
    "- Chunks have been embedded with an embedding model and stored in a vector index.\n",
    "\n",
    "The important resulting artifact of this notebook is the chunked vector index. This will be used in the next notebook to power our Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3777205-4dfe-418c-9d21-c67961a18070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ‘‰ START HERE: How to Use This Notebook\n",
    "\n",
    "Follow these steps to build and refine your data pipeline's quality:\n",
    "\n",
    "1. **Build a v0 index with default settings**\n",
    "    - Configure the data source and destination tables in the `1ï¸âƒ£ ðŸ“‚ Data source & destination configuration` cells\n",
    "    - Press `Run All` to create the vector index.\n",
    "\n",
    "    *Note: While you can adjust the other settings and modify the parsing/chunking code, we suggest doing so only after evaluating your Agent's quality so you can make improvements that specifically address root causes of quality issues.*\n",
    "\n",
    "2. **Use later notebooks to integrate the retriever into an the agent and evaluate the agent/retriever's quality.**\n",
    "\n",
    "3. **If the evaluation results show retrieval issues as a root cause, use this notebook to iterate on your data pipeline's code & config.** Below are some potential fixes you can try, see the AI Cookbook's [debugging retrieval issues](https://ai-cookbook.io/nbs/5-hands-on-improve-quality-step-1-retrieval.html) section for details.**\n",
    "    - Add missing, but relevant source documents into in the index.\n",
    "    - Resolve any conflicting information in source documents.\n",
    "    - Adjust the data pipeline configuration:\n",
    "      - Modify chunk size or overlap.\n",
    "      - Experiment with different embedding models.\n",
    "    - Adjust the data pipeline code:\n",
    "      - Create a custom parser or use different parsing libraries.\n",
    "      - Develop a custom chunker or use different chunking techniques.\n",
    "      - Extract additional metadata for each document.\n",
    "    - Adjust the Agent's code/config in subsequent notebooks:\n",
    "      - Change the number of documents retrieved (K).\n",
    "      - Try a re-ranker.\n",
    "      - Use hybrid search.\n",
    "      - Apply extracted metadata as filters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a6053b9-3135-4097-9ed0-64bdb03a6b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Important note:** Throughout this notebook, we indicate which cells you:\n",
    "- âœ…âœï¸ *should* customize - these cells contain code & config with business logic that you should edit to meet your requirements & tune quality\n",
    "- ðŸš«âœï¸ *typically will not* customize - these cells contain boilerplate code required to execute the pipeline\n",
    "\n",
    "*Cells that don't require customization still need to be run!  You CAN change these cells, but if this is the first time using this notebook, we suggest not doing so.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b35cfd-7c99-4419-8978-33939faf24a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install Python libraries\n",
    "\n",
    "ðŸš«âœï¸ Only modify if you need additional packages in your code changes to the document parsing or chunking logic.\n",
    "\n",
    "Versions of Databricks code are not locked since Databricks ensures changes are backwards compatible.\n",
    "Versions of open source packages are locked since package authors often make backwards compatible changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4eebb3-448a-4236-99fb-19e44858e3c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -qqqq -U -r requirements.txt\n",
    "# %pip install -qqqq -U -r requirements_datapipeline.txt\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running from an IDE with [`databricks-connect`](https://docs.databricks.com/en/dev-tools/databricks-connect/python/index.html), connect to a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.databricks_utils import get_cluster_url\n",
    "from cookbook.databricks_utils import get_active_cluster_id\n",
    "from cookbook.databricks_utils.install_cluster_library import install_requirements\n",
    "# cluster_id = get_active_cluster_id()\n",
    "# print(f\"Installing packages on the active cluster: {get_cluster_url(cluster_id)}\")\n",
    "\n",
    "# # TODO: build the utils wheel and install it \n",
    "# install_requirements(cluster_id, \"requirements.txt\")\n",
    "# install_requirements(cluster_id, \"requirements_datapipeline.txt\")\n",
    "\n",
    "# Get Spark session if using Databricks Connect from an IDE\n",
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "if not du.is_in_databricks_notebook():\n",
    "    from databricks.connect import DatabricksSession\n",
    "\n",
    "    spark = DatabricksSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a27dc10-44ae-4489-bc75-0d61c89b4268",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1ï¸âƒ£ ðŸ“‚ Data source & destination configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf8bd6ab-827e-4ba6-805f-091349906ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### âœ…âœï¸ Configure the data pipeline's source location.\n",
    "\n",
    "Choose a [Unity Catalog Volume](https://docs.databricks.com/en/volumes/index.html) containing PDF, HTML, etc documents to be parsed/chunked/embedded.\n",
    "\n",
    "- `uc_catalog_name`: Name of the Unity Catalog.\n",
    "- `uc_schema_name`: Name of the Unity Catalog schema.\n",
    "- `uc_volume_name`: Name of the Unity Catalog volume.\n",
    "\n",
    "Running this cell with validate that the UC Volume exists, trying to create it if not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59b3efc5-0591-4a44-b88d-184003cabfb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume /Volumes/ep/cookbook_local_test/product_docs exists.  View here: https://e2-dogfood.staging.cloud.databricks.com/explore/data/volumes/ep/cookbook_local_test/product_docs\n"
     ]
    }
   ],
   "source": [
    "from cookbook.config.data_pipeline.uc_volume_source import UCVolumeSourceConfig\n",
    "\n",
    "# Configure the UC Volume that contains the source documents\n",
    "source_config = UCVolumeSourceConfig(\n",
    "    # uc_catalog_name=\"REPLACE_ME\", # REPLACE_ME\n",
    "    # uc_schema_name=\"REPLACE_ME\", # REPLACE_ME\n",
    "    # uc_volume_name=f\"REPLACE_ME\", # REPLACE_ME\n",
    "    uc_catalog_name=\"ep\", # REPLACE_ME\n",
    "    uc_schema_name=\"cookbook_local_test\", # REPLACE_ME\n",
    "    uc_volume_name=f\"product_docs\", # REPLACE_ME\n",
    ")\n",
    "\n",
    "# Check if volume exists, create otherwise\n",
    "is_valid, msg = source_config.create_or_validate_volume()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "083e203f-e468-4ce7-b645-31507a36c86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### âœ…âœï¸ Configure the data pipeline's output location.\n",
    " \n",
    "Choose where the data pipeline outputs the parsed, chunked, and embedded documents.\n",
    "\n",
    "Required parameters:\n",
    "* `uc_catalog_name`: Unity Catalog name where tables will be created\n",
    "* `uc_schema_name`: Schema name within the catalog \n",
    "* `base_table_name`: Core name used as prefix for all generated tables\n",
    "* `vector_search_endpoint`: Vector Search endpoint to store the index\n",
    "\n",
    "Optional parameters:\n",
    "* `docs_table_postfix`: Suffix for the parsed documents table (default: \"docs\")\n",
    "* `chunked_table_postfix`: Suffix for the chunked documents table (default: \"docs_chunked\") \n",
    "* `vector_index_postfix`: Suffix for the vector index (default: \"docs_chunked_index\")\n",
    "* `version_suffix`: Version identifier (e.g. 'v1', 'test') to maintain multiple versions\n",
    "\n",
    "The generated tables follow this naming convention:\n",
    "* Parsed docs: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{docs_table_postfix}__{version_suffix}\n",
    "* Chunked docs: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{chunked_table_postfix}__{version_suffix}\n",
    "* Vector index: {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{vector_index_postfix}__{version_suffix}\n",
    "\n",
    "*Note: If you are comparing different chunking/parsing/embedding strategies, set the `version_suffix` parameter to maintain multiple versions of the pipeline output with the same base_table_name.*\n",
    "\n",
    "*Databricks suggests sharing a Vector Search endpoint across multiple agents.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All catalogs and schemas exist for parsed_docs_table, chunked_docs_table, and vector_index.\n",
      "Vector Search endpoint 'ericpeter_vector_search' exists.\n"
     ]
    }
   ],
   "source": [
    "from cookbook.config.data_pipeline.data_pipeline_output import DataPipelineOuputConfig\n",
    "\n",
    "# Output configuration\n",
    "output_config = DataPipelineOuputConfig(\n",
    "    # Required parameters\n",
    "    uc_catalog_name=source_config.uc_catalog_name, # usually same as source volume catalog, by default is the same as the source volume catalog\n",
    "    uc_schema_name=source_config.uc_schema_name, # usually same as source volume schema, by default is the same as the source volume schema\n",
    "    base_table_name=\"abc_123\", # usually similar / same as the source volume name; by default, is the same as the volume_name\n",
    "    # vector_search_endpoint=\"REPLACE_ME\", # Vector Search endpoint to store the index\n",
    "    vector_search_endpoint=\"ericpeter_vector_search\", # Vector Search endpoint to store the index\n",
    "\n",
    "    # Optional parameters, showing defaults\n",
    "    docs_table_postfix=\"docs\",              # default value is `docs`\n",
    "    chunked_table_postfix=\"docs_chunked\",   # default value is `docs_chunked`\n",
    "    vector_index_postfix=\"docs_chunked_index\", # default value is `docs_chunked_index`\n",
    "    version_suffix=\"v2\"                     # default is None\n",
    "\n",
    "    # Output tables / indexes follow this naming convention:\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{docs_table_postfix}__{version_suffix}\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{chunked_table_postfix}__{version_suffix}\n",
    "    # {uc_catalog_name}.{uc_schema_name}.{base_table_name}_{vector_index_postfix}__{version_suffix}\n",
    ")\n",
    "\n",
    "# Alternatively, you can directly pass in the UC locations of the tables / indexes\n",
    "# output_config = DataPipelineOuputConfig(\n",
    "#     chunked_docs_table=\"catalog.schema.docs_chunked\",\n",
    "#     parsed_docs_table=\"catalog.schema.parsed_docs\",\n",
    "#     vector_index=\"catalog.schema.docs_chunked_index\",\n",
    "#     vector_search_endpoint=\"REPLACE_ME\",\n",
    "# )\n",
    "\n",
    "# Check UC locations exist\n",
    "is_valid, msg = output_config.validate_catalog_and_schema()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)\n",
    "\n",
    "# Check Vector Search endpoint exists\n",
    "is_valid, msg = output_config.validate_vector_search_endpoint()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b380e5-1d9a-4c93-b8fe-ec23f00442a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### âœ…âœï¸ Configure chunk size and the embedding model.\n",
    "\n",
    "**Chunk size and overlap** control how a larger document is turned into smaller chunks that can be processed by an embedding model.  See the AI Cookbook [chunking deep dive](https://ai-cookbook.io/nbs/3-deep-dive-data-pipeline.html#chunking) for more details.\n",
    "\n",
    "**The embedding model** is an AI model that is used to identify the most similar documents to a given user's query.  See the AI Cookbook [embedding model deep dive](https://ai-cookbook.io/nbs/3-deep-dive-data-pipeline.html#embedding-model) for more details.\n",
    "\n",
    "This notebook supports the following [Foundational Models](https://docs.databricks.com/en/machine-learning/foundation-models/index.html) or [External Model](https://docs.databricks.com/en/generative-ai/external-models/index.html) of type `/llm/v1/embeddings`/.  If you want to try another model, you will need to modify the `utils/get_recursive_character_text_splitter` Notebook to add support.\n",
    "- `databricks-gte-large-en` or `databricks-bge-large-en`\n",
    "- Azure OpenAI or OpenAI External Model of type `text-embedding-ada-002`, `text-embedding-3-small` or `text-embedding-3-large`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06ee684b-c7bd-4c0e-8fd8-f54416948a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated serving endpoint databricks-gte-large-en as READY and of type llm/v1/embeddings.  View here: https://e2-dogfood.staging.cloud.databricks.com/ml/endpoints/databricks-gte-large-en\n",
      "Chunk size in tokens: 1024 and chunk overlap in tokens: 256 are valid.  Using 16.0% (1280 tokens) of the 8192 token context window.\n"
     ]
    }
   ],
   "source": [
    "from cookbook.config.data_pipeline.recursive_text_splitter import RecursiveTextSplitterChunkingConfig\n",
    "\n",
    "chunking_config = RecursiveTextSplitterChunkingConfig(\n",
    "    embedding_model_endpoint=\"databricks-gte-large-en\",  # A Model Serving endpoint supporting the /llm/v1/embeddings task\n",
    "    chunk_size_tokens=1024,\n",
    "    chunk_overlap_tokens=256,\n",
    ")\n",
    "\n",
    "# Validate the embedding endpoint & chunking config\n",
    "is_valid, msg = chunking_config.validate_embedding_endpoint()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)\n",
    "\n",
    "is_valid, msg = chunking_config.validate_chunk_size_and_overlap()\n",
    "if not is_valid:\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸš«âœï¸ Write the data pipeline configuration to a YAML\n",
    "\n",
    "This allows the configuration to be loaded referenced by the Agent's notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.config.data_pipeline import DataPipelineConfig\n",
    "from cookbook.config import serializable_config_to_yaml_file\n",
    "\n",
    "data_pipeline_config = DataPipelineConfig(\n",
    "    source=source_config,\n",
    "    output=output_config,\n",
    "    chunking_config=chunking_config,\n",
    ")\n",
    "\n",
    "serializable_config_to_yaml_file(data_pipeline_config, \"./configs/data_pipeline_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a28cbf99-c4ca-4adc-905a-e7ebfe015730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸ›‘ If you are running your initial data pipeline, you do not need to configure anything else, you can just `Run All` the notebook cells before.  You can modify these cells later to tune the quality of your data pipeline by changing the parsing logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95b6971b-b00b-4f42-bbe8-cc64eea2fff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3ï¸âƒ£ âŒ¨ï¸ Data pipeline code\n",
    "\n",
    "The code below executes the data pipeline.  You can modify the below code as indicated to implement different parsing or chunking strategies or to extract additional metadata fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c85ddc92-10c5-405c-ae78-8ded5462333e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pipeline step 1: Load & parse documents into a Delta Table\n",
    "\n",
    "In this step, we'll load files from the UC Volume defined in `source_config` into the Delta Table `storage_config.parsed_docs_table` . The contents of each file will become a separate row in our delta table.\n",
    "\n",
    "The path to the source document will be used as the `doc_uri` which is displayed to your end users in the Agent Evalution web application.\n",
    "\n",
    "After you test your POC with stakeholders, you can return here to change the parsing logic or extraction additional metadata about the documents to help improve the quality of your retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27466460-1ee7-4fe4-8faf-da9ddff11847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### âœ…âœï¸ Customize the parsing function\n",
    "\n",
    "This default implementation parses PDF, HTML, and DOCX files using open source libraries.  Adjust `file_parser(...)` and `ParserReturnValue` in `cookbook/data_pipeline/default_parser.py` to add change the parsing logic, add support for more file types, or extract additional metadata about each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d09fd38c-5b7b-47c5-aa6a-ff571ce2f83b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ParserReturnValue(TypedDict):\n",
      "    # DO NOT CHANGE THESE NAMES\n",
      "    # Parsed content of the document\n",
      "    content: str  # do not change this name\n",
      "    # The status of whether the parser succeeds or fails, used to exclude failed files downstream\n",
      "    parser_status: str  # do not change this name\n",
      "    # Unique ID of the document\n",
      "    doc_uri: str  # do not change this name\n",
      "\n",
      "    # OK TO CHANGE THESE NAMES\n",
      "    # Optionally, you can add additional metadata fields here\n",
      "    # example_metadata: str\n",
      "    last_modified: datetime\n",
      "\n",
      "def file_parser(\n",
      "    raw_doc_contents_bytes: bytes,\n",
      "    doc_path: str,\n",
      "    modification_time: datetime,\n",
      "    doc_bytes_length: int,\n",
      ") -> ParserReturnValue:\n",
      "    \"\"\"\n",
      "    Parses the content of a PDF document into a string.\n",
      "\n",
      "    This function takes the raw bytes of a PDF document and its path, attempts to parse the document using PyPDF,\n",
      "    and returns the parsed content and the status of the parsing operation.\n",
      "\n",
      "    Parameters:\n",
      "    - raw_doc_contents_bytes (bytes): The raw bytes of the document to be parsed (set by Spark when loading the file)\n",
      "    - doc_path (str): The DBFS path of the document, used to verify the file extension (set by Spark when loading the file)\n",
      "    - modification_time (timestamp): The last modification time of the document (set by Spark when loading the file)\n",
      "    - doc_bytes_length (long): The size of the document in bytes (set by Spark when loading the file)\n",
      "\n",
      "    Returns:\n",
      "    - ParserReturnValue: A dictionary containing the parsed document content and the status of the parsing operation.\n",
      "      The 'contenty will contain the parsed text as a string, and the 'parser_status' key will indicate\n",
      "      whether the parsing was successful or if an error occurred.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        from markdownify import markdownify as md\n",
      "\n",
      "        filename, file_extension = os.path.splitext(doc_path)\n",
      "\n",
      "        if file_extension == \".pdf\":\n",
      "            pdf_doc = fitz.Document(stream=raw_doc_contents_bytes, filetype=\"pdf\")\n",
      "            md_text = pymupdf4llm.to_markdown(pdf_doc)\n",
      "\n",
      "            parsed_document = {\n",
      "                \"content\": md_text.strip(),\n",
      "                \"parser_status\": \"SUCCESS\",\n",
      "            }\n",
      "        elif file_extension == \".html\":\n",
      "            html_content = raw_doc_contents_bytes.decode(\"utf-8\")\n",
      "\n",
      "            markdown_contents = md(\n",
      "                str(html_content).strip(), heading_style=markdownify.ATX\n",
      "            )\n",
      "            markdown_stripped = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_contents.strip())\n",
      "\n",
      "            parsed_document = {\n",
      "                \"content\": markdown_stripped,\n",
      "                \"parser_status\": \"SUCCESS\",\n",
      "            }\n",
      "        elif file_extension == \".docx\":\n",
      "            with tempfile.NamedTemporaryFile(delete=True) as temp_file:\n",
      "                temp_file.write(raw_doc_contents_bytes)\n",
      "                temp_file_path = temp_file.name\n",
      "                md = pypandoc.convert_file(temp_file_path, \"markdown\", format=\"docx\")\n",
      "\n",
      "                parsed_document = {\n",
      "                    \"content\": md.strip(),\n",
      "                    \"parser_status\": \"SUCCESS\",\n",
      "                }\n",
      "        elif file_extension in [\".txt\", \".md\"]:\n",
      "            parsed_document = {\n",
      "                \"content\": raw_doc_contents_bytes.decode(\"utf-8\").strip(),\n",
      "                \"parser_status\": \"SUCCESS\",\n",
      "            }\n",
      "        elif file_extension in [\".json\", \".jsonl\"]:\n",
      "            # NOTE: This is a placeholder for a JSON parser.  It's not a \"real\" parser, it just returns the raw JSON formatted into XML-like strings that LLMs tend to like.\n",
      "            json_data = json.loads(raw_doc_contents_bytes.decode(\"utf-8\"))\n",
      "\n",
      "            def flatten_json_to_xml(obj, parent_key=\"\"):\n",
      "                xml_parts = []\n",
      "                if isinstance(obj, dict):\n",
      "                    for key, value in obj.items():\n",
      "                        if isinstance(value, (dict, list)):\n",
      "                            xml_parts.append(flatten_json_to_xml(value, key))\n",
      "                        else:\n",
      "                            xml_parts.append(f\"<{key}>{str(value)}</{key}>\")\n",
      "                elif isinstance(obj, list):\n",
      "                    for i, item in enumerate(obj):\n",
      "                        if isinstance(item, (dict, list)):\n",
      "                            xml_parts.append(\n",
      "                                flatten_json_to_xml(item, f\"{parent_key}_{i}\")\n",
      "                            )\n",
      "                        else:\n",
      "                            xml_parts.append(\n",
      "                                f\"<{parent_key}_{i}>{str(item)}</{parent_key}_{i}>\"\n",
      "                            )\n",
      "                else:\n",
      "                    xml_parts.append(f\"<{parent_key}>{str(obj)}</{parent_key}>\")\n",
      "                return \"\\n\".join(xml_parts)\n",
      "\n",
      "            flattened_content = flatten_json_to_xml(json_data)\n",
      "            parsed_document = {\n",
      "                \"content\": flattened_content.strip(),\n",
      "                \"parser_status\": \"SUCCESS\",\n",
      "            }\n",
      "        else:\n",
      "            raise Exception(f\"No supported parser for {doc_path}\")\n",
      "\n",
      "        # Extract the required doc_uri\n",
      "        # convert from `dbfs:/Volumes/catalog/schema/pdf_docs/filename.pdf` to `/Volumes/catalog/schema/pdf_docs/filename.pdf`\n",
      "        modified_path = urlparse(doc_path).path\n",
      "        parsed_document[\"doc_uri\"] = modified_path\n",
      "\n",
      "        # Sample metadata extraction logic\n",
      "        # if \"test\" in parsed_document[\"content\n",
      "        #     parsed_document[\"example_metadata\"] = \"test\"\n",
      "        # else:\n",
      "        #     parsed_document[\"example_metadata\"] = \"not test\"\n",
      "\n",
      "        # Add the modified time\n",
      "        parsed_document[\"last_modified\"] = modification_time\n",
      "\n",
      "        return parsed_document\n",
      "\n",
      "    except Exception as e:\n",
      "        status = f\"An error occurred: {e}\\n{traceback.format_exc()}\"\n",
      "        warnings.warn(status)\n",
      "        return {\n",
      "            \"content\": \"\",\n",
      "            \"parser_status\": f\"ERROR: {status}\",\n",
      "        }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cookbook.data_pipeline.default_parser import file_parser, ParserReturnValue\n",
    "\n",
    "# Print the code of file_parser function for inspection\n",
    "import inspect\n",
    "print(inspect.getsource(ParserReturnValue))\n",
    "print(inspect.getsource(file_parser))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61034803-4bdd-4f0b-b173-a82448ee1790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The below cell is debugging code to test your parsing function on a single record. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a3ab67-2e30-4e39-b05e-3a8ff304fd5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the raw files from /Volumes/ep/cookbook_local_test/product_docs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731304323.145653 5761315 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 553 files from /Volumes/ep/cookbook_local_test/product_docs.  Files: ['000e6aa1-14c4-4e7b-80b9-ff7345ac6654.jsonl', '002df8aa-dd68-4276-8de0-68828bba8b82.jsonl', '00ef3061-6751-4230-b6a7-c6b8b2ab12a5.jsonl', '0162e248-d52f-45bd-9946-25042ee3d2ca.jsonl', '01648e19-af52-4058-bc2d-55b5ac13e2b5.jsonl', '020ef5a5-e10f-4315-932d-4ecd072dac49.jsonl', '027a8e9b-8b66-4819-b322-28b6600ba36a.jsonl', '0398c899-431d-4596-838d-1c8912336674.jsonl', '040081dc-321d-45ed-aa86-b4275a20f6e7.jsonl', '043138d9-7574-4678-9966-b0706b0ec52b.jsonl', '04399a6d-e5c0-44f7-97d6-41bc82a0bfa8.jsonl', '0459585a-b812-47ca-a674-9bbd0b6fbb37.jsonl', '04928816-ae35-47ca-a0b4-0baafa8ae985.jsonl', '0497bef6-6492-418a-a3c1-6badfee19a11.jsonl', '0564b37d-7802-4f1a-b95a-be1f29c1a51b.jsonl', '057f98e0-a9ec-438f-956d-b31d60cc9203.jsonl', '05f047c2-7883-42cb-afe7-272d5a71e084.jsonl', '05f22255-1314-4c88-b479-9ce1a173312d.jsonl', '06181a64-e149-4c2f-979f-e1aef835c098.jsonl', '068e3a53-846a-4a59-bed2-986c0e969443.jsonl', '06a5d9e1-3a79-46cc-8861-6fdfe0aef38e.jsonl', '06d37bc6-5d05-4f2d-9f82-b099abfca87b.jsonl', '06ed0d77-cbf6-4ca7-89d6-6a003e2476c3.jsonl', '071ff87e-e312-44a1-8e3f-fbf9c0ebb627.jsonl', '0771d318-c48e-4ee8-9e94-2a9951fe6c1c.jsonl', '0850a34d-5f8a-4b6e-b190-9c4dca3a53cd.jsonl', '0886669f-b019-4112-951f-8662bfe8fee4.jsonl', '096cda57-42e2-4c13-9756-34628413b0c9.jsonl', '0b062947-01a3-4e1c-b862-c7f75bc767ba.jsonl', '0b89681b-0733-4a19-81e3-253d9b789757.jsonl', '0beacb97-db62-48e1-b736-6e9a795ff156.jsonl', '0d272a51-189a-4abb-9ad4-e48b9261cecb.jsonl', '0df41aa7-d886-4483-ab64-b18bf1df1456.jsonl', '0e770b32-bc9b-43f4-bbf9-f671526126ec.jsonl', '0eedc550-ed92-4bec-a800-bb9224faba07.jsonl', '0feb0cc8-261b-4dbb-b005-d2f8ee807863.jsonl', '0ff7805a-539d-4524-92a6-301e13a7fe10.jsonl', '105f6da5-5db8-413b-b990-7893ee4d3833.jsonl', '10ab389d-8251-40bc-8446-1fd8cffbbf90.jsonl', '10cb947c-9bbe-424e-9d27-a7c470a39fbd.jsonl', '111068c2-c6c5-4df3-9136-7d7caef8d298.jsonl', '11446411-9d89-40ee-9bfe-e2d71faa9d2d.jsonl', '1180e605-b540-433b-9dfc-3b40ccc230a4.jsonl', '11acf679-231a-469e-9682-a3e8461a9429.jsonl', '11b9e55a-fbb8-4c20-8878-3f8cb7e3aefa.jsonl', '11d3e40e-b5b6-4238-90c6-3ddd7deb7579.jsonl', '11d916b4-ee41-4dc9-a0d5-e08e096d3a05.jsonl', '12df3e17-b4c2-4db7-acff-f11fcab35ab6.jsonl', '13042beb-0478-4f67-af34-49c9f4b566c7.jsonl', '1338b91b-975f-463a-acdc-d6e2b6defe35.jsonl', '139c37c1-4ad6-4430-a1b3-8b3d5a20e525.jsonl', '13aafd74-cafc-4258-8ac7-01c5840acbd7.jsonl', '13dea192-76c8-4950-a572-16e6622eb7ab.jsonl', '1447a074-16eb-4723-b537-0819191c8be9.jsonl', '14a89fee-3699-4eea-add3-b2f3a6bf686c.jsonl', '1594c440-c0b3-45d7-a4dd-3d62983eaffb.jsonl', '167a0ce3-33b0-4397-890e-e1dbfe35d97a.jsonl', '16e3bff4-17b1-408b-9cdb-0b587cecc16c.jsonl', '179d8765-3a2d-4194-8133-bfe472556e80.jsonl', '17cafca0-47da-450e-9519-9d0a94b4801f.jsonl', '17d6285e-5163-482f-af08-629d0069e1cb.jsonl', '17e7be01-cbfa-42bc-aa90-440909716a54.jsonl', '18456e39-6d8c-4454-a38d-df4151aadb82.jsonl', '19bf77b2-e0fb-429a-a1ec-3e75f99508d6.jsonl', '1a194215-bad4-442b-a71d-b56183f907a2.jsonl', '1a2303ed-ffb9-4c28-b9bf-481fdf984e58.jsonl', '1ad63e52-9838-4532-8397-85456c203861.jsonl', '1be746e9-859c-42a1-a79b-9b71b81fd128.jsonl', '1c75553f-1990-4506-a28f-c4aedfd0efac.jsonl', '1d27f4d0-c295-436b-b895-fac380eb2414.jsonl', '1d544c5d-a87d-4da3-ac62-5bfe3c85710c.jsonl', '1d57235e-9b9b-4dbc-8707-b80f790d4b00.jsonl', '1e08c3b6-d571-4543-8f75-a1ef775b970b.jsonl', '1e8e33ed-befd-46af-b451-a0c8b1cac833.jsonl', '1ec7a1b0-9291-41d3-b385-b84762cc9ed5.jsonl', '1f698e9b-dafc-4028-8791-fad0c454d86f.jsonl', '1f6be4ae-923b-44ae-a5e8-36f97d0b1093.jsonl', '20071821-e63a-4a50-9915-5584ee685b83.jsonl', '2010a40f-fc7e-4e75-acfe-8f00cf9f6dac.jsonl', '2012470b-d790-499f-915b-b27e5591c41a.jsonl', '206a7cd3-ebdc-4618-93f2-31df228addbf.jsonl', '2109f57c-665c-4435-b194-650fc186dcd4.jsonl', '218ad83c-37ca-45ab-8be5-582f5559097b.jsonl', '21ba0285-1c61-4763-b821-6a7ca6144afc.jsonl', '21effbd3-a76e-4bd2-a521-ecd31cc6896f.jsonl', '224a7e95-154b-426c-be6b-3bd2cf14b3c7.jsonl', '225de600-b0c5-4e99-b4fc-f10956fbbc13.jsonl', '228489db-62c6-4c51-ae07-b47019bacb31.jsonl', '228b1821-691a-463d-a0cc-1b0b80f40a52.jsonl', '22e139e6-85cf-4108-abbe-28451cec79d3.jsonl', '235e78c9-f7c0-43e6-ae1b-1267f7d2c9f8.jsonl', '23fee8a2-812c-4ad3-b830-6eb068c30655.jsonl', '24295f0c-c6e0-421b-a824-562d817d2c89.jsonl', '247e4f2d-9b20-42da-8c0b-37c44f4ba119.jsonl', '24c1b28c-aa9b-4ad3-a8f0-0a814cbd2972.jsonl', '24d944a6-a037-481f-9902-807d7d9f22c0.jsonl', '2533fda5-7f3e-4030-9358-e308730de60e.jsonl', '2554d3ad-80aa-43eb-a175-04e19b877147.jsonl', '257b3307-8653-4335-8876-9ae1a1cbcf4e.jsonl', '25ab1e7f-232f-4cde-ba19-cd53bf25ff09.jsonl', '26575552-9780-4984-8791-4bc6e180d084.jsonl', '26602f36-2888-404e-9f0b-3717eca65da1.jsonl', '267952ec-6ec9-4a84-9262-fd56defed5d4.jsonl', '284cf7ba-7a1a-4925-8b89-e491e321f8c9.jsonl', '2886db2f-be9d-4ce6-85a6-1c9234c169d8.jsonl', '293640ab-da18-477f-9b04-80362e936dc2.jsonl', '297000ec-9848-4411-8f7a-399174c77e87.jsonl', '29776b29-8ccb-47b2-86e7-733faac1dbd4.jsonl', '299b0e6b-ed80-4f76-a28a-c276c023c130.jsonl', '29a2d3f3-cdda-47bb-b810-095f7f8a2423.jsonl', '29c71299-eb2a-4c87-98f1-393690568cfd.jsonl', '29cd860c-3909-4239-b36e-1107ee5da583.jsonl', '2a59fa26-a053-48cd-a7f6-0f92509ca36a.jsonl', '2ab4409b-955c-4565-834e-6f54e5e6dba9.jsonl', '2b4b718b-20b6-454e-86ae-24587812ccf6.jsonl', '2bd79a88-17a5-4fcc-846c-e0294fc8a33d.jsonl', '2ec4b8d1-c01c-4e7b-9095-4a9f1922eae7.jsonl', '2eda4ee9-ad4d-4444-a883-9a40ed6c94cf.jsonl', '2f484ee6-4877-4122-8296-da5cdc4c6dc2.jsonl', '304223d9-3d75-4889-9df4-e2b59b25b32b.jsonl', '309d760d-3b0a-4937-a35e-791a1369880b.jsonl', '31884464-64fe-4c3c-9338-ed54cff1a295.jsonl', '31f5ce5f-581e-447a-b835-cd0445f2361c.jsonl', '3390c0ee-d38b-4b8f-b7d2-51d9390c3d07.jsonl', '33a84a52-5de6-490d-8c2c-c07dcfdc8ba2.jsonl', '34651df6-0b5b-4191-8a21-2c1072413d80.jsonl', '34c6461f-98c1-4c6d-b487-e99a930fa680.jsonl', '34ea2768-d7d1-4640-af5b-29d3ab3a7941.jsonl', '3550ce60-11c3-4ae9-9b25-1c2fe654b73c.jsonl', '359efb40-ef57-4c31-b422-6641213bb5c9.jsonl', '35ecd778-0524-4ca8-a44f-54ae2500f053.jsonl', '36be1e94-9d0c-443d-96da-6d5e0335f37c.jsonl', '3725908f-8071-4df3-8b18-2f665e6de41a.jsonl', '37abc432-b9c8-46a7-b310-d2b69dd15785.jsonl', '37b8ede5-6820-44a5-962b-77092805e92a.jsonl', '38346737-6920-4b6f-9591-8958ae110e1f.jsonl', '38985df5-b644-4e35-96b3-3e2aa8b0604c.jsonl', '3972c7ec-3c0b-4289-b3b4-245f556442a0.jsonl', '3af7f2ac-806c-483a-8735-a814510f899d.jsonl', '3ccf8cc6-5b4b-4eb6-b12d-111d257bc27c.jsonl', '3d2a4183-5bb8-4e9b-a9f1-f15184be04ad.jsonl', '3dc5ef25-b869-4a2d-8a9e-84c742c1a26b.jsonl', '3dca837b-754d-4f17-a37f-72a3b88f548f.jsonl', '3dcf8ee0-822c-4f45-b26e-9e67852919af.jsonl', '3de8f0d9-7e14-4673-b181-61194ababee4.jsonl', '3e161842-63b1-493b-b3ae-d5740b66584f.jsonl', '3e26a64c-4229-40cd-88d9-001d114f5dc8.jsonl', '3e34bd8b-6804-444b-bbf4-f563e0a10d53.jsonl', '3e7eb3a7-0614-4356-ba4f-f93f5ec5be9c.jsonl', '3efdd1ab-243b-4b83-9363-17c6eea42795.jsonl', '3f047021-8040-42ab-a826-1ee198cf5841.jsonl', '4048e626-ad94-4ec3-bce7-2f339e04ede3.jsonl', '406d5f47-c817-4f03-b1a5-9c6944c26e7e.jsonl', '40a18c92-4222-417f-a932-6d53ce7ae12f.jsonl', '4193c96b-b44a-4f64-924a-3571913c8467.jsonl', '41dd8651-c5c5-4ec4-b48a-ca1749b94e44.jsonl', '4224bd2d-3f88-46fc-a9a2-e6e29ed8dbd6.jsonl', '424e5c02-0cdc-436c-baa3-ac01855c8b0c.jsonl', '4261da0f-4142-4156-bf5c-10cb9f3541d5.jsonl', '427413e5-417c-4901-b45e-585b56c528ff.jsonl', '42ea48c5-9def-4230-ae56-4598bf34cf90.jsonl', '42f23900-e7f6-4eb5-83a7-23d78ee12521.jsonl', '439c6ead-7cd6-46b2-99cd-7a032171fe6d.jsonl', '43bdd12a-f506-4d16-970d-578f4b13969b.jsonl', '43c113e1-0eb9-4057-a167-cc84ff04efdf.jsonl', '444854e3-f66b-489e-b41b-142765aa100f.jsonl', '4481217c-e359-4c8a-bb8b-26c61929cf66.jsonl', '44858051-6647-4f1a-997d-f40f269b0626.jsonl', '449d0880-35ff-4595-af10-2fc2988bbbd4.jsonl', '44ad0352-ed4e-48f4-a6d8-7610e955c776.jsonl', '453f146a-ef7a-4ea7-b554-d2839a8dd901.jsonl', '458ae897-43b0-4623-824d-c21e5154bb19.jsonl', '45c6bc17-f26f-40ce-9a2e-530f2bde94ed.jsonl', '467691d9-5031-4f62-80fa-c664d14fc32a.jsonl', '47aa8d15-41ed-444e-8043-935fe99aa2fd.jsonl', '48621be0-9498-4df4-9eec-17845adf288a.jsonl', '48a2d7db-247b-4b0a-a9ea-fefcc27573e1.jsonl', '498cd716-f5ad-4c55-94d1-38f78bcf16c2.jsonl', '49b74654-6c95-4915-81ce-f9749450a7bd.jsonl', '4a96182c-4526-4b8b-b195-fb6c6d2fc1e0.jsonl', '4aadf7a7-1e8e-42d4-a001-8f1db5373ee6.jsonl', '4bb40062-32ec-49e9-a468-96838c53c233.jsonl', '4cfea929-7a40-4224-8262-da28ce778f92.jsonl', '4d7cff3c-50a2-4cf0-ac69-3663a7d80129.jsonl', '4dc64930-66a0-4087-849d-cbb6da3bb32a.jsonl', '4dfecf64-c880-4691-8a99-19537fbb59ea.jsonl', '4e3588ef-cbd8-401c-ab55-09ba5b880490.jsonl', '4eab3e48-dc91-4731-be90-fbffe009a089.jsonl', '4efcc021-799d-48c1-9760-e4efc5f7f2b2.jsonl', '4f42332b-9d94-4b37-9eff-9666a932afcd.jsonl', '4f8adb6d-f28d-43f7-8e06-419bbeab9256.jsonl', '50238167-d6ce-452a-8267-ee41d13bd4f3.jsonl', '5116c1e3-7197-4bfd-aa83-6187337f3ecb.jsonl', '52171984-4a89-489c-8c45-01c19540f45b.jsonl', '524c0097-c470-479d-a34e-fa323825fc6d.jsonl', '532bb873-93b1-43d7-b83e-90fff560aa88.jsonl', '54ab71e7-6a4d-452b-b120-3a549f44d8a5.jsonl', '54d5b355-5c1b-4584-b631-9994464117bd.jsonl', '5519ad8a-e015-4cb8-878c-a7c791bb8b0f.jsonl', '557fc748-2a4f-4501-b4a6-91231b92c478.jsonl', '56348660-8e5d-479b-8d97-916b950115b3.jsonl', '563b5268-9468-4676-9ad5-90421185d573.jsonl', '56b10eb4-aa36-42dc-80b4-10354bff853c.jsonl', '56c5694f-ddc5-4c5f-9337-3c954e542106.jsonl', '56df3076-4d63-48ea-8943-78fff060f331.jsonl', '573387c9-359a-492d-bf3a-9a9d45075235.jsonl', '576cfa18-bd4e-4090-a109-839fac26bfd2.jsonl', '57c25ff5-4ca8-42e1-8df9-f420e720f5cf.jsonl', '57d1ec20-f00a-4a28-91d0-59de9e29d209.jsonl', '58392140-814e-489b-9bb8-9ffca4d46de2.jsonl', '5860a179-9647-4800-b983-76e9fcc7b5c9.jsonl', '58659ccd-a30c-4478-ab02-436b51857263.jsonl', '58a0af1a-bf01-4a4b-950c-4a69ac43ce61.jsonl', '58b7e5fd-eabe-4a02-96d7-1ff467056bf3.jsonl', '592f9262-0abb-4219-ab36-396afc2f993a.jsonl', '594ef1d3-3dfc-4829-9e98-5276c70c152f.jsonl', '59522557-f664-4b3e-aea9-1d5a4376b941.jsonl', '5a4252ef-caf7-40e3-b898-273074385fe7.jsonl', '5a88412d-8446-4464-b017-ab3860b90741.jsonl', '5acc38ca-df62-4a2e-b9ab-0edd0ee375bb.jsonl', '5db9df50-24cd-404e-8038-8ff10a1878f0.jsonl', '5ddd9d33-601f-4e30-a16f-b2bf7e64022a.jsonl', '5f7cd552-a3bb-4242-b6c7-8bd8e1c43708.jsonl', '5fc72cf3-781c-48f7-8b7a-5edabc952d87.jsonl', '600d4d81-817a-4348-8945-5f1efa1c3a12.jsonl', '608148dd-0389-4f48-b9fb-64042477fb9f.jsonl', '61d874c6-1641-4f00-8d3c-80d1efb38939.jsonl', '61dd2bf3-4593-4bc3-a18c-3c0d28ccc1e8.jsonl', '6308949e-4a99-47a3-aa6f-7bd26eb831a8.jsonl', '63ff741a-3aac-4b73-b8dd-866ab91243b5.jsonl', '6425d82c-b2a5-4a5c-94cc-5349ff7d07fc.jsonl', '64637d3e-aaa2-44e8-aab4-88436a657bbc.jsonl', '679773b0-1035-42db-a7f5-082db99b364a.jsonl', '67d5f2af-0f13-40b8-bf7b-d4d5da874ab9.jsonl', '68a1cdd5-af21-457b-8271-bc6e5a3ddcdb.jsonl', '68a8dda5-f9dd-4d79-ba56-8af090cb8c7d.jsonl', '68cee286-d8f3-40b6-a31f-f7af92d3a6e8.jsonl', '6952b17d-70e7-465e-bc7d-cec6c078e285.jsonl', '69e9bcce-2ad9-4b44-a24b-cda38003b5ff.jsonl', '6a117192-32c3-4502-9235-b6dbaa37ff1e.jsonl', '6ac8789b-a051-432c-8a16-b4ba90e9d69a.jsonl', '6b0b47a4-7dd5-4567-a419-afe7643bb4c7.jsonl', '6b304830-8524-484c-8425-c2c84af14404.jsonl', '6b44447a-eb59-4693-b0f3-4b67c10a3ad6.jsonl', '6b683e67-554c-4744-8b97-126e879c53fb.jsonl', '6c886a43-2a7d-4c84-9925-8f387b877cea.jsonl', '6c94a08a-8790-4d9b-99de-81803341315e.jsonl', '6d1f29f6-1577-4cfd-9ea2-ab561e40f586.jsonl', '6d5382a8-b8f7-4b8b-918f-cf242d65608f.jsonl', '6da23064-4210-45ad-9c66-5445d8cbca8e.jsonl', '6eff3337-7742-4e66-be08-f27b49bd7c47.jsonl', '6f171b10-8349-423d-b31d-fbed8c5b9d27.jsonl', '6f266993-575f-4a5a-a0da-bc802045968f.jsonl', '6f5892bc-228b-4640-8073-8361de2f6fdb.jsonl', '6f859056-8518-4993-8d15-51e05ab18ff8.jsonl', '701eb097-c97f-4fd8-9230-76dc87c8b5b6.jsonl', '7078614a-afb1-4d4b-906e-a1cf9a04197b.jsonl', '70abea19-95d4-4fcb-8496-6f3e765878a3.jsonl', '7124747d-dcce-4f00-ab79-0ae611cac2d2.jsonl', '7286180f-0990-46d4-8708-c8cbf4ddc790.jsonl', '72cd179d-7323-4e4b-90ab-8fe796fe06eb.jsonl', '72d3cf8b-353b-448a-9b1f-d0cc7121e29e.jsonl', '7320e5c4-a452-4dae-bc38-a9b7e9fd4720.jsonl', '73325b24-aaca-4798-832b-57649e69c074.jsonl', '7337ee0e-2997-44e1-b982-c8b81ef8de18.jsonl', '743fa201-3b0f-4e8d-823c-f0ef44e6e9cc.jsonl', '7479b8db-1ecd-4724-a89f-a9af118503a5.jsonl', '75959544-40a0-489b-a72c-73dc2f99e437.jsonl', '75c63454-601a-42ab-aa5b-cb84e1581ff7.jsonl', '75d1c3f2-9acc-47ed-975f-11ae0486c6f3.jsonl', '76353974-2a54-42d4-9168-70057e45afea.jsonl', '769e6d85-69a6-4687-99f9-26767a941d46.jsonl', '780a624e-32ed-46a7-b4b1-592ce5411dad.jsonl', '782476b3-c5ee-485b-bba3-9e58e6365ea9.jsonl', '78f4a16e-ca98-42e1-bce2-2d3ef39b404f.jsonl', '792c9ee9-b00a-4ef8-9965-074112f313f2.jsonl', '7a2fbf07-411c-46f0-b91b-8c4e6e38be20.jsonl', '7a31243c-c4c1-4e82-a990-444242d1a318.jsonl', '7b4e1305-4521-4d40-99dd-a23c18482566.jsonl', '7b57c14e-1fc4-4e68-aee1-f8e9a5817b29.jsonl', '7ba161e3-08ab-4a43-a239-ae3d2c1786f3.jsonl', '7ce412ff-ef78-48a2-9c8e-b72c7422eccb.jsonl', '7d5c9743-a5f1-4a75-9554-0a351b287859.jsonl', '7e56b633-f17f-4bc2-abd3-85d7e59966e1.jsonl', '7f0e97d9-de43-47ba-8942-15df08b15d38.jsonl', '7f3e1d19-39fb-40c7-a19a-7c02ef44148b.jsonl', '8014a194-75c0-43db-9bbb-e15c973187c9.jsonl', '80299374-fe61-495c-a747-fac29112ca6a.jsonl', '8056d253-35b8-43b0-bfc5-f8ff0361cc8a.jsonl', '80b97a26-054b-45c2-a3c9-4c6801a4e84b.jsonl', '810c2a4a-3568-412d-8af8-f1b73a9fc661.jsonl', '818b4450-3302-43a1-9668-ef93646384c4.jsonl', '82103746-d694-46fb-9497-69c341142c00.jsonl', '831e7a3a-7dbc-43b5-9ad4-1beafe6340a3.jsonl', '84a9f463-3938-4d7e-996d-bcd693f528e2.jsonl', '861660a8-be93-4686-a188-8ede24f4490b.jsonl', '8710ffa7-514d-41ca-be8e-83ace27438b3.jsonl', '8768a31c-2504-4391-b17c-120a24431651.jsonl', '87a74ad9-5384-4983-8d2c-d0bed2588e5e.jsonl', '89b18fc7-76d5-4c38-939b-90d6a588fa1f.jsonl', '8a341724-8ea0-4d17-ad2d-c984afd57342.jsonl', '8a4aa9dd-3866-4463-b327-a5974afaae12.jsonl', '8ab0c4c0-eab8-4bf6-a57a-b1700a14e077.jsonl', '8b480cd6-7ff4-4d7e-96b3-d1983ab5a29f.jsonl', '8bb40b67-42e2-4b6d-806a-08c13e1387c2.jsonl', '8bf7e3b6-ab2c-4942-bd92-1b50567a21e3.jsonl', '8d69a33f-1d52-449f-ad1d-e07a8637b454.jsonl', '8dfeb92d-fc46-4f01-b99e-14c05c7281e7.jsonl', '8e6ca082-0370-4c6e-b5e4-fb2544606903.jsonl', '8e9df3d2-fb2c-4162-a4a9-12f9bd420c05.jsonl', '8ea43855-daa2-41dc-9d31-cf6fabf49957.jsonl', '8ea774a5-5ae3-4a1f-8877-5e12395e08d2.jsonl', '8efb4ac6-890a-4f2d-9e69-7a839de69296.jsonl', '8f69b900-9979-437f-b1ba-1c06b417fb00.jsonl', '91784a69-3c6f-4a48-a9b2-0c360ba8d825.jsonl', '9278c143-3cbb-4de8-96f7-d98932fe958e.jsonl', '92a1a44b-76a3-4416-9cea-b4682c529a76.jsonl', '92c929e6-4c7c-4c00-aab6-a4fbfe4b041e.jsonl', '9382bfba-a092-44e1-9ff1-d372678a2bac.jsonl', '939a5c95-7add-4df1-be63-4f1f70dd11ce.jsonl', '93b5f503-d0ce-42df-bbae-8b41a418bc2f.jsonl', '940b2eed-a079-4158-aba0-aad5c90b68c1.jsonl', '94afa4a9-0e25-4e0b-b7ce-efccd8674138.jsonl', '95084d5b-48f6-460b-bcf4-f8d6910fe3d2.jsonl', '95b8cf28-a878-4817-8ba1-80757da52337.jsonl', '965b0ae6-d6a0-4057-8528-d82473cde7e3.jsonl', '968d5708-160e-47f2-9df4-c1a938724f18.jsonl', '96930b5f-85ac-482a-8ae9-d102610068b7.jsonl', '973ee8a1-0c15-4023-aa67-cb5c46b934a2.jsonl', '9782eda0-ecf1-4408-8e30-1239dedd7bd4.jsonl', '984ef9d5-924e-428e-9e49-efd73edd9c27.jsonl', '9889a28a-6728-47e6-8c32-3ac957990c2c.jsonl', '98b43c95-36b6-4226-8404-8719078697f0.jsonl', '98d8fc06-c5e4-4044-9d81-6daa69d26bf2.jsonl', '98dea49e-4538-4874-a911-73eadd7b4a59.jsonl', '99a2fe51-3ff2-468d-aa12-d92846bcb912.jsonl', '9a1fbf17-1f8a-4ee2-bf94-00bc647e24d7.jsonl', '9ae0cd80-a34d-4508-9a7b-72ec1d3fc1a6.jsonl', '9b50973d-8d68-427c-92a2-54136a4cffa7.jsonl', '9b564d71-c538-411d-9c39-034be84a3c70.jsonl', '9bb66ac3-8188-4367-b2dd-e46d924fb0c7.jsonl', '9bc4499e-ff70-424b-ae16-ac4efcd5a7ef.jsonl', '9d0a052f-b551-4632-8c80-af255f80fb94.jsonl', '9d41264a-3b6c-4c52-b2f9-e05f958889b4.jsonl', '9d7974fc-fe15-4742-a588-c68447d6a55b.jsonl', '9d8ca5af-fd98-466b-870d-15922694cf2c.jsonl', '9dd08db3-b1bf-4d90-91c2-af0826143abe.jsonl', '9e43ad48-c96c-4b74-b9ea-365e47294a14.jsonl', '9e48c46a-2aec-4b01-84d4-1f6a6a76fd55.jsonl', '9eee3547-9ca0-4fe4-8be5-a6c2dd2ef96d.jsonl', '9f8624cc-7f0c-46d0-8d99-f90fb0bd22bf.jsonl', '9fcc891b-f924-4463-b5cb-7e42c9b6560c.jsonl', 'a097cfc3-3ee7-4f58-976f-0b0f34678aaf.jsonl', 'a0d97c61-a248-44ae-a495-ca5782e7ea77.jsonl', 'a103c093-555b-47d9-ba9c-5788818d93e2.jsonl', 'a12127f8-0bb2-4d8a-9365-3473fe70efb6.jsonl', 'a13252f3-3838-440d-943f-72dc6d239834.jsonl', 'a3131e6e-4218-480b-b17a-0e56a91dfdf5.jsonl', 'a32ad298-b281-4b81-ba49-63d96b7f82ed.jsonl', 'a3a039e1-7b93-4454-9197-a34150b985b0.jsonl', 'a3c4d4ba-2899-4422-880d-1fd954f8bff8.jsonl', 'a3fe6fa1-59e8-499e-bd30-a2b0cfa0ba2c.jsonl', 'a482e45b-20d5-4860-b699-f8596b300064.jsonl', 'a4949345-9e5b-41bc-b8c2-7aeb08cc776a.jsonl', 'a5096b8b-80e3-46fa-b2b0-068f9f0875cc.jsonl', 'a523b2cd-bd21-4799-8d37-df2e6b3f8b83.jsonl', 'a5ec582d-d436-4a31-b0bf-404ddde1b4f3.jsonl', 'a6f49474-f09d-4172-8813-68588f030fa4.jsonl', 'a7708304-c724-4a32-93f6-32ea1ab67091.jsonl', 'a830d1c0-56db-4f72-b680-6c68148a0968.jsonl', 'a836a9aa-a17e-4709-b37f-281ef0fb74ca.jsonl', 'a8925642-7b0b-47d3-a5a6-e16290c992a0.jsonl', 'a89f24d4-3c48-4874-a35f-2d0c6552a159.jsonl', 'a8b4ca8c-b4ad-4d37-9e64-ae3d8a9baecb.jsonl', 'aa046b8d-7fe1-4577-9993-544dfe8919c0.jsonl', 'aa210ee3-ed26-43be-89a5-e819a2452935.jsonl', 'ac0f1fb5-0cfa-48a5-9661-fc0074d4fed6.jsonl', 'ac86426a-60d4-4d59-8ce0-f4871c571ffd.jsonl', 'acd2d1c9-cca0-4014-8f25-2ee01edf08ed.jsonl', 'ad1e3ec9-d29d-4186-962e-de36e1c439ac.jsonl', 'ad407429-3f42-4572-8f5d-bc3d8e187218.jsonl', 'adad8127-df3b-4c1c-acd7-d0ff15916f23.jsonl', 'aded115c-e4a7-4c8b-ba51-d6bbb6feeacb.jsonl', 'ae4917cc-3d5b-48e6-9768-e78ee394a2dd.jsonl', 'af65473e-460e-40d0-8487-a9cc13072fd0.jsonl', 'b0491d43-29df-42da-ba99-abe3747a37f2.jsonl', 'b0546c29-4ecc-4770-b4c6-695332e87d1a.jsonl', 'b09309c4-108e-4102-b9bf-8904e1328b99.jsonl', 'b0a32d50-7cfc-4de2-bf30-6335550f4574.jsonl', 'b0a405eb-39eb-4d2a-bf75-c01be4d365ea.jsonl', 'b10d3eba-2c3f-456e-9978-eb0738dc069a.jsonl', 'b13ab88a-7fda-417a-ac0f-bcaf7b740c49.jsonl', 'b1b8e876-6bdc-41c7-88b1-5855c7fa1d50.jsonl', 'b2235d97-008b-43ea-9d94-879a31bc470f.jsonl', 'b23cc381-2e02-4fc4-8f5c-1047b947da5e.jsonl', 'b2d21fdd-2abb-48b3-8d9f-3f4902385035.jsonl', 'b2ef8862-049a-49e8-8df5-cc2169da2811.jsonl', 'b2f152c9-d661-4246-a080-97536ada1dee.jsonl', 'b37ecc27-6cef-4396-b68c-f696102bd820.jsonl', 'b40c530c-aa1c-4556-a0d9-e373d56fbccd.jsonl', 'b4c619d7-09c8-482d-afe6-a5f5b21931fe.jsonl', 'b4d5bf47-4ceb-4d49-b0a0-01cfef6f67d6.jsonl', 'b4f1a576-5ce4-43b0-abb7-40b6ec758a91.jsonl', 'b4f9a6de-448c-46cc-9377-f2fa8b27bf17.jsonl', 'b5d508b1-91d0-4016-9b43-862df45e0dd6.jsonl', 'b6b2322a-7148-4b30-a8f0-de7876fe5f89.jsonl', 'b6fa0325-ef65-4484-8f7e-7a3945938830.jsonl', 'b739e42e-ba7d-40da-8e4b-f5472506a442.jsonl', 'b8579c5c-947c-4722-a730-dfcf29fec8a4.jsonl', 'b8be4938-c562-469b-96b2-d8702a32c2ba.jsonl', 'b97d782f-4d5e-4364-b265-bda964c408e3.jsonl', 'b997a9fa-5396-4196-a935-37a648097b60.jsonl', 'ba953010-bb20-461c-9451-4fbf549d8959.jsonl', 'baa37594-99f3-4e55-ba1d-d50ec71ab5d8.jsonl', 'bae682ee-3050-4d0f-813a-ea714d5246fc.jsonl', 'bb0de98e-871f-41f7-a628-79ccd6f2baf3.jsonl', 'bb40b16b-e9e9-4918-a57b-56dc5a568281.jsonl', 'bb4ce626-b216-47ed-8d14-90af8a4231d7.jsonl', 'bbcfa791-f816-4c9e-8355-e0f75144c588.jsonl', 'bbd55d08-2b5d-47c8-89f7-fd51c1bedad0.jsonl', 'bc2efa8d-611b-42ee-aadf-2433fd465b69.jsonl', 'bc62dab2-9cb7-46cd-852e-11710e6cebf2.jsonl', 'bc844082-9e46-4b4c-a6f6-25950e3fe552.jsonl', 'bca1b89a-af11-4a57-95c7-87f6d3b0e3a6.jsonl', 'bdb2c893-b70f-4bd7-a434-33320c95e153.jsonl', 'bdb60a53-c0a4-4694-b230-cdabcbf51a91.jsonl', 'bf0d2491-e42e-40f8-98fa-17b7260dbd0d.jsonl', 'bf48f059-7dcf-4d41-8d4c-96bb5b4d69d5.jsonl', 'bfaa8ff2-af2c-4ceb-a473-c325c5c295cd.jsonl', 'bfbdd157-b413-4c8b-ab3c-81e46def27d6.jsonl', 'bfd73235-95ba-4a3d-a171-303eefe98d81.jsonl', 'bff42cfb-2730-490c-9da8-a0987e686ec9.jsonl', 'c0523e60-d06e-453d-9bab-79e7c8351695.jsonl', 'c0bae534-e0b8-4e92-ae9e-d1ec64ab1e32.jsonl', 'c0d6bc20-f79e-49bc-9123-7e9c1b1124a2.jsonl', 'c10e8b33-29fa-47c3-841a-5002d3095bc7.jsonl', 'c113a5cd-3479-4a14-a588-08f386bc4a12.jsonl', 'c2d4faa5-18e5-4852-bc23-b0716056612b.jsonl', 'c30af9f0-30f4-4bd9-bfd5-4645468440d7.jsonl', 'c34e89aa-692d-4b25-853f-af5f5ab17193.jsonl', 'c383a5ce-4e7d-49f6-a825-9cf4c1215bc4.jsonl', 'c3a070fe-56af-4833-b390-19d3abc8ab6d.jsonl', 'c3a6ab48-4663-4749-b587-fec482641d8c.jsonl', 'c468bbb8-f2cd-4952-8272-e1ff7b16fd93.jsonl', 'c4c23c5c-557f-4c92-8efa-442adccd2702.jsonl', 'c647fea3-91b3-4923-876a-b9bf5aaa47a2.jsonl', 'c671edba-5e29-41f2-8812-e9f78151f82f.jsonl', 'c6f6eae4-dcd4-4277-816a-95f2b774af81.jsonl', 'c70b0ea8-52eb-4ba4-9563-543c9639cfbe.jsonl', 'c722b3db-1714-48b4-ba38-e00d94631d9f.jsonl', 'c7398ee3-97d8-40b5-badb-c8f0f9ec1f2b.jsonl', 'c79fb03b-5ef2-4d9b-9ebb-c0d744fd46e7.jsonl', 'c831b95b-12d5-47fc-a92b-2ae597a64dc8.jsonl', 'c937e2e7-845b-48d5-854f-3906223279e9.jsonl', 'c9de4a5e-01f3-4fe1-bc5d-33aed2fd4e8a.jsonl', 'cac18521-2a66-4aa3-90ff-b2e70b553eec.jsonl', 'cafbcf92-df54-4930-99d2-c298aa0db060.jsonl', 'cc045959-58e6-4c36-9453-b62ca7079a81.jsonl', 'ccdfe1ae-9234-491d-b714-54815327ed07.jsonl', 'cd66ccb5-d70d-4762-8bae-cb68fd1b927b.jsonl', 'cdada2ca-e73a-4b08-94e5-95b0c9af010f.jsonl', 'cdd5c3aa-498e-4722-89f3-420e5a36ad06.jsonl', 'ce19e577-6924-4056-a77b-e6805e4327e2.jsonl', 'ce332593-f894-43a0-89d6-64d3e308e386.jsonl', 'cea059db-5a3b-4b08-bb35-cd9f5e5849d3.jsonl', 'cefbc60c-8fb0-404b-b51d-7ca888b8dfb4.jsonl', 'ceffdc14-a525-459d-9b43-f2ae07ff2a2c.jsonl', 'cf5c4500-30ff-42cc-ad41-4625d8b81818.jsonl', 'cfbe4547-38c1-428e-a15a-8988f30aabec.jsonl', 'd021a2c4-69e8-4e98-a06d-7ecb2a7b736f.jsonl', 'd123b55d-d1bc-420b-8504-4c017e311247.jsonl', 'd1ee339f-0774-4bae-ad50-80c05db4de93.jsonl', 'd2047237-7458-4ba0-82a0-cd6c66f919cd.jsonl', 'd27be763-0842-4151-a997-4a3b82a99e0b.jsonl', 'd401587e-d824-4c1a-8b15-962ef586a45b.jsonl', 'd4c61cc2-fb44-4817-9841-1773087ce4e1.jsonl', 'd4ef8bb1-a51e-4740-8456-2964cb6f3a8b.jsonl', 'd5b7e377-a099-481a-a161-fe08f60d8922.jsonl', 'd652c884-223b-437d-9ca2-47fc5930fd32.jsonl', 'd6861758-f0fa-41e8-aa49-8120312624bc.jsonl', 'd6de6a0e-bb03-4a03-91a6-e274e187cef9.jsonl', 'd6fd413a-fa70-414a-9faf-334aabcbeed4.jsonl', 'd7413579-25b4-4c12-a2bf-a69f8adf3976.jsonl', 'd9c8d768-146e-4410-9b2b-1ed8cd8421a7.jsonl', 'dbf3555c-841f-4ae4-b137-1846d1e84e00.jsonl', 'dc2a97a6-545d-452e-b164-1fe3017c5a48.jsonl', 'dcf81e30-bfc2-48b9-9f34-2bf54e4faa4d.jsonl', 'de979c6c-b2ff-42e4-a7a7-68b957509b2d.jsonl', 'dea19597-df4a-46c9-8e70-38c47a869c1e.jsonl', 'df94cacb-d593-4749-9c9d-c3d3de2f7299.jsonl', 'e025faf5-f60d-43de-9a24-f029bbd18875.jsonl', 'e0e0b288-2a9a-45a1-a1a3-e5cad1d0b920.jsonl', 'e1a7c5b3-9eb1-43a3-bf42-775bec9136a5.jsonl', 'e34a186a-b270-472a-9476-0f7b84e9aee5.jsonl', 'e380c9e1-23ea-4039-bc98-6dc2eeef33f4.jsonl', 'e3b8febe-d74f-4a07-8ea5-215d93a87e69.jsonl', 'e4414ece-256f-4823-87d6-aebbc0ab830e.jsonl', 'e49adaa8-0991-4cab-a413-64cd8176f509.jsonl', 'e4cd7839-f78c-4f68-a380-7e185715bcf9.jsonl', 'e5cd808a-26e9-4dfb-97bf-1278b8554486.jsonl', 'e602123d-1ee4-4944-8c0e-8c6863a49ae3.jsonl', 'e6037cc1-1ab1-4512-829b-86a19e19b6ad.jsonl', 'e6b0ea4e-3eee-407a-bd8c-05bf270ffb7e.jsonl', 'e7046c1c-417f-47be-8fc9-e2e580dd3919.jsonl', 'e738b765-9198-417e-99f4-8ebe66e127f6.jsonl', 'e7a2de9a-e1fb-41dc-9287-5191cf6d9629.jsonl', 'e8c6ba94-7479-4538-bbba-4ab10db64b6e.jsonl', 'ea1311b6-8349-44b2-a043-393afd5521c1.jsonl', 'eb0bb0d9-d756-48ff-a70b-fc6762c9c353.jsonl', 'eb130919-733b-4e2a-8ca8-4815fbd8829a.jsonl', 'eb598486-ec17-4096-98e6-785b35e0c2c7.jsonl', 'ec194fd5-fcb8-4617-bd8b-dd75c9ca4aa0.jsonl', 'ec2872db-e125-4e3d-9d1a-619e78d06dca.jsonl', 'ec9096a0-aeb2-47d4-93a6-4314f32a16fe.jsonl', 'ecc8284b-b3aa-4c3b-a4bc-f70bf7adac47.jsonl', 'eccd5f70-9018-421b-8bc0-ee3315b99f63.jsonl', 'ed0c12f2-bc9e-4fb2-9d80-dece895233d2.jsonl', 'edaa693b-2342-4a1d-8ff0-234010eee931.jsonl', 'f10bba90-0f00-48b9-8141-2037d01c0f89.jsonl', 'f1e25df9-ca08-4272-8767-df0f9bdcbaba.jsonl', 'f2451f03-ff35-4208-aca4-00aa56c7bf6e.jsonl', 'f288ae9f-e328-41aa-94d6-22c63ceb1ada.jsonl', 'f29fe395-abb6-4cba-9610-91adaeb1c4e2.jsonl', 'f2ea55ac-cbd2-4803-9fc9-400838a34a72.jsonl', 'f3bc78fa-9a32-40c3-a4af-d3358c3b955f.jsonl', 'f4134916-be77-4661-84c9-d86cae3a13ac.jsonl', 'f42632ad-583f-4545-905f-96aae29f3045.jsonl', 'f463a271-e9c6-4441-9cb8-de46c1df1287.jsonl', 'f46a24ba-631b-4732-85ba-58a07b5acd68.jsonl', 'f47193f9-79a0-4b70-9a57-167825fd6f95.jsonl', 'f487812e-4644-4928-9252-2a3527f799b5.jsonl', 'f4ec84f7-799c-4fbb-8bde-ebbe11d0f6fc.jsonl', 'f64987e0-a582-4e89-9c06-72435a044960.jsonl', 'f6f04c82-a2f0-4fd4-a525-21ed210c9394.jsonl', 'f7080aee-eb4a-49e7-a158-220d253a48df.jsonl', 'f712fdbe-0edc-4c80-9b6c-a982b529892c.jsonl', 'f7385613-0b01-43ae-ba1e-d3113b597ceb.jsonl', 'f751c4c9-3942-43b5-800f-a1b62480b5a8.jsonl', 'f81cdd22-198d-4342-abc7-741bfeff0891.jsonl', 'f8380fd5-bc48-4ad4-bd2c-a66d59dfe8c3.jsonl', 'f8881263-2052-48c0-a4ca-4de1dc6ac51e.jsonl', 'fa4a67d7-a36b-4de3-9c1d-664a288fff45.jsonl', 'fb94bd19-2e4d-46e1-8cb8-8bf15824c01c.jsonl', 'fc59157b-21c8-486c-ad3c-07db92bdeed4.jsonl', 'fdbe2426-ac6b-404d-b863-e6de3f2cacc6.jsonl', 'fdcbc45b-065e-48b4-8b60-d0d53428f1cb.jsonl', 'fddca773-7fdc-46b2-af07-043a51fbd51f.jsonl', 'fde78b54-267d-4222-9b84-f21807d0ae94.jsonl', 'fe075185-597c-4f3b-8350-6da2e2ea1e76.jsonl', 'fe11b1d5-680c-4fbd-9a23-12b5f892c0e2.jsonl', 'fe3b535f-a3cf-4bca-9640-66b59220ad33.jsonl', 'fe8b5286-a963-4ff6-937b-a9476a1b696b.jsonl', 'ffd9075a-c4e9-45c4-9e63-93ecee3d80e5.jsonl']\n",
      "\n",
      "Testing parsing for file:  dbfs:/Volumes/ep/cookbook_local_test/product_docs/096cda57-42e2-4c13-9756-34628413b0c9.jsonl\n",
      "\n",
      "{'content': \"<product_category>Electronics</product_category>\\n<product_sub_category>Smart Thermostat</product_sub_category>\\n<product_name>ThermaPro ST300</product_name>\\n<product_doc>## ThermaPro ST300 Smart Thermostat\\n\\n### Introduction and Overview\\n\\nThe ThermaPro ST300 is a cutting-edge smart thermostat designed to provide seamless temperature control and energy efficiency for your home. Offering compatibility with popular smart home assistants like Alexa and Google Home, the ST300 allows you to manage your home environment effortlessly via voice commands or through its intuitive mobile app.\\n\\n**Key Features:**\\n- Wi-Fi connectivity for remote access\\n- Voice control via Alexa and Google Home\\n- Energy-saving modes for efficient power usage\\n- Customizable temperature scheduling\\n- Easy-to-use mobile app for remote management\\n- Real-time alerts and notifications\\n\\n**Box Contents:**\\n- ThermaPro ST300 Smart Thermostat\\n- Wall mounting plate\\n- Screws and anchors\\n- Quick Start Guide\\n- Warranty card\\n\\n### Installation Guide\\n\\n**Step-by-Step Installation Instructions**\\n1. **Turn Off Power**: Ensure the HVAC system power is off before installation to prevent electrical shock.\\n2. **Remove Old Thermostat**: Remove the existing thermostat and label the wires for easy identification.\\n3. **Mount ThermaPro ST300**: Attach the wall mounting plate using the provided screws and anchors.\\n4. **Connect Wiring**: Carefully connect the labeled wires to the corresponding terminals on the ST300.\\n5. **Attach Thermostat**: Secure the smart thermostat onto the mounting plate.\\n6. **Power On**: Restore power to the HVAC system.\\n\\n**Tools Required:**\\n- Screwdriver\\n- Wire labels (optional)\\n\\n**Safety Precautions:**\\n- Switch off power at the circuit breaker to avoid electrical hazards.\\n- Handle wiring with caution to prevent damage.\\n\\n### Setup and Configuration\\n\\n**Connecting to Wi-Fi**\\n- Open the ThermaPro mobile app and select 'Add Device.'\\n- Choose 'ThermaPro ST300' from the device list.\\n- Select your home Wi-Fi network and enter the password.\\n\\n**Pairing with Smart Assistants**\\n- For Alexa: Enable the 'ThermaPro Skill' in the Alexa app and follow the prompts.\\n- For Google Home: In the Google Home app, tap 'Add Device,' then 'ThermaPro ST300.'\\n\\n**App Installation and Initial Setup**\\n- Download the ThermaPro app from the App Store or Google Play.\\n- Follow the on-screen instructions to complete your account setup and device pairing.\\n\\n### User Interface and Controls\\n\\n**Thermostat Display and Controls**\\n- **Home Screen:** Displays current temperature, mode, and upcoming schedules.\\n- **Navigation:** Use the touch interface to adjust settings and access features.\\n\\n**Mobile App Navigation Tips**\\n- Access the menu to configure settings, schedules, and energy-saving modes.\\n- View usage history and reports for energy consumption insights.\\n\\n### Features and Functionalities\\n\\n**Energy-Saving Settings**\\n- Activate 'Eco Mode' to automatically adjust temperatures when no one is home.\\n- Set temperature ranges to avoid excessive heating or cooling.\\n\\n**Scheduling**\\n- Create daily or weekly schedules through the mobile app.\\n- Customize settings for different times of the day.\\n\\n**Remote Control**\\n- Adjust the temperature or change settings from anywhere using your smartphone.\\n- Receive notifications about system status changes and energy usage.\\n\\n### Troubleshooting\\n\\n**Common Issues and Solutions**\\n- **Wi-Fi Connection Issues:** Ensure the Wi-Fi network is 2.4GHz. Reset both the router and thermostat if problems persist.\\n- **Device Not Responding:** Perform a factory reset by pressing and holding the reset button for 10 seconds.\\n\\n**Error Messages and Corrective Actions**\\n- 'Network Error': Check Wi-Fi signal strength and retry connection.\\n- 'Temperature Sensor Fault': Inspect wiring connections and contact support if necessary.\\n\\n**FAQs**\\n- **Q: Can I use the ST300 without Wi-Fi?**\\n  A: Yes, the thermostat will continue to control temperatures locally, but remote features will be unavailable.\\n\\n### Maintenance and Support\\n\\n**Regular Maintenance Tips**\\n- Dust the thermostat regularly to avoid debris obstructing sensors.\\n- Ensure firmware is up-to-date through the app to maintain functionality.\\n\\n**Warranty and Customer Support**\\n- The ThermaPro ST300 comes with a one-year warranty covering manufacturing defects.\\n- Customer Support: support@thermapro.com | 1-800-555-0199\\n\\n### Technical Specifications\\n\\n- **Power Requirements:** 24V\\n- **Wi-Fi Standards Supported:** 802.11b/g/n\\n- **Dimensions:** 4.5 x 4.5 x 1 inches\\n\\n### Safety and Compliance\\n\\n**Important Safety Warnings**\\n- Disconnect power before installation or maintenance.\\n- Follow all safety precautions in the Quick Start Guide.\\n\\n**Compliance**\\n- Complies with FCC regulations and Energy Star standards.\\n\\nBy carefully following the instructions outlined in this manual, you can enjoy the comfort and convenience offered by the ThermaPro ST300 Smart Thermostat.</product_doc>\\n<product_id>096cda57-42e2-4c13-9756-34628413b0c9</product_id>\", 'parser_status': 'SUCCESS', 'doc_uri': '/Volumes/ep/cookbook_local_test/product_docs/096cda57-42e2-4c13-9756-34628413b0c9.jsonl', 'last_modified': Timestamp('2024-11-06 18:15:57')}\n"
     ]
    }
   ],
   "source": [
    "from cookbook.data_pipeline.parse_docs import load_files_to_df\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "raw_files_df = load_files_to_df(\n",
    "    spark=spark,\n",
    "    source_path=source_config.volume_path,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {raw_files_df.count()} files from {source_config.volume_path}.  Files: {source_config.list_files()}\")\n",
    "\n",
    "test_records_dict = raw_files_df.toPandas().to_dict(orient=\"records\")\n",
    "\n",
    "for record in test_records_dict:\n",
    "  print()\n",
    "  print(\"Testing parsing for file: \", record[\"path\"])\n",
    "  print()\n",
    "  test_result = file_parser(raw_doc_contents_bytes=record['content'], doc_path=record['path'], modification_time=record['modificationTime'], doc_bytes_length=record['length'])\n",
    "  print(test_result)\n",
    "  break # pause after 1 file.  if you want to test more files, remove the break statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fb6db6c-faa0-4dac-be84-a832bbbb49b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš«âœï¸ The below cell is boilerplate code to apply the parsing function using Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "165706b2-5824-42e7-a22b-3ca0edfd0a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the raw files from /Volumes/ep/cookbook_local_test/product_docs...\n",
      "Applying parsing & metadata extraction to 553 files using Spark - this may take a long time if you have many documents...\n",
      "Parsed 553 / 553 documents successfully.  Inspect `parsed_files_no_errors_df` or visit https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs__v2 to see all parsed documents, including any errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>parser_status</th>\n",
       "      <th>doc_uri</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;product_category&gt;Online Services&lt;/product_cat...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/a...</td>\n",
       "      <td>2024-11-06 18:15:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;product_category&gt;Home Office&lt;/product_categor...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/4...</td>\n",
       "      <td>2024-11-06 18:16:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;product_category&gt;Kitchen Appliances&lt;/product_...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/3...</td>\n",
       "      <td>2024-11-06 18:15:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;product_category&gt;Gadgets&lt;/product_category&gt;\\n...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/7...</td>\n",
       "      <td>2024-11-06 18:15:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;product_category&gt;Smart Home&lt;/product_category...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/1...</td>\n",
       "      <td>2024-11-06 18:15:49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>&lt;product_category&gt;Health and Fitness&lt;/product_...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/8...</td>\n",
       "      <td>2024-11-06 18:15:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>&lt;product_category&gt;Subscription&lt;/product_catego...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/8...</td>\n",
       "      <td>2024-11-06 18:15:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>&lt;product_category&gt;Men/Women/Kids&lt;/product_cate...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/5...</td>\n",
       "      <td>2024-11-06 18:15:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>&lt;product_category&gt;Furniture&lt;/product_category&gt;...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/0...</td>\n",
       "      <td>2024-11-06 18:15:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>&lt;product_category&gt;Fashion&lt;/product_category&gt;\\n...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/4...</td>\n",
       "      <td>2024-11-06 18:16:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>553 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content parser_status  \\\n",
       "0    <product_category>Online Services</product_cat...       SUCCESS   \n",
       "1    <product_category>Home Office</product_categor...       SUCCESS   \n",
       "2    <product_category>Kitchen Appliances</product_...       SUCCESS   \n",
       "3    <product_category>Gadgets</product_category>\\n...       SUCCESS   \n",
       "4    <product_category>Smart Home</product_category...       SUCCESS   \n",
       "..                                                 ...           ...   \n",
       "548  <product_category>Health and Fitness</product_...       SUCCESS   \n",
       "549  <product_category>Subscription</product_catego...       SUCCESS   \n",
       "550  <product_category>Men/Women/Kids</product_cate...       SUCCESS   \n",
       "551  <product_category>Furniture</product_category>...       SUCCESS   \n",
       "552  <product_category>Fashion</product_category>\\n...       SUCCESS   \n",
       "\n",
       "                                               doc_uri       last_modified  \n",
       "0    /Volumes/ep/cookbook_local_test/product_docs/a... 2024-11-06 18:15:34  \n",
       "1    /Volumes/ep/cookbook_local_test/product_docs/4... 2024-11-06 18:16:00  \n",
       "2    /Volumes/ep/cookbook_local_test/product_docs/3... 2024-11-06 18:15:53  \n",
       "3    /Volumes/ep/cookbook_local_test/product_docs/7... 2024-11-06 18:15:46  \n",
       "4    /Volumes/ep/cookbook_local_test/product_docs/1... 2024-11-06 18:15:49  \n",
       "..                                                 ...                 ...  \n",
       "548  /Volumes/ep/cookbook_local_test/product_docs/8... 2024-11-06 18:15:56  \n",
       "549  /Volumes/ep/cookbook_local_test/product_docs/8... 2024-11-06 18:15:42  \n",
       "550  /Volumes/ep/cookbook_local_test/product_docs/5... 2024-11-06 18:15:32  \n",
       "551  /Volumes/ep/cookbook_local_test/product_docs/0... 2024-11-06 18:15:59  \n",
       "552  /Volumes/ep/cookbook_local_test/product_docs/4... 2024-11-06 18:16:10  \n",
       "\n",
       "[553 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cookbook.data_pipeline.parse_docs import (\n",
    "    load_files_to_df,\n",
    "    apply_parsing_fn,\n",
    "    check_parsed_df_for_errors,\n",
    "    check_parsed_df_for_empty_parsed_files\n",
    ")\n",
    "from cookbook.data_pipeline.utils.typed_dicts_to_spark_schema import typed_dicts_to_spark_schema\n",
    "from cookbook.databricks_utils import get_table_url\n",
    "\n",
    "# Tune this parameter to optimize performance.  More partitions will improve performance, but may cause out of memory errors if your cluster is too small.\n",
    "NUM_PARTITIONS = 50\n",
    "\n",
    "# Load the UC Volume files into a Spark DataFrame\n",
    "raw_files_df = load_files_to_df(\n",
    "    spark=spark,\n",
    "    source_path=source_config.volume_path,\n",
    ").repartition(NUM_PARTITIONS)\n",
    "\n",
    "# Apply the parsing UDF to the Spark DataFrame\n",
    "parsed_files_df = apply_parsing_fn(\n",
    "    raw_files_df=raw_files_df,\n",
    "    # Modify this function to change the parser, extract additional metadata, etc\n",
    "    parse_file_fn=file_parser,\n",
    "    # The schema of the resulting Delta Table will follow the schema defined in ParserReturnValue\n",
    "    parsed_df_schema=typed_dicts_to_spark_schema(ParserReturnValue),\n",
    ")\n",
    "\n",
    "# Write to a Delta Table\n",
    "parsed_files_df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(\n",
    "    output_config.parsed_docs_table\n",
    ")\n",
    "\n",
    "# Get resulting table\n",
    "parsed_files_df = spark.table(output_config.parsed_docs_table)\n",
    "parsed_files_no_errors_df = parsed_files_df.filter(\n",
    "    parsed_files_df.parser_status == \"SUCCESS\"\n",
    ")\n",
    "\n",
    "# Show successfully parsed documents\n",
    "print(f\"Parsed {parsed_files_df.count()} / {parsed_files_no_errors_df.count()} documents successfully.  Inspect `parsed_files_no_errors_df` or visit {get_table_url(output_config.parsed_docs_table)} to see all parsed documents, including any errors.\")\n",
    "display(parsed_files_no_errors_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show any parsing failures or successfully parsed files that resulted in an empty document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents were parsed.\n",
      "All documents produced non-null parsing results.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Any documents that failed to parse\n",
    "is_error, msg, failed_docs_df = check_parsed_df_for_errors(parsed_files_df)\n",
    "if is_error:\n",
    "    display(failed_docs_df.toPandas())\n",
    "    raise Exception(msg)\n",
    "    \n",
    "# Any documents that returned empty parsing results\n",
    "is_error, msg, empty_docs_df = check_parsed_df_for_empty_parsed_files(parsed_files_df)\n",
    "if is_error:\n",
    "    display(empty_docs_df.toPandas())\n",
    "    raise Exception(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e21c84e8-7682-4a7a-86fc-7f4f990bb490",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Pipeline step 2: Compute chunks of documents\n",
    "\n",
    "In this step, we will split our documents into smaller chunks so they can be indexed in our vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eecd460c-f287-47ce-98f1-cea78a1f3f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "##### âœ…âœï¸ Chunking logic.\n",
    "\n",
    "We provide a default implementation of a recursive text splitter.  To create your own chunking logic, adapt the `get_recursive_character_text_splitter()` function inside `cookbook.data_pipeline.recursive_character_text_splitter.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02c40228-f933-4af8-9121-ed2efa0985dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk size in tokens: 1024 and chunk overlap in tokens: 256 are valid.  Using 16.0% (1280 tokens) of the 8192 token context window.\n"
     ]
    }
   ],
   "source": [
    "from cookbook.data_pipeline.recursive_character_text_splitter import (\n",
    "    get_recursive_character_text_splitter,\n",
    ")\n",
    "\n",
    "# Get the chunking function\n",
    "recursive_character_text_splitter_fn = get_recursive_character_text_splitter(\n",
    "    model_serving_endpoint=chunking_config.embedding_model_endpoint,\n",
    "    chunk_size_tokens=chunking_config.chunk_size_tokens,\n",
    "    chunk_overlap_tokens=chunking_config.chunk_overlap_tokens,\n",
    ")\n",
    "\n",
    "# Determine which columns to propagate from the docs table to the chunks table.\n",
    "\n",
    "# Get the columns from the parser except for the content\n",
    "# You can modify this to adjust which fields are propagated from the docs table to the chunks table.\n",
    "propagate_columns = [\n",
    "    field.name\n",
    "    for field in typed_dicts_to_spark_schema(ParserReturnValue).fields\n",
    "    if field.name != \"content\"\n",
    "]\n",
    "\n",
    "# If you want to implement retrieval strategies such as presenting the entire document vs. the chunk to the LLM, include `contentich contains the doc's full parsed text.  By default this is not included because the size of contcontentquite large and cause performance issues.\n",
    "# propagate_columns = [\n",
    "#     field.name\n",
    "#     for field in typed_dicts_to_spark_schema(ParserReturnValue).fields\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b17add2c-e7f0-4903-8ae9-40ca0633a8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ðŸš«âœï¸ Run the chunking function within Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dfa90f8-c4dc-4485-8fa8-dcd4c7d40618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying chunking UDF to 553 documents using Spark - this may take a long time if you have many documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731304355.901157 5761315 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 581 chunks.  Inspect `chunked_docs_df` or visit https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs_chunked__v2 to see the results.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>content_chunked</th>\n",
       "      <th>parser_status</th>\n",
       "      <th>doc_uri</th>\n",
       "      <th>last_modified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14e651e54ddd8aea7892970f7d61e7a0</td>\n",
       "      <td>&lt;product_category&gt;Services&lt;/product_category&gt;\\...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/0...</td>\n",
       "      <td>2024-11-06 18:15:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>383294ca41637916b8910df37df91cce</td>\n",
       "      <td>&lt;product_category&gt;Networking&lt;/product_category...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/9...</td>\n",
       "      <td>2024-11-06 18:16:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30d7b9352841ec48fcd45af358c5f5b6</td>\n",
       "      <td>&lt;product_category&gt;Clothing&lt;/product_category&gt;\\...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/5...</td>\n",
       "      <td>2024-11-06 18:15:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7fe980b1cc3eaa71f6d536fe848b4134</td>\n",
       "      <td>---\\n\\n#### FAQs\\n- **Can I return the gown?**...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/5...</td>\n",
       "      <td>2024-11-06 18:15:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8461e17f09ea17ddcf36f0b747bfcafc</td>\n",
       "      <td>&lt;product_category&gt;Software&lt;/product_category&gt;\\...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/9...</td>\n",
       "      <td>2024-11-06 18:15:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>e1202d9714e06b561efbe746780c0a4e</td>\n",
       "      <td>&lt;product_category&gt;Streaming Services&lt;/product_...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/c...</td>\n",
       "      <td>2024-11-06 18:15:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>d4382942873e98ab4b7d9583ba6d24be</td>\n",
       "      <td>&lt;product_category&gt;Fitness Equipment&lt;/product_c...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/4...</td>\n",
       "      <td>2024-11-06 18:16:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>3e230c0a00469cdb54bcdef70c73e17b</td>\n",
       "      <td>&lt;product_category&gt;Kitchen Appliances&lt;/product_...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/3...</td>\n",
       "      <td>2024-11-06 18:16:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>e6290a918682429b429e84bec9ffab84</td>\n",
       "      <td>&lt;product_category&gt;Wearable Technology&lt;/product...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/7...</td>\n",
       "      <td>2024-11-06 18:15:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>750443b87995d5e6843d315624eb9b46</td>\n",
       "      <td>&lt;product_category&gt;Electronics&lt;/product_categor...</td>\n",
       "      <td>SUCCESS</td>\n",
       "      <td>/Volumes/ep/cookbook_local_test/product_docs/8...</td>\n",
       "      <td>2024-11-06 18:15:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>581 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             chunk_id  \\\n",
       "0    14e651e54ddd8aea7892970f7d61e7a0   \n",
       "1    383294ca41637916b8910df37df91cce   \n",
       "2    30d7b9352841ec48fcd45af358c5f5b6   \n",
       "3    7fe980b1cc3eaa71f6d536fe848b4134   \n",
       "4    8461e17f09ea17ddcf36f0b747bfcafc   \n",
       "..                                ...   \n",
       "576  e1202d9714e06b561efbe746780c0a4e   \n",
       "577  d4382942873e98ab4b7d9583ba6d24be   \n",
       "578  3e230c0a00469cdb54bcdef70c73e17b   \n",
       "579  e6290a918682429b429e84bec9ffab84   \n",
       "580  750443b87995d5e6843d315624eb9b46   \n",
       "\n",
       "                                       content_chunked parser_status  \\\n",
       "0    <product_category>Services</product_category>\\...       SUCCESS   \n",
       "1    <product_category>Networking</product_category...       SUCCESS   \n",
       "2    <product_category>Clothing</product_category>\\...       SUCCESS   \n",
       "3    ---\\n\\n#### FAQs\\n- **Can I return the gown?**...       SUCCESS   \n",
       "4    <product_category>Software</product_category>\\...       SUCCESS   \n",
       "..                                                 ...           ...   \n",
       "576  <product_category>Streaming Services</product_...       SUCCESS   \n",
       "577  <product_category>Fitness Equipment</product_c...       SUCCESS   \n",
       "578  <product_category>Kitchen Appliances</product_...       SUCCESS   \n",
       "579  <product_category>Wearable Technology</product...       SUCCESS   \n",
       "580  <product_category>Electronics</product_categor...       SUCCESS   \n",
       "\n",
       "                                               doc_uri       last_modified  \n",
       "0    /Volumes/ep/cookbook_local_test/product_docs/0... 2024-11-06 18:15:40  \n",
       "1    /Volumes/ep/cookbook_local_test/product_docs/9... 2024-11-06 18:16:05  \n",
       "2    /Volumes/ep/cookbook_local_test/product_docs/5... 2024-11-06 18:15:51  \n",
       "3    /Volumes/ep/cookbook_local_test/product_docs/5... 2024-11-06 18:15:51  \n",
       "4    /Volumes/ep/cookbook_local_test/product_docs/9... 2024-11-06 18:15:55  \n",
       "..                                                 ...                 ...  \n",
       "576  /Volumes/ep/cookbook_local_test/product_docs/c... 2024-11-06 18:15:50  \n",
       "577  /Volumes/ep/cookbook_local_test/product_docs/4... 2024-11-06 18:16:01  \n",
       "578  /Volumes/ep/cookbook_local_test/product_docs/3... 2024-11-06 18:16:05  \n",
       "579  /Volumes/ep/cookbook_local_test/product_docs/7... 2024-11-06 18:15:47  \n",
       "580  /Volumes/ep/cookbook_local_test/product_docs/8... 2024-11-06 18:15:36  \n",
       "\n",
       "[581 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cookbook.data_pipeline.chunk_docs import apply_chunking_fn\n",
    "from cookbook.databricks_utils import get_table_url\n",
    "\n",
    "# Tune this parameter to optimize performance.  More partitions will improve performance, but may cause out of memory errors if your cluster is too small.\n",
    "NUM_PARTITIONS = 50\n",
    "\n",
    "# Load parsed docs\n",
    "parsed_files_df = spark.table(output_config.parsed_docs_table).repartition(NUM_PARTITIONS)\n",
    "\n",
    "chunked_docs_df = chunked_docs_table = apply_chunking_fn(\n",
    "    # The source documents table.\n",
    "    parsed_docs_df=parsed_files_df,\n",
    "    # The chunking function that takes a string (document) and returns a list of strings (chunks).\n",
    "    chunking_fn=recursive_character_text_splitter_fn,\n",
    "    # Choose which columns to propagate from the docs table to chunks table. `doc_uri` column is required we can propagate the original document URL to the Agent's web app.\n",
    "    propagate_columns=propagate_columns,\n",
    ")\n",
    "\n",
    "# Write to Delta Table\n",
    "chunked_docs_df.write.mode(\"overwrite\").option(\n",
    "    \"overwriteSchema\", \"true\"\n",
    ").saveAsTable(output_config.chunked_docs_table)\n",
    "\n",
    "# Get resulting table\n",
    "chunked_docs_df = spark.table(output_config.chunked_docs_table)\n",
    "\n",
    "# Show number of chunks created\n",
    "print(f\"Created {chunked_docs_df.count()} chunks.  Inspect `chunked_docs_df` or visit {get_table_url(output_config.chunked_docs_table)} to see the results.\")\n",
    "\n",
    "# enable CDC feed for VS index sync\n",
    "cdc_results = spark.sql(f\"ALTER TABLE {output_config.chunked_docs_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "# Show chunks\n",
    "display(chunked_docs_df.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fe923a8-89c2-4852-9cea-98074b3ce404",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸš«âœï¸ Pipeline step 3: Create the vector index\n",
    "\n",
    "In this step, we'll embed the documents to compute the vector index over the chunks and create our retriever index that will be used to query relevant documents to the user question.  The embedding pipeline is handled within Databricks Vector Search using [Delta Sync](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d53faa42-2a65-40b0-8fc1-6c27e88df6d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector search index \"ep.cookbook_local_test.abc_123_docs_chunked_index__v2\" on endpoint \"ericpeter_vector_search\"\n",
      "Computing document embeddings and Vector Search Index. This can take 15 minutes or much longer if you have a larger number of documents.\n",
      "Successfully created vector search index ep.cookbook_local_test.abc_123_docs_chunked_index__v2.\n",
      "NOTE: This cell will complete before the vector index has finished syncing/embedding your chunks & is ready for queries!\n",
      "View sync status here: https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs_chunked_index__v2\n"
     ]
    }
   ],
   "source": [
    "from cookbook.data_pipeline.build_retriever_index import build_retriever_index\n",
    "from cookbook.databricks_utils import get_table_url\n",
    "\n",
    "is_error, msg = retriever_index_result = build_retriever_index(\n",
    "    # Spark requires `` to escape names with special chars, VS client does not.\n",
    "    chunked_docs_table_name=output_config.chunked_docs_table.replace(\"`\", \"\"),\n",
    "    vector_search_endpoint=output_config.vector_search_endpoint,\n",
    "    vector_search_index_name=output_config.vector_index,\n",
    "\n",
    "    # Must match the embedding endpoint you used to chunk your documents\n",
    "    embedding_endpoint_name=chunking_config.embedding_model_endpoint,\n",
    "\n",
    "    # Set to true to re-create the vector search endpoint when re-running the data pipeline.  If set to True, syncing will not work if re-run the pipeline and change the schema of chunked_docs_table_name.  Keeping this as False will allow Vector Search to avoid recomputing embeddings for any row with that has a chunk_id that was previously computed.\n",
    "    force_delete_index_before_create=False,\n",
    ")\n",
    "if is_error:\n",
    "    raise Exception(msg)\n",
    "else:\n",
    "    print(\"NOTE: This cell will complete before the vector index has finished syncing/embedding your chunks & is ready for queries!\")\n",
    "    print(f\"View sync status here: {get_table_url(output_config.vector_index)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1ad14b-2573-4485-8369-d417f7a548f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### ðŸš«âœï¸ Print links to view the resulting tables/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd40431-4cd3-4cc9-b38d-5ab817c40043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed docs table: https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs__v2\n",
      "\n",
      "Chunked docs table: https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs_chunked__v2\n",
      "\n",
      "Vector search index: https://e2-dogfood.staging.cloud.databricks.com/explore/data/ep/cookbook_local_test/abc_123_docs_chunked_index__v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from cookbook.databricks_utils import get_table_url\n",
    "\n",
    "print()\n",
    "print(f\"Parsed docs table: {get_table_url(output_config.parsed_docs_table)}\\n\")\n",
    "print(f\"Chunked docs table: {get_table_url(output_config.chunked_docs_table)}\\n\")\n",
    "print(f\"Vector search index: {get_table_url(output_config.vector_index)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_data_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai-cookbook-T2SdtsNM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
