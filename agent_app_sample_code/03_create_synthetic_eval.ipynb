{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ‘‰ START HERE: How to use this notebook\n",
    "\n",
    "### Step 1: Create synthetic evaluation data\n",
    "\n",
    "To measure your Agent's quality, you need a diverse, representative evaluation set.  This notebook turns your unstructured documents into a high-quality synthetic evaluation set so that you can start to evaluate and improve your Agent's quality before subject matter experts are available to label data.\n",
    "\n",
    "This notebook does the following:\n",
    "1. Loads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** Throughout this notebook, we indicate which cells you:\n",
    "- âœ…âœï¸ *should* customize - these cells contain config settings to change\n",
    "- ðŸš«âœï¸ *typically will not* customize - these cells contain  code that is parameterized by your configuration.\n",
    "\n",
    "*Cells that don't require customization still need to be run!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš«âœï¸ Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbcdef70-657e-4f12-b564-90d0f5b74e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -qqqq -U -r requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš«âœï¸ Connect to Databricks\n",
    "\n",
    "If running locally in an IDE using Databricks Connect, connect the Spark client & configure MLflow to use Databricks Managed MLflow.  If this running in a Databricks Notebook, these values are already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "if not du.is_in_databricks_notebook():\n",
    "    from databricks.connect import DatabricksSession\n",
    "    import os\n",
    "\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = \"databricks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸš«âœï¸ Load the Agent's storage locations\n",
    "\n",
    "This notebook writes to the evaluation set table that you specified in the [Agent setup](02_agent_setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation set `main.eric_peter_agents.my_agent_eval_set` does not exist.  This notebook will create a new Delta Table at this location.\n"
     ]
    }
   ],
   "source": [
    "from utils.cookbook.agent_storage_config import AgentStorageConfig\n",
    "from utils.cookbook.databricks_utils import get_table_url\n",
    "\n",
    "# Load the Agent's storage configuration\n",
    "agent_storage_config = AgentStorageConfig.from_yaml_file('./configs/agent_storage_config.yaml')\n",
    "\n",
    "# Check if the evaluation set already exists\n",
    "try:\n",
    "    eval_dataset = spark.table(agent_storage_config.evaluation_set_uc_table)\n",
    "    if eval_dataset.count() > 0:\n",
    "        print(f\"Evaluation set {get_table_url(agent_storage_config.evaluation_set_uc_table)} already exists!  By default, this notebook will append to the evaluation dataset.  If you would like to overwrite the existing evaluation set, please delete the table before running this notebook.\")\n",
    "    else:\n",
    "        print(f\"Evaluation set {get_table_url(agent_storage_config.evaluation_set_uc_table)} exists, but is empty!  By default, this notebook will NOT change the schema of this table - if you experience schema related errors, drop this table before running this notebook so it can be recreated with the correct schema.\")\n",
    "except Exception:\n",
    "    print(f\"Evaluation set `{agent_storage_config.evaluation_set_uc_table}` does not exist.  This notebook will create a new Delta Table at this location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœ…âœï¸ Load the source documents for synthetic evaluation data generation\n",
    "\n",
    "Most often, this will be the same as the document output table from the [data pipeline](01_data_pipeline.ipynb).\n",
    "\n",
    "Here, we provide code to load the documents table that was created in the [data pipeline](01_data_pipeline.ipynb).\n",
    "\n",
    "Alternatively, this can be a Spark DataFrame, Pandas DataFrame, or list of dictionaries with the following keys/columns:\n",
    "- `doc_uri`: A URI pointing to the document.\n",
    "- `content`: The content of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+-------------------+\n",
      "|             content|parser_status|             doc_uri|      last_modified|\n",
      "+--------------------+-------------+--------------------+-------------------+\n",
      "|**1. Create a UC ...|      SUCCESS|/Volumes/ep/cookb...|2024-11-04 01:32:30|\n",
      "+--------------------+-------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils.data_pipeline.data_pipeline_config import DataPipelineConfig\n",
    "from pyspark.errors import PySparkAttributeError\n",
    "\n",
    "datapipeline_config = DataPipelineConfig.from_yaml_file('./configs/data_pipeline_config.yaml')\n",
    "\n",
    "source_documents = spark.table(datapipeline_config.output.parsed_docs_table)\n",
    "\n",
    "try: # if in DBX notebook, render the dataframe, otherwise print it\n",
    "    source_documents.display()\n",
    "except PySparkAttributeError as e:\n",
    "    source_documents.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### âœ…âœï¸ Run the synthetic evaluation data generation\n",
    "\n",
    "Optionally, you can customize the guidelines to guide the synthetic data generation.  By default, guidelines are not applied - to apply the guidelines, uncomment `guidelines=guidelines` in the `generate_evals_df(...)` call.  See our [documentation](https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cb950a-84b1-4e1d-a7fb-5179a0aa69de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating evaluations:   0%|          | 0/1 documents processed [Elapsed: 00:00, Remaining: ?]I0000 00:00:1730763702.841937 1622882 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "WARNING:databricks.rag_eval.datasets.synthetic_evals_generation:Failed to generate questions for doc /Volumes/ep/cookbook_local_test/source_docs/Updating RAG Studio to Ingest PDFs.pdf: 401 Client Error: Unauthorized. Credential was not sent or was of an unsupported type for this API.\n",
      "Generating evaluations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 documents processed [Elapsed: 00:00, Remaining: 00:00]\n"
     ]
    },
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 31\u001b[0m\n\u001b[1;32m     22\u001b[0m synthesized_evals_df \u001b[38;5;241m=\u001b[39m generate_evals_df(\n\u001b[1;32m     23\u001b[0m     docs\u001b[38;5;241m=\u001b[39msource_documents,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# The number of evaluations to generate for each doc.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# guidelines=guidelines\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Write the synthetic evaluation data to the evaluation set table\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynthesized_evals_df\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(agent_storage_config\u001b[38;5;241m.\u001b[39mevaluation_set_uc_table)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Display the synthetic evaluation data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m eval_set_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(agent_storage_config\u001b[38;5;241m.\u001b[39mevaluation_set_uc_table)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/genai-cookbook-T2SdtsNM-py3.11/lib/python3.11/site-packages/pyspark/sql/connect/session.py:501\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(LocalRelation(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, schema\u001b[38;5;241m=\u001b[39m_schema\u001b[38;5;241m.\u001b[39mjson()), \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    502\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_EMPTY_SCHEMA\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    503\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    504\u001b[0m         )\n\u001b[1;32m    506\u001b[0m _table: Optional[pa\u001b[38;5;241m.\u001b[39mTable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;66;03m# Logic was borrowed from `_create_from_pandas_with_arrow` in\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;66;03m# `pyspark.sql.pandas.conversion.py`. Should ideally deduplicate the logics.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \n\u001b[1;32m    512\u001b[0m     \u001b[38;5;66;03m# If no schema supplied by user then get the names of columns only\u001b[39;00m\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_INFER_EMPTY_SCHEMA] Can not infer schema from an empty dataset."
     ]
    }
   ],
   "source": [
    "from databricks.agents.eval import generate_evals_df\n",
    "\n",
    "# NOTE: The guidelines you provide are a free-form string. The markdown string below is the suggested formatting for the set of guidelines, however you are free\n",
    "# to add your sections here. Note that this will be prompt-engineering an LLM that generates the synthetic data, so you may have to iterate on these guidelines before\n",
    "# you get the results you desire.\n",
    "guidelines = \"\"\"\n",
    "# Task Description\n",
    "The Agent is a RAG chatbot that answers questions about using Spark on Databricks. The Agent has access to a corpus of Databricks documents, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response. The corpus covers a lot of info, but the Agent is specifically designed to interact with Databricks users who have questions about Spark. So questions outside of this scope are considered irrelevant.\n",
    "\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "synthesized_evals_df = generate_evals_df(\n",
    "    docs=source_documents,\n",
    "    # The number of evaluations to generate for each doc.\n",
    "    num_questions_per_doc=2,\n",
    "    # A optional set of guidelines that help guide the synthetic generation. This is a free-form string that will be used to prompt the generation.\n",
    "    # guidelines=guidelines\n",
    ")\n",
    "\n",
    "# Write the synthetic evaluation data to the evaluation set table\n",
    "spark.createDataFrame(synthesized_evals_df).write.format(\"delta\").mode(\"append\").saveAsTable(agent_storage_config.evaluation_set_uc_table)\n",
    "\n",
    "# Display the synthetic evaluation data\n",
    "eval_set_df = spark.table(agent_storage_config.evaluation_set_uc_table).show()\n",
    "try:\n",
    "    eval_set_df.display()\n",
    "except PySparkAttributeError as e:\n",
    "    eval_set_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_create_synthetic_eval",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai-cookbook-T2SdtsNM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
