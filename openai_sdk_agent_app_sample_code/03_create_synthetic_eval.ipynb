{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üëâ START HERE: How to use this notebook\n",
    "\n",
    "### Step 1: Create synthetic evaluation data\n",
    "\n",
    "To measure your Agent's quality, you need a diverse, representative evaluation set.  This notebook turns your unstructured documents into a high-quality synthetic evaluation set so that you can start to evaluate and improve your Agent's quality before subject matter experts are available to label data.\n",
    "\n",
    "This notebook does the following:\n",
    "1. <TODO>\n",
    "\n",
    "THIS DOES NOT WORK FROM LOCAL IDE YET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** Throughout this notebook, we indicate which cells you:\n",
    "- ‚úÖ‚úèÔ∏è *should* customize - these cells contain config settings to change\n",
    "- üö´‚úèÔ∏è *typically will not* customize - these cells contain  code that is parameterized by your configuration.\n",
    "\n",
    "*Cells that don't require customization still need to be run!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Install Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbcdef70-657e-4f12-b564-90d0f5b74e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -qqqq -U -r requirements.txt\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Connect to Databricks\n",
    "\n",
    "If running locally in an IDE using Databricks Connect, connect the Spark client & configure MLflow to use Databricks Managed MLflow.  If this running in a Databricks Notebook, these values are already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "if not du.is_in_databricks_notebook():\n",
    "    from databricks.connect import DatabricksSession\n",
    "    import os\n",
    "\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = \"databricks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Load the Agent's storage locations\n",
    "\n",
    "This notebook writes to the evaluation set table that you specified in the [Agent setup](02_agent_setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.config.shared.agent_storage_location import AgentStorageConfig\n",
    "from cookbook.databricks_utils import get_table_url\n",
    "from cookbook.config import load_serializable_config_from_yaml_file\n",
    "\n",
    "# Load the Agent's storage configuration\n",
    "agent_storage_config: AgentStorageConfig = load_serializable_config_from_yaml_file('./configs/agent_storage_config.yaml')\n",
    "\n",
    "# Check if the evaluation set already exists\n",
    "try:\n",
    "    eval_dataset = spark.table(agent_storage_config.evaluation_set_uc_table)\n",
    "    if eval_dataset.count() > 0:\n",
    "        print(f\"Evaluation set {get_table_url(agent_storage_config.evaluation_set_uc_table)} already exists!  By default, this notebook will append to the evaluation dataset.  If you would like to overwrite the existing evaluation set, please delete the table before running this notebook.\")\n",
    "    else:\n",
    "        print(f\"Evaluation set {get_table_url(agent_storage_config.evaluation_set_uc_table)} exists, but is empty!  By default, this notebook will NOT change the schema of this table - if you experience schema related errors, drop this table before running this notebook so it can be recreated with the correct schema.\")\n",
    "except Exception:\n",
    "    print(f\"Evaluation set `{agent_storage_config.evaluation_set_uc_table}` does not exist.  This notebook will create a new Delta Table at this location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ‚úèÔ∏è Load the source documents for synthetic evaluation data generation\n",
    "\n",
    "Most often, this will be the same as the document output table from the [data pipeline](01_data_pipeline.ipynb).\n",
    "\n",
    "Here, we provide code to load the documents table that was created in the [data pipeline](01_data_pipeline.ipynb).\n",
    "\n",
    "Alternatively, this can be a Spark DataFrame, Pandas DataFrame, or list of dictionaries with the following keys/columns:\n",
    "- `doc_uri`: A URI pointing to the document.\n",
    "- `content`: The content of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.config.data_pipeline import DataPipelineConfig\n",
    "from cookbook.config import load_serializable_config_from_yaml_file\n",
    "\n",
    "datapipeline_config: DataPipelineConfig= load_serializable_config_from_yaml_file('./configs/data_pipeline_config.yaml')\n",
    "\n",
    "source_documents = spark.table(datapipeline_config.output.parsed_docs_table)\n",
    "\n",
    "display(source_documents.toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ‚úèÔ∏è Run the synthetic evaluation data generation\n",
    "\n",
    "Optionally, you can customize the guidelines to guide the synthetic data generation.  By default, guidelines are not applied - to apply the guidelines, uncomment `guidelines=guidelines` in the `generate_evals_df(...)` call.  See our [documentation](https://docs.databricks.com/en/generative-ai/agent-evaluation/synthesize-evaluation-set.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7cb950a-84b1-4e1d-a7fb-5179a0aa69de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.agents.eval import generate_evals_df\n",
    "\n",
    "# NOTE: The guidelines you provide are a free-form string. The markdown string below is the suggested formatting for the set of guidelines, however you are free\n",
    "# to add your sections here. Note that this will be prompt-engineering an LLM that generates the synthetic data, so you may have to iterate on these guidelines before\n",
    "# you get the results you desire.\n",
    "guidelines = \"\"\"\n",
    "# Task Description\n",
    "The Agent is a RAG chatbot that answers questions about using Spark on Databricks. The Agent has access to a corpus of Databricks documents, and its task is to answer the user's questions by retrieving the relevant docs from the corpus and synthesizing a helpful, accurate response. The corpus covers a lot of info, but the Agent is specifically designed to interact with Databricks users who have questions about Spark. So questions outside of this scope are considered irrelevant.\n",
    "\n",
    "# User personas\n",
    "- A developer who is new to the Databricks platform\n",
    "- An experienced, highly technical Data Scientist or Data Engineer\n",
    "\n",
    "# Example questions\n",
    "- what API lets me parallelize operations over rows of a delta table?\n",
    "- Which cluster settings will give me the best performance when using Spark?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\"\n",
    "\n",
    "synthesized_evals_df = generate_evals_df(\n",
    "    docs=source_documents,\n",
    "    # The number of evaluations to generate for each doc.\n",
    "    num_questions_per_doc=2,\n",
    "    # A optional set of guidelines that help guide the synthetic generation. This is a free-form string that will be used to prompt the generation.\n",
    "    # guidelines=guidelines\n",
    ")\n",
    "\n",
    "# Write the synthetic evaluation data to the evaluation set table\n",
    "spark.createDataFrame(synthesized_evals_df).write.format(\"delta\").mode(\"append\").saveAsTable(agent_storage_config.evaluation_set_uc_table)\n",
    "\n",
    "# Display the synthetic evaluation data\n",
    "eval_set_df = spark.table(agent_storage_config.evaluation_set_uc_table).show()\n",
    "display(eval_set_df.toPandas())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_create_synthetic_eval",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai-cookbook-T2SdtsNM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
