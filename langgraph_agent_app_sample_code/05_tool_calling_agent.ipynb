{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31661828-f9bb-4fc2-a1bd-94424a27ed52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üëâ START HERE: How to use this notebook\n",
    "\n",
    "# Step 3: Build, evaluate, & deploy your Agent\n",
    "\n",
    "Use this notebook to iterate on the code and configuration of your Agent.\n",
    "\n",
    "By the end of this notebook, you will have 1+ registered versions of your Agent, each coupled with a detailed quality evaluation.\n",
    "\n",
    "Optionally, you can deploy a version of your Agent that you can interact with in the [Mosiac AI Playground](https://docs.databricks.com/en/large-language-models/ai-playground.html) and let your business stakeholders who don't have Databricks accounts interact with it & provide feedback in the [Review App](https://docs.databricks.com/en/generative-ai/agent-evaluation/human-evaluation.html#review-app-ui).\n",
    "\n",
    "\n",
    "For each version of your agent, you will have an MLflow run inside your MLflow experiment that contains:\n",
    "- Your Agent's code & config\n",
    "- Evaluation metrics for cost, quality, and latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9f685a-fdb7-49a4-9e3a-a4a9e964d045",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Important note:** Throughout this notebook, we indicate which cell's code you:\n",
    "- ‚úÖ‚úèÔ∏è should customize - these cells contain code & config with business logic that you should edit to meet your requirements & tune quality.\n",
    "- üö´‚úèÔ∏è should not customize - these cells contain boilerplate code required to load/save/execute your Agent\n",
    "\n",
    "*Cells that don't require customization still need to be run!  You CAN change these cells, but if this is the first time using this notebook, we suggest not doing so.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb4f8cc0-1797-4beb-a9f2-df21a9db79f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üö´‚úèÔ∏è Install Python libraries\n",
    "\n",
    "You do not need to modify this cell unless you need additional Python packages in your Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d4030e8-ae97-4351-bebd-9651d283578f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install -qqqq -U -r requirements.txt\n",
    "# # Restart to load the packages into the Python environment\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Connect to Databricks\n",
    "\n",
    "If running locally in an IDE using Databricks Connect, connect the Spark client & configure MLflow to use Databricks Managed MLflow.  If this running in a Databricks Notebook, these values are already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.utils import databricks_utils as du\n",
    "\n",
    "if not du.is_in_databricks_notebook():\n",
    "    from databricks.connect import DatabricksSession\n",
    "    import os\n",
    "\n",
    "    spark = DatabricksSession.builder.getOrCreate()\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = \"databricks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Load the Agent's UC storage locations; set up MLflow experiment\n",
    "\n",
    "This notebook uses the UC model, MLflow Experiment, and Evaluation Set that you specified in the [Agent setup](02_agent_setup.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.config.shared.agent_storage_location import AgentStorageConfig\n",
    "from cookbook.databricks_utils import get_mlflow_experiment_url\n",
    "from cookbook.config import load_serializable_config_from_yaml_file\n",
    "import mlflow \n",
    "\n",
    "# Load the Agent's storage locations\n",
    "agent_storage_config: AgentStorageConfig= load_serializable_config_from_yaml_file(\"./configs/agent_storage_config.yaml\")\n",
    "\n",
    "# Show the Agent's storage locations\n",
    "agent_storage_config.pretty_print()\n",
    "\n",
    "# set the MLflow experiment\n",
    "experiment_info = mlflow.set_experiment(agent_storage_config.mlflow_experiment_name)\n",
    "# If running in a local IDE, set the MLflow experiment name as an environment variable\n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = agent_storage_config.mlflow_experiment_name\n",
    "\n",
    "print(f\"View the MLflow Experiment `{agent_storage_config.mlflow_experiment_name}` at {get_mlflow_experiment_url(experiment_info.experiment_id)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö´‚úèÔ∏è Helper method to log the Agent's code & config to MLflow\n",
    "\n",
    "Before we start, let's define a helper method to log the Agent's code & config to MLflow.  We will use this to log the agent's code & config to MLflow & the Unity Catalog.  It is used in evaluation & for deploying to Agent Evaluation's [Review App](https://docs.databricks.com/en/generative-ai/agent-evaluation/human-evaluation.html#review-app-ui) (a chat UI for your stakeholders to test this agent) and later, deplying the Agent to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "from mlflow.types.llm import CHAT_MODEL_INPUT_SCHEMA\n",
    "from mlflow.models.rag_signatures import StringResponse\n",
    "from cookbook.agents.utils.signatures import STRING_RESPONSE_WITH_MESSAGES\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from cookbook.agents.function_calling_agent import FunctionCallingAgent\n",
    "from cookbook.agents.function_calling_agent import FunctionCallingAgentConfig\n",
    "\n",
    "# This helper will log the Agent's code & config to an MLflow run and return the logged model's URI\n",
    "# If run from inside a mlfow.start_run() block, it will log to that run, otherwise it will log to a new run.\n",
    "# This logged Agent is ready for deployment, so if you are happy with your evaluation, it is ready to deploy!\n",
    "def log_function_calling_agent_to_mlflow(agent_config: FunctionCallingAgentConfig):\n",
    "    # Get the agent's code path from the imported Agent class\n",
    "    agent_code_path = f\"{os.getcwd()}/{FunctionCallingAgent.__module__.replace('.', '/')}.py\"\n",
    "\n",
    "    # Get the pip requirements from the requirements.txt file\n",
    "    with open(\"requirements.txt\", \"r\") as file:\n",
    "        pip_requirements = [line.strip() for line in file.readlines()] + [\"pyspark\"] # manually add pyspark\n",
    "\n",
    "    logged_agent_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"agent\",\n",
    "            python_model=agent_code_path,\n",
    "            input_example=agent_config.input_example,\n",
    "            model_config=agent_config.model_dump(),\n",
    "            resources=agent_config.get_resource_dependencies(), # This allows the agents.deploy() command to securely provision credentials for the Agent's databricks resources e.g., vector index, model serving endpoints, etc\n",
    "            signature=ModelSignature(\n",
    "            inputs=CHAT_MODEL_INPUT_SCHEMA,\n",
    "            # outputs=STRING_RESPONSE_WITH_MESSAGES #TODO: replace with MLflow signature\n",
    "            outputs=StringResponse()\n",
    "        ),\n",
    "        code_paths=[os.path.join(os.getcwd(), \"cookbook\")],\n",
    "        pip_requirements=pip_requirements,\n",
    "    )\n",
    "\n",
    "    return logged_agent_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9933d05f-29fa-452e-abdc-2a02328fbe22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1Ô∏è‚É£ Iterate on the Agent's code & config to improve quality\n",
    "\n",
    "The below cells are used to execute your inner dev loop to improve the Agent's quality.\n",
    "\n",
    "We suggest the following process:\n",
    "1. Vibe check the Agent for 5 - 10 queries to verify it works\n",
    "2. Make any necessary changes to the code/config\n",
    "3. Use Agent Evaluation to evaluate the Agent using your evaluation set, which will provide a quality assessment & identify the root causes of any quality issues\n",
    "4. Based on that evaluation, make & test changes to the code/config to improve quality\n",
    "5. üîÅ Repeat steps 3 and 4 until you are satisified with the Agent's quality\n",
    "6. Deploy the Agent to Agent Evaluation's [Review App](https://docs.databricks.com/en/generative-ai/agent-evaluation/human-evaluation.html#review-app-ui) for pre-production testing\n",
    "7. Use the following notebooks to review that feedback (optionally adding new records to your evaluation set) & identify any further quality issues\n",
    "8. üîÅ Repeat steps 3 and 4 to fix any issues identified in step 7\n",
    "9. Deploy the Agent to a production-ready REST API endpoint (using the same cells in this notebook as step 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cookbook Agent configurations, which are Pydantic models\n",
    "from cookbook.config import serializable_config_to_yaml_file\n",
    "from cookbook.config.agents.function_calling_agent import (\n",
    "    FunctionCallingAgentConfig,\n",
    ")\n",
    "from cookbook.config.data_pipeline import (\n",
    "    DataPipelineConfig,\n",
    ")\n",
    "from cookbook.config.shared.llm import LLMConfig, LLMParametersConfig\n",
    "from cookbook.config import load_serializable_config_from_yaml_file\n",
    "from cookbook.tools.vector_search import (\n",
    "    VectorSearchRetrieverTool,\n",
    "    VectorSearchSchema,\n",
    ")\n",
    "import json\n",
    "from cookbook.tools.uc_tool import UCTool\n",
    "\n",
    "\n",
    "########################\n",
    "# #### üö´‚úèÔ∏è Load the Vector Index Unity Cataloglocation from the data pipeline configuration\n",
    "# Usage:\n",
    "# - If you used `01_data_pipeline` to create your Vector Index, run this cell.\n",
    "# - If your Vector Index was created elsewhere, comment out this logic and set the UC location in the Retriever config.\n",
    "########################\n",
    "\n",
    "data_pipeline_config: DataPipelineConfig = load_serializable_config_from_yaml_file(\n",
    "    \"./configs/data_pipeline_config.yaml\"\n",
    ")\n",
    "\n",
    "########################\n",
    "# #### ‚úÖ‚úèÔ∏è Retriever tool that connects to the Vector Search index\n",
    "########################\n",
    "\n",
    "retriever_tool = VectorSearchRetrieverTool(\n",
    "    name=\"search_product_docs\",\n",
    "    description=\"Use this tool to search for product documentation.\",\n",
    "    vector_search_index=\"ep.cookbook_local_test.product_docs_docs_chunked_index__v1\",\n",
    "    vector_search_schema=VectorSearchSchema(\n",
    "        # These columns are the default values used in the `01_data_pipeline` notebook\n",
    "        # If you used a different column names in that notebook OR you are using a pre-built vector index, update the column names here.\n",
    "        chunk_text=\"content_chunked\",  # Contains the text of each document chunk\n",
    "        document_uri=\"doc_uri\",  # The document URI of the chunk e.g., \"/Volumes/catalog/schema/volume/file.pdf\" - displayed as the document ID in the Review App\n",
    "        # additional_metadata_columns=[],  # Additional columns to return from the vector database and present to the LLM\n",
    "    ),\n",
    "    # Optional parameters, see VectorSearchRetrieverTool.__doc__ for details.  The default values are shown below.\n",
    "    # doc_similarity_threshold=0.0,\n",
    "    # vector_search_parameters=VectorSearchParameters(\n",
    "    #     num_results=5,\n",
    "    #     query_type=\"ann\"\n",
    "    # ),\n",
    "    # Adding columns here will allow the Agent's LLM to dynamically apply filters based on the user's query.\n",
    "    # filterable_columns=[]\n",
    ")\n",
    "\n",
    "########################\n",
    "# #### ‚úÖ‚úèÔ∏è Add Unity Catalog tools to the Agent\n",
    "########################\n",
    "\n",
    "translate_sku_tool = UCTool(uc_function_name=\"ep.cookbook_local_test.sku_sample_translator\")\n",
    "\n",
    "\n",
    "########################\n",
    "# #### ‚úÖ‚úèÔ∏è Add a local Python function as a tool in the Agent\n",
    "########################\n",
    "\n",
    "from cookbook.tools.local_function import LocalFunctionTool\n",
    "from tools.sample_tool import sku_sample_translator\n",
    "\n",
    "# translate_sku_tool = LocalFunctionTool(func=translate_sku, description=\"Translates a pre-2024 SKU formatted as 'OLD-XXX-YYYY' to the new SKU format 'NEW-YYYY-XXX'.\")\n",
    "\n",
    "########################\n",
    "#### ‚úÖ‚úèÔ∏è Agent's LLM configuration\n",
    "########################\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "## Role\n",
    "You are a helpful assistant that answers questions using a set of tools. If needed, you ask the user follow-up questions to clarify their request.\n",
    "\n",
    "## Objective\n",
    "Your goal is to provide accurate, relevant, and helpful response based solely on the outputs from these tools. You are concise and direct in your responses.\n",
    "\n",
    "## Instructions\n",
    "1. **Understand the Query**: Think step by step to analyze the user's question and determine the core need or problem. \n",
    "\n",
    "2. **Assess available tools**: Think step by step to consider each available tool and understand their capabilities in the context of the user's query.\n",
    "\n",
    "3. **Select the appropriate tool(s) OR ask follow up questions**: Based on your understanding of the query and the tool descriptions, decide which tool(s) should be used to generate a response. If you do not have enough information to use the available tools to answer the question, ask the user follow up questions to refine their request.  If you do not have a relevant tool for a question or the outputs of the tools are not helpful, respond with: \"I'm sorry, I can't help you with that.\"\n",
    "\"\"\".strip()\n",
    "\n",
    "fc_agent_config = FunctionCallingAgentConfig(\n",
    "    llm_config=LLMConfig(\n",
    "        llm_endpoint_name=\"ep-gpt4o-new\",  # Model serving endpoint w/ a Chat Completions API\n",
    "        llm_system_prompt_template=system_prompt,  # System prompt template\n",
    "        llm_parameters=LLMParametersConfig(\n",
    "            temperature=0.01, max_tokens=1500\n",
    "        ),  # LLM parameters\n",
    "    ),\n",
    "    # Add one or more tools that comply with the CookbookTool interface\n",
    "    tools=[retriever_tool, translate_sku_tool],\n",
    "    # tools=[retriever_tool],\n",
    ")\n",
    "\n",
    "# Print the configuration as a JSON string to see it all together\n",
    "# print(json.dumps(fc_agent_config.model_dump(), indent=4))\n",
    "\n",
    "########################\n",
    "##### Dump the configuration to a YAML\n",
    "# Optional step, this allows the Agent's code file to be run by itself (e.g., outside of this notebook) using the above configuration.\n",
    "########################\n",
    "# Import the default YAML config file name from the Agent's code file\n",
    "from cookbook.agents.function_calling_agent import FC_AGENT_DEFAULT_YAML_CONFIG_FILE_NAME\n",
    "\n",
    "# Dump the configuration to a YAML file\n",
    "serializable_config_to_yaml_file(fc_agent_config, \"./configs/\"+FC_AGENT_DEFAULT_YAML_CONFIG_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ‚úèÔ∏è Optionally, adjust the Agent's code\n",
    "\n",
    "Here, we import the Agent's code so we can run the Agent locally within the notebook.  To modify the code, open the Agent's code file in a separate window, enable reload, make your changes, and re-run this cell.\n",
    "\n",
    "**Typically, when building the first version of your agent, we suggest first trying to tune the configuration (prompts, etc) to improve quality.  If you need more control to fix quality issues, you can then modify the Agent's code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.agents.function_calling_agent import FunctionCallingAgent\n",
    "import inspect\n",
    "\n",
    "# Print the Agent code for inspection\n",
    "print(inspect.getsource(FunctionCallingAgent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ‚úèÔ∏è üÖ∞ Vibe check the Agent for a single query\n",
    "\n",
    "Running this cell will produce an MLflow Trace that you can use to see the Agent's outputs and understand the steps it took to produce that output.\n",
    "\n",
    "If you are running in a local IDE, browse to the MLflow Experiment page to view the Trace (link to the Experiment UI is at the top of this notebook).  If running in a Databricks Notebook, your trace will appear inline below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cookbook.databricks_utils import get_mlflow_experiment_traces_url\n",
    "from cookbook.agents.function_calling_agent import FunctionCallingAgent\n",
    "\n",
    "# Load the Agent's code with the above configuration\n",
    "agent = FunctionCallingAgent(agent_config=fc_agent_config)\n",
    "\n",
    "# Vibe check the Agent for a single query\n",
    "output = agent.predict(model_input={\"messages\": [{\"role\": \"user\", \"content\": \"How does the blender work?\"}]})\n",
    "# output = agent.predict(model_input={\"messages\": [{\"role\": \"user\", \"content\": \"Translate the sku `OLD-abs-1234` to the new format\"}]})\n",
    "\n",
    "print(f\"View the MLflow Traces at {get_mlflow_experiment_traces_url(experiment_info.experiment_id)}\")\n",
    "print(f\"Agent's final response:\\n----\\n{output['content']}\\n----\")\n",
    "print()\n",
    "# print(f\"Agent's full message history (useful for debugging):\\n----\\n{json.dumps(output['messages'], indent=2)}\\n----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test a multi-turn conversation with the Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_turn = {'messages': output['messages'] + [{\"role\": \"user\", \"content\": \"How do I turn it on?\"}]}\n",
    "\n",
    "# Run the Agent again with the same input to continue the conversation\n",
    "second_turn_output = agent.predict(model_input=second_turn)\n",
    "\n",
    "print(f\"View the MLflow Traces at {get_mlflow_experiment_traces_url(experiment_info.experiment_id)}\")\n",
    "print(f\"Agent's final response:\\n----\\n{second_turn_output['content']}\\n----\")\n",
    "print()\n",
    "print(f\"Agent's full message history (useful for debugging):\\n----\\n{json.dumps(second_turn_output['messages'], indent=2)}\\n----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚úÖ‚úèÔ∏è üÖ± Evaluate the Agent using your evaluation set\n",
    "\n",
    "Note: If you do not have an evaluation set, you can create a synthetic evaluation set by using the 03_synthetic_evaluation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_set = spark.table(agent_storage_config.evaluation_set_uc_table)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = log_function_calling_agent_to_mlflow(fc_agent_config)\n",
    "\n",
    "    # Run the agent for these queries, using Agent evaluation to parallelize the calls\n",
    "    eval_results = mlflow.evaluate(\n",
    "        model=logged_agent_info.model_uri,  # use the MLflow logged Agent\n",
    "        data=evaluation_set,  # Evaluate the Agent for every row of the evaluation set\n",
    "        model_type=\"databricks-agent\",  # use Agent Evaluation\n",
    "    )\n",
    "\n",
    "    # Show all outputs.  Click on a row in this table to display the MLflow Trace.\n",
    "    display(eval_results.tables[\"eval_results\"])\n",
    "\n",
    "    # Click 'View Evaluation Results' to see the Agent's inputs/outputs + quality evaluation displayed in a UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Deploy a version of your Agent - either to the Review App or Production\n",
    "\n",
    "Once you have a version of your Agent that has sufficient quality, you will register the Agent's model from the MLflow Experiment into the Unity Catalog & use Agent Framework's `agents.deploy(...)` command to deploy it.  Note these steps are the same for deploying to pre-production (e.g., the [Review App](https://docs.databricks.com/en/generative-ai/agent-evaluation/human-evaluation.html#review-app-ui) or production.\n",
    "\n",
    "By the end of this step, you will have deployed a version of your Agent that you can interact with and share with your business stakeholders for feedback, even if they don't have access to your Databricks workspace:\n",
    "\n",
    "1. A production-ready scalable REST API deployed as a Model Serving endpoint that logged every request/request/MLflow Trace to a Delta Table.\n",
    "    - REST API for querying the Agent\n",
    "    - REST API for sending user feedback from your UI to the Agent\n",
    "2. Agent Evaluation's [Review App](https://docs.databricks.com/en/generative-ai/agent-evaluation/human-evaluation.html#review-app-ui) connected to these endpoints.\n",
    "3. [Mosiac AI Playground](https://docs.databricks.com/en/large-language-models/ai-playground.html) connected to these endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Deploy the last agent you logged above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Use Unity Catalog as the model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Register the Agent's model to the Unity Catalog\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "    model_uri=logged_agent_info.model_uri, name=agent_storage_config.uc_model_name\n",
    ")\n",
    "\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "agents.deploy(agent_storage_config.uc_model_name, uc_registered_model_info.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Log the latest copy of the Agent's code/config and deploy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Use Unity Catalog as the model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = log_function_calling_agent_to_mlflow(fc_agent_config)\n",
    "\n",
    "    # Register the Agent's model to the Unity Catalog\n",
    "    uc_registered_model_info = mlflow.register_model(\n",
    "        model_uri=logged_agent_info.model_uri, name=agent_storage_config.uc_model_name\n",
    "    )\n",
    "\n",
    "# Deploy the model to the review app and a model serving endpoint\n",
    "# agents.deploy(agent_storage_config.uc_model_name, uc_registered_model_info.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the logged model to test it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_agent_info.model_uri)\n",
    "\n",
    "loaded_model.predict({\"messages\": [{\"role\": \"user\", \"content\": \"A test question?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "\n",
    "# Use Unity Catalog as the model registry\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    logged_agent_info = log_agent_to_mlflow(fc_agent_config)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "02_agent__function_calling_mlflow_sdk",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "genai-cookbook-T2SdtsNM-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
